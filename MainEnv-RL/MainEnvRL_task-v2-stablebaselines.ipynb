{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To register MainEnvRL with local copy of Gym\n",
    "\"\"\"\n",
    "from gym.envs.registration import register\n",
    " \n",
    "register(\n",
    "    id='MainEnvRL-v2',\n",
    "    #entry_point='balance_bot.envs:BalancebotEnv',\n",
    "    entry_point='MainEnv_RL.envs:MainEnvRL',\n",
    ")\n",
    "\n",
    "\"\"\"\n",
    "#import gym\n",
    "#for env in gym.envs.registry.env_specs:\n",
    "#    print(env)\n",
    "    \n",
    "    #if 'MainEnvRL-v1' == env:\n",
    "    #     print('Remove {} from registry'.format(env))\n",
    "         #del gym.envs.registry.env_specs[env]\n",
    "            \n",
    "            \n",
    "#import MainEnv_RL\n",
    "\n",
    "#env = gym.make('MainEnvRL-v2')\n",
    "#env.reset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nimport gym\\n\\nfrom stable_baselines.common.policies import MlpPolicy\\nfrom stable_baselines.common.vec_env import DummyVecEnv\\nfrom stable_baselines import PPO2\\n\\nenv = gym.make('CartPole-v1')\\n# Optional: PPO2 requires a vectorized environment to run\\n# the env is now wrapped automatically when passing it to the constructor\\n# env = DummyVecEnv([lambda: env])\\n\\nmodel = PPO2(MlpPolicy, env, verbose=0)\\nmodel.learn(total_timesteps=10000)\\n\\nobs = env.reset()\\nfor i in range(1000):\\n    action, _states = model.predict(obs)\\n    obs, rewards, dones, info = env.step(action)\\n    env.render()\\n\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "import gym\n",
    "\n",
    "from stable_baselines.common.policies import MlpPolicy\n",
    "from stable_baselines.common.vec_env import DummyVecEnv\n",
    "from stable_baselines import PPO2\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "# Optional: PPO2 requires a vectorized environment to run\n",
    "# the env is now wrapped automatically when passing it to the constructor\n",
    "# env = DummyVecEnv([lambda: env])\n",
    "\n",
    "model = PPO2(MlpPolicy, env, verbose=0)\n",
    "model.learn(total_timesteps=10000)\n",
    "\n",
    "obs = env.reset()\n",
    "for i in range(1000):\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, rewards, dones, info = env.step(action)\n",
    "    env.render()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep: 8 Dist (m) inf  Reward: -10 Target: (0.7, 0.085, 0.789)  Final Position:  [0.6885825341727702, 0.023, 0.822]\n",
      "action list [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#https://github.com/ZhizhenQin/BalancingBot/blob/master/balance-bot/balance_bot/balancebot_task.py\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore',category=FutureWarning)\n",
    "import tensorflow as tf\n",
    "#import tensorflow.contrib.layers as layers\n",
    "import tensorflow as tf\n",
    "#from tensorflow import keras\n",
    "#from tensorflow.keras import layers\n",
    "#from keras.utils import to_categorical\n",
    "#from keras.models import Sequential\n",
    "#from keras.layers import Dense, Dropout, Embedding, LSTM, GlobalMaxPooling1D, SpatialDropout1D\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "\n",
    "import gym\n",
    "from stable_baselines import DQN\n",
    "from stable_baselines import results_plotter\n",
    "from stable_baselines.bench import Monitor\n",
    "from stable_baselines.results_plotter import load_results, ts2xy\n",
    "from stable_baselines.common.noise import AdaptiveParamNoiseSpec\n",
    "from stable_baselines.common.callbacks import BaseCallback\n",
    "from stable_baselines.deepq.policies import MlpPolicy\n",
    "from stable_baselines.common.policies import FeedForwardPolicy, register_policy,LstmPolicy\n",
    "from stable_baselines.common.tf_layers import conv, linear, conv_to_fc, lstm\n",
    "from stable_baselines.common.env_checker import check_env\n",
    "#import balance_bot\n",
    "\n",
    "\"\"\"\n",
    "for env in gym.envs.registry.env_specs:\n",
    "     if 'MainEnvRL-v0' in env:\n",
    "        print('Remove {} from registry'.format(env))\n",
    "        del gym.registry.env_specs[env]\n",
    "\"\"\"                \n",
    "import MainEnv_RL\n",
    "\n",
    "def callback(lcl, glb):\n",
    "    # stop training if reward exceeds 199\n",
    "    is_solved = lcl['t'] > 100 and sum(lcl['episode_rewards'][-101:-1]) / 100 >= 199\n",
    "    \n",
    "    return is_solved\n",
    "\n",
    "#https://stable-baselines.readthedocs.io/en/master/guide/custom_policy.html\n",
    "class ScottCustomLSTMPolicy(LstmPolicy):\n",
    "    def __init__(self, sess, ob_space, ac_space, n_env, n_steps, n_batch, n_lstm=64, reuse=False, **_kwargs):\n",
    "        super().__init__(sess, ob_space, ac_space, n_env, n_steps, n_batch, n_lstm, reuse,\n",
    "                         net_arch=[8, 'lstm', dict(vf=[5, 10], pi=[10])],\n",
    "                         layer_norm=True, feature_extraction=\"mlp\", **_kwargs)\n",
    "        \n",
    "\n",
    "        \n",
    "def main():\n",
    "    #env = gym.make(\"MainEnvRL-v2\") # <-- this we need to create\n",
    "    #env = gym.make(\"CartPole-v0\")\n",
    "    #policy_kwargs= dict(act_fun=tf.nn.tanh, net_arch=[32, 32])\n",
    "    \n",
    "    \n",
    "    env= gym.make(\"MainEnvRL-v2\")\n",
    "    \n",
    "    #model = DQN(\"MlpPolicy\", env, learning_rate=1e-3, prioritized_replay=True,gamma=1 , buffer_size=50000,param_noise=False,\n",
    "    #            exploration_initial_eps=0.1, exploration_final_eps=0.1,learning_starts=1, verbose=1)\n",
    "    model = DQN(\"MlpPolicy\", env, learning_rate=1e-3, prioritized_replay=True,gamma=1 , buffer_size=50000,param_noise=False,\n",
    "                exploration_initial_eps=0.1, exploration_final_eps=0.1,learning_starts=1, verbose=1)\n",
    "    model.learn(total_timesteps=50000)\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    register_policy('ScottCustomLSTMPolicy', ScottCustomLSTMPolicy)\n",
    "    \n",
    "    model = DQN(policy='ScottCustomLSTMPolicy', env= gym.make(\"MainEnvRL-v2\"), learning_rate=1e-3, prioritized_replay=True,gamma=1 , buffer_size=50000,param_noise=False,\n",
    "                exploration_initial_eps=0.1, exploration_final_eps=0.1,learning_starts=1, verbose=1)\n",
    "    env = model.get_env()\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    #check_env(env)\n",
    "    #https://stable-baselines.readthedocs.io/en/master/modules/dqn.html\n",
    "    #model.learn(total_timesteps=25000)\n",
    "    #del model # remove to demonstrate saving and loading\n",
    "    #model = DQN.load(\"deepq_cartpole\")\n",
    "    \"\"\"\n",
    "    \n",
    "    act = deepq.learn(\n",
    "        env, network= model,  lr=1e-3, \n",
    "        total_timesteps=50000, buffer_size=50000, exploration_fraction=0.1,prioritized_replay=True,param_noise=True,\n",
    "        exploration_final_eps=0.02, print_freq=100, callback=callback\n",
    "    )\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    #default\n",
    "    act = deepq.learn(\n",
    "        env, network= 'mlp',  lr=1e-3,\n",
    "        total_timesteps=50000, buffer_size=50000, exploration_fraction=0.1,prioritized_replay=True,param_noise=False\n",
    "        exploration_final_eps=0.02, print_freq=100, callback=callback\n",
    "    )\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    \"\"\"\n",
    "    #rescale rewards to be between -1 and +1\n",
    "    obs = env.reset()\n",
    "    while True:\n",
    "        action, _states = model.predict(obs)\n",
    "        obs, rewards, done, info = env.step(action)\n",
    "        #obs = env.reset()\n",
    "        #env.render()\n",
    "        if done:\n",
    "            #print(\"Done!\")\n",
    "            #break\n",
    "            obs = env.reset()\n",
    "            \n",
    "    \"\"\"\n",
    "    \n",
    "    # save trained model\n",
    "    #act.save(\"balance.pkl\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
