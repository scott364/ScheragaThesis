{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfor env in gym.envs.registry.env_specs:\\n     if 'MainEnvRL-v0' in env:\\n        print('Remove {} from registry'.format(env))\\n        del gym.registry.env_specs[env]\\n\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#To register MainEnvRL with local copy of Gym\n",
    "\"\"\"\n",
    "from gym.envs.registration import register\n",
    " \n",
    "register(\n",
    "    id='MainEnvRL-v2',\n",
    "    #entry_point='balance_bot.envs:BalancebotEnv',\n",
    "    entry_point='MainEnv_RL.envs:MainEnvRL',\n",
    ")\n",
    "\n",
    "\"\"\"\n",
    "#import gym\n",
    "#for env in gym.envs.registry.env_specs:\n",
    "#    print(env)\n",
    "    \n",
    "    #if 'MainEnvRL-v1' == env:\n",
    "    #     print('Remove {} from registry'.format(env))\n",
    "         #del gym.envs.registry.env_specs[env]\n",
    "            \n",
    "            \n",
    "#import MainEnv_RL\n",
    "\n",
    "#env = gym.make('MainEnvRL-v2')\n",
    "#env.reset()\n",
    "\"\"\"\n",
    "for env in gym.envs.registry.env_specs:\n",
    "     if 'MainEnvRL-v0' in env:\n",
    "        print('Remove {} from registry'.format(env))\n",
    "        del gym.registry.env_specs[env]\n",
    "\"\"\"                \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep: 20   Total Steps taken: 570\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "| approxkl           | 0.0023683954   |\n",
      "| clipfrac           | 0.05           |\n",
      "| explained_variance | 5.96e-08       |\n",
      "| fps                | 0              |\n",
      "| n_updates          | 19             |\n",
      "| policy_entropy     | 1.0420827      |\n",
      "| policy_loss        | -0.00025040904 |\n",
      "| serial_timesteps   | 570            |\n",
      "| time_elapsed       | 769            |\n",
      "| total_timesteps    | 570            |\n",
      "| value_loss         | 55842.266      |\n",
      "---------------------------------------\n",
      "Ep: 20  tStep: 1 Z difference 0.0026681306183338194  Reward: -1.9973318693816662\n",
      "Ep: 20  tStep: 2 Z difference -0.0018850859079506677  Reward: -2.0018850859079507\n",
      "Ep: 20  tStep: 3 Z difference -0.001199866786971704  Reward: -2.0011998667869717\n",
      "Ep: 20  tStep: 4 Z difference 0.048722276799753494  Reward: -1.9512777232002465\n",
      "Ep: 20  tStep: 5 Z difference 0.05390673970803617  Reward: -1.9460932602919638\n",
      "Ep: 20  tStep: 6 Z difference 0.05143939354531479  Reward: -1.9485606064546852\n",
      "Ep: 20  tStep: 7 Z difference 0.05324014704860769  Reward: -1.9467598529513923\n",
      "Ep: 20  tStep: 8 Z difference 0.08919933772981192  Reward: -1.910800662270188\n",
      "Ep: 20  tStep: 9 Z difference 0.09094597182907194  Reward: -1.909054028170928\n",
      "Ep: 20  tStep: 10 Z difference 0.089914623186365  Reward: -1.910085376813635\n",
      "Ep: 20  tStep: 11 Z difference 0.09262865996584324  Reward: -1.9073713400341568\n",
      "Ep: 20  tStep: 12 Z difference 0.09149083249568957  Reward: -1.9085091675043104\n",
      "Ep: 20  tStep: 13 Z difference 0.09124766184017075  Reward: -1.9087523381598293\n",
      "Ep: 20  tStep: 14 Z difference 0.09568193300962458  Reward: -1.9043180669903754\n",
      "Ep: 20  tStep: 15 Z difference 0.09350557030923667  Reward: -1.9064944296907633\n",
      "Ep: 20  tStep: 16 Z difference 0.09654696348384029  Reward: -1.9034530365161597\n",
      "Ep: 20  tStep: 17 Z difference 0.10252048436962102  Reward: -1.897479515630379\n",
      "Ep: 20  tStep: 18 Z difference 0.10385894963033504  Reward: -1.896141050369665\n",
      "Ep: 20  tStep: 19 Z difference 0.10190053119659437  Reward: -1.8980994688034056\n",
      "Ep: 20  tStep: 20 Z difference 0.10125095168314902  Reward: -1.898749048316851\n",
      "Ep: 20  tStep: 21 Z difference 0.10156202825754912  Reward: -1.8984379717424509\n",
      "Ep: 20  tStep: 22 Z difference 0.10252356433570409  Reward: -1.897476435664296\n",
      "Ep: 20  tStep: 23 Z difference 0.10935624909363684  Reward: -1.8906437509063632\n",
      "Ep: 20  tStep: 24 Z difference 0.10748656301610193  Reward: -1.892513436983898\n",
      "Ep: 20  tStep: 25 Z difference 0.10902229277119035  Reward: -1.8909777072288096\n",
      "Ep: 20  tStep: 26 Z difference 0.11292460979856545  Reward: -1.8870753902014346\n",
      "Ep: 20  tStep: 27 Z difference 0.1168517132196576  Reward: -1.8831482867803424\n",
      "Ep: 20  tStep: 28 Z difference 0.11826087103523308  Reward: -1.881739128964767\n",
      "Ep: 20  tStep: 29 Z difference 0.11959684299007067  Reward: -1.8804031570099293\n",
      "Ep: 20  tStep: 30 Z difference 0.12235385262966147  Reward: -100\n",
      "   Actionlist: [3, 3, 3, 4, 3, 1, 3, 4, 1, 3, 1, 1, 3, 4, 1, 4, 4, 3, 1, 1, 1, 3, 4, 1, 1, 4, 4, 1, 4, 1]\n",
      "------------------------------------\n",
      "| approxkl           | 0.037476797 |\n",
      "| clipfrac           | 0.425       |\n",
      "| explained_variance | 1.19e-07    |\n",
      "| fps                | 0           |\n",
      "| n_updates          | 20          |\n",
      "| policy_entropy     | 1.0279938   |\n",
      "| policy_loss        | -0.03441208 |\n",
      "| serial_timesteps   | 600         |\n",
      "| time_elapsed       | 811         |\n",
      "| total_timesteps    | 600         |\n",
      "| value_loss         | 1000.70483  |\n",
      "------------------------------------\n",
      "Ep: 21  tStep: 1 Z difference -0.0007020856019108734  Reward: -2.000702085601911\n",
      "Ep: 21  tStep: 2 Z difference -0.0018252465669066353  Reward: -2.0018252465669066\n",
      "Ep: 21  tStep: 3 Z difference -0.001608622285723893  Reward: -2.001608622285724\n",
      "Ep: 21  tStep: 4 Z difference -4.399951547373604e-05  Reward: -2.0000439995154737\n",
      "Ep: 21  tStep: 5 Z difference 0.04853777216486632  Reward: -1.9514622278351337\n",
      "Ep: 21  tStep: 6 Z difference 0.05303320266082867  Reward: -1.9469667973391713\n",
      "Ep: 21  tStep: 7 Z difference 0.04596790713109078  Reward: -1.9540320928689092\n",
      "Ep: 21  tStep: 8 Z difference 0.051934974754601626  Reward: -1.9480650252453984\n",
      "Ep: 21  tStep: 9 Z difference 0.04966034646965545  Reward: -1.9503396535303446\n",
      "Ep: 21  tStep: 10 Z difference 0.05089951949045046  Reward: -1.9491004805095495\n",
      "Ep: 21  tStep: 11 Z difference 0.050152261052653024  Reward: -1.949847738947347\n",
      "Ep: 21  tStep: 12 Z difference 0.051119663732871246  Reward: -1.9488803362671288\n",
      "Ep: 21  tStep: 13 Z difference 0.05117744976319383  Reward: -1.9488225502368062\n",
      "Ep: 21  tStep: 14 Z difference 0.04965315988212815  Reward: -1.9503468401178718\n",
      "Ep: 21  tStep: 15 Z difference 0.050439284558594366  Reward: -1.9495607154414056\n",
      "Ep: 21  tStep: 16 Z difference 0.0497046393152325  Reward: -1.9502953606847675\n",
      "Ep: 21  tStep: 17 Z difference 0.05059900279976404  Reward: -1.949400997200236\n",
      "Ep: 21  tStep: 18 Z difference 0.049732359009980964  Reward: -1.950267640990019\n",
      "Ep: 21  tStep: 19 Z difference 0.05126574212424462  Reward: -1.9487342578757554\n",
      "Ep: 21  tStep: 20 Z difference 0.05014859442636377  Reward: -1.9498514055736362\n",
      "Ep: 21  tStep: 21 Z difference 0.04956252088025215  Reward: -1.9504374791197479\n",
      "Ep: 21  tStep: 22 Z difference 0.047766900653764655  Reward: -100\n",
      "Ep: 21  tStep: 23 Z difference 0.04757814273238159  Reward: -100\n",
      "Ep: 21  tStep: 24 Z difference 0.07832691079117371  Reward: -100\n",
      "Ep: 21  tStep: 25 Z difference 0.08097128167115164  Reward: -100\n",
      "Ep: 21  tStep: 26 Z difference 0.07954672402515994  Reward: -100\n",
      "Ep: 21  tStep: 27 Z difference 0.07942836532853548  Reward: -100\n",
      "Ep: 21  tStep: 28 Z difference 0.08085556294545526  Reward: -100\n",
      "Ep: 21  tStep: 29 Z difference 0.08033152871616167  Reward: -100\n",
      "Ep: 21  tStep: 30 Z difference 0.08366258536763471  Reward: -100\n",
      "   Actionlist: [1, 1, 1, 1, 4, 3, 3, 3, 1, 3, 3, 3, 3, 3, 1, 3, 1, 3, 3, 1, 3, 1, 1, 4, 3, 3, 3, 3, 1, 3]\n",
      "------------------------------------\n",
      "| approxkl           | 0.008335788 |\n",
      "| clipfrac           | 0.14166667  |\n",
      "| explained_variance | 0           |\n",
      "| fps                | 0           |\n",
      "| n_updates          | 21          |\n",
      "| policy_entropy     | 0.9450283   |\n",
      "| policy_loss        | 0.00507909  |\n",
      "| serial_timesteps   | 630         |\n",
      "| time_elapsed       | 852         |\n",
      "| total_timesteps    | 630         |\n",
      "| value_loss         | 88095.76    |\n",
      "------------------------------------\n",
      "Ep: 22  tStep: 1 Z difference 0.0037384921647607428  Reward: -1.9962615078352393\n",
      "Ep: 22  tStep: 2 Z difference 0.0012938790850340887  Reward: -1.998706120914966\n",
      "Ep: 22  tStep: 3 Z difference 0.0032745906066149466  Reward: -1.996725409393385\n",
      "Ep: 22  tStep: 4 Z difference 0.0010527617402376066  Reward: -1.9989472382597624\n",
      "Ep: 22  tStep: 5 Z difference 0.0030290733102709133  Reward: -1.996970926689729\n",
      "Ep: 22  tStep: 6 Z difference 0.0022206555459649557  Reward: -1.997779344454035\n",
      "Ep: 22  tStep: 7 Z difference 0.002240748658031144  Reward: -1.9977592513419689\n",
      "Ep: 22  tStep: 8 Z difference 0.0010559883713723295  Reward: -1.9989440116286277\n",
      "Ep: 22  tStep: 9 Z difference 0.05468347782120109  Reward: -1.945316522178799\n",
      "Ep: 22  tStep: 10 Z difference 0.05420138979665934  Reward: -1.9457986102033407\n",
      "Ep: 22  tStep: 11 Z difference 0.05250256850421442  Reward: -1.9474974314957856\n",
      "Ep: 22  tStep: 12 Z difference 0.05605420939326278  Reward: -1.9439457906067372\n",
      "Ep: 22  tStep: 13 Z difference 0.05438120114989564  Reward: -1.9456187988501044\n",
      "Ep: 22  tStep: 14 Z difference 0.05671742875650532  Reward: -1.9432825712434947\n",
      "Ep: 22  tStep: 15 Z difference 0.05323838706798867  Reward: -1.9467616129320113\n",
      "Ep: 22  tStep: 16 Z difference 0.05428894883245228  Reward: -1.9457110511675477\n",
      "Ep: 22  tStep: 17 Z difference 0.05472263738997274  Reward: -1.9452773626100273\n",
      "Ep: 22  tStep: 18 Z difference 0.05610730214193449  Reward: -1.9438926978580655\n",
      "Ep: 22  tStep: 19 Z difference 0.0554731224589049  Reward: -1.944526877541095\n",
      "Ep: 22  tStep: 20 Z difference 0.05561318758316336  Reward: -1.9443868124168366\n",
      "Ep: 22  tStep: 21 Z difference 0.05703290528245297  Reward: -100\n",
      "Ep: 22  tStep: 22 Z difference 0.05656812373399722  Reward: -100\n",
      "Ep: 22  tStep: 23 Z difference 0.057029825316369465  Reward: -100\n",
      "Ep: 22  tStep: 24 Z difference 0.05746982047110816  Reward: -100\n",
      "Ep: 22  tStep: 25 Z difference 0.05609674225822081  Reward: -100\n",
      "Ep: 22  tStep: 26 Z difference 0.05892239114195119  Reward: -100\n",
      "Ep: 22  tStep: 27 Z difference 0.056527790844813186  Reward: -100\n",
      "Ep: 22  tStep: 28 Z difference 0.05636660595312737  Reward: -100\n",
      "Ep: 22  tStep: 29 Z difference 0.05519636550657436  Reward: -100\n",
      "Ep: 22  tStep: 30 Z difference 0.05581529202423985  Reward: -100\n",
      "   Actionlist: [3, 3, 3, 3, 1, 3, 3, 3, 4, 1, 3, 1, 3, 1, 3, 3, 3, 1, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\n",
      "-------------------------------------\n",
      "| approxkl           | 0.023621315  |\n",
      "| clipfrac           | 0.15         |\n",
      "| explained_variance | 0            |\n",
      "| fps                | 0            |\n",
      "| n_updates          | 22           |\n",
      "| policy_entropy     | 0.7784471    |\n",
      "| policy_loss        | -0.013933607 |\n",
      "| serial_timesteps   | 660          |\n",
      "| time_elapsed       | 893          |\n",
      "| total_timesteps    | 660          |\n",
      "| value_loss         | 106113.484   |\n",
      "-------------------------------------\n",
      "Ep: 23  tStep: 1 Z difference 0.0031873249009253257  Reward: -1.9968126750990747\n",
      "Ep: 23  tStep: 2 Z difference 0.001963111715391097  Reward: -1.998036888284609\n",
      "Ep: 23  tStep: 3 Z difference 0.0034196423426271494  Reward: -1.9965803576573729\n",
      "Ep: 23  tStep: 4 Z difference 0.0009121099557729551  Reward: -1.999087890044227\n",
      "Ep: 23  tStep: 5 Z difference 0.004480617325753222  Reward: -1.9955193826742468\n",
      "Ep: 23  tStep: 6 Z difference 0.05073129467628901  Reward: -1.949268705323711\n",
      "Ep: 23  tStep: 7 Z difference 0.05562961406894029  Reward: -1.9443703859310597\n",
      "Ep: 23  tStep: 8 Z difference 0.08371641144156472  Reward: -1.9162835885584353\n",
      "Ep: 23  tStep: 9 Z difference 0.08407412750236709  Reward: -1.915925872497633\n",
      "Ep: 23  tStep: 10 Z difference 0.08379018396250926  Reward: -1.9162098160374907\n",
      "Ep: 23  tStep: 11 Z difference 0.09063533524982637  Reward: -1.9093646647501736\n",
      "Ep: 23  tStep: 12 Z difference 0.08959665335454048  Reward: -1.9104033466454595\n",
      "Ep: 23  tStep: 13 Z difference 0.0918255221433939  Reward: -1.908174477856606\n",
      "Ep: 23  tStep: 14 Z difference 0.09070925443582256  Reward: -1.9092907455641774\n",
      "Ep: 23  tStep: 15 Z difference 0.09334262543693184  Reward: -1.9066573745630682\n",
      "Ep: 23  tStep: 16 Z difference 0.09261927340254195  Reward: -1.907380726597458\n",
      "Ep: 23  tStep: 17 Z difference 0.09304958866387603  Reward: -1.906950411336124\n",
      "Ep: 23  tStep: 18 Z difference 0.0934210912395268  Reward: -1.9065789087604732\n",
      "Ep: 23  tStep: 19 Z difference 0.09434640104994196  Reward: -1.905653598950058\n",
      "Ep: 23  tStep: 20 Z difference 0.0938219268254934  Reward: -1.9061780731745066\n",
      "Ep: 23  tStep: 21 Z difference 0.09321590683236725  Reward: -100\n",
      "Ep: 23  tStep: 22 Z difference 0.09307012177109719  Reward: -100\n",
      "Ep: 23  tStep: 23 Z difference 0.09339131823405644  Reward: -100\n",
      "Ep: 23  tStep: 24 Z difference 0.09378496723249574  Reward: -100\n",
      "Ep: 23  tStep: 25 Z difference 0.09252760774530477  Reward: -100\n",
      "Ep: 23  tStep: 26 Z difference 0.0930469486929475  Reward: -100\n",
      "Ep: 23  tStep: 27 Z difference 0.09215537184439615  Reward: -100\n",
      "Ep: 23  tStep: 28 Z difference 0.09223090434595926  Reward: -100\n",
      "Ep: 23  tStep: 29 Z difference 0.09203481317199769  Reward: -100\n",
      "Ep: 23  tStep: 30 Z difference 0.09304284207150326  Reward: -100\n",
      "   Actionlist: [1, 3, 3, 3, 1, 4, 3, 4, 3, 3, 4, 3, 3, 3, 3, 3, 3, 3, 3, 1, 1, 3, 3, 3, 1, 3, 3, 3, 3, 3]\n",
      "-------------------------------------\n",
      "| approxkl           | 0.013860511  |\n",
      "| clipfrac           | 0.13333334   |\n",
      "| explained_variance | -1.19e-07    |\n",
      "| fps                | 0            |\n",
      "| n_updates          | 23           |\n",
      "| policy_entropy     | 0.68544656   |\n",
      "| policy_loss        | -0.006628869 |\n",
      "| serial_timesteps   | 690          |\n",
      "| time_elapsed       | 934          |\n",
      "| total_timesteps    | 690          |\n",
      "| value_loss         | 105454.04    |\n",
      "-------------------------------------\n",
      "Ep: 24  tStep: 1 Z difference 0.0015993823874738133  Reward: -1.9984006176125262\n",
      "Ep: 24  tStep: 2 Z difference -0.0011117210909725728  Reward: -2.0011117210909726\n",
      "Ep: 24  tStep: 3 Z difference -0.0005045277774335943  Reward: -2.0005045277774336\n",
      "Ep: 24  tStep: 4 Z difference 0.04675080517642183  Reward: -1.9532491948235782\n",
      "Ep: 24  tStep: 5 Z difference 0.08556659106723963  Reward: -1.9144334089327604\n",
      "Ep: 24  tStep: 6 Z difference 0.08650803403332796  Reward: -1.913491965966672\n",
      "Ep: 24  tStep: 7 Z difference 0.08525126120634363  Reward: -1.9147487387936564\n",
      "Ep: 24  tStep: 8 Z difference 0.08434281787686038  Reward: -1.9156571821231396\n",
      "Ep: 24  tStep: 9 Z difference 0.08640507516711926  Reward: -1.9135949248328807\n",
      "Ep: 24  tStep: 10 Z difference 0.08512014265023149  Reward: -1.9148798573497685\n",
      "Ep: 24  tStep: 11 Z difference 0.09064178851209581  Reward: -1.9093582114879042\n",
      "Ep: 24  tStep: 12 Z difference 0.09126628830172079  Reward: -1.9087337116982792\n",
      "Ep: 24  tStep: 13 Z difference 0.0918007357496764  Reward: -1.9081992642503236\n",
      "Ep: 24  tStep: 14 Z difference 0.09280113806650014  Reward: -1.9071988619334999\n",
      "Ep: 24  tStep: 15 Z difference 0.09325667971670626  Reward: -1.9067433202832937\n",
      "Ep: 24  tStep: 16 Z difference 0.09371706131361401  Reward: -1.906282938686386\n",
      "Ep: 24  tStep: 17 Z difference 0.09515423215404128  Reward: -1.9048457678459587\n",
      "Ep: 24  tStep: 18 Z difference 0.09417450960949036  Reward: -1.9058254903905096\n",
      "Ep: 24  tStep: 19 Z difference 0.09387531290426843  Reward: -1.9061246870957316\n",
      "Ep: 24  tStep: 20 Z difference 0.10120445886179796  Reward: -1.898795541138202\n",
      "Ep: 24  tStep: 21 Z difference 0.10114300620518613  Reward: -1.8988569937948139\n",
      "Ep: 24  tStep: 22 Z difference 0.10509665600061391  Reward: -1.894903343999386\n",
      "Ep: 24  tStep: 23 Z difference 0.10575356876663822  Reward: -100\n",
      "Ep: 24  tStep: 24 Z difference 0.10641869477555144  Reward: -100\n",
      "Ep: 24  tStep: 25 Z difference 0.10749506958909327  Reward: -100\n",
      "Ep: 24  tStep: 26 Z difference 0.10808964970819623  Reward: -100\n",
      "Ep: 24  tStep: 27 Z difference 0.10669398507736583  Reward: -100\n",
      "Ep: 24  tStep: 28 Z difference 0.11416304949410216  Reward: -100\n",
      "Ep: 24  tStep: 29 Z difference 0.11388203925527618  Reward: -100\n",
      "Ep: 24  tStep: 30 Z difference 0.11484181535281213  Reward: -100\n",
      "   Actionlist: [3, 3, 3, 4, 4, 3, 3, 3, 3, 1, 4, 3, 3, 3, 3, 1, 3, 3, 3, 4, 3, 4, 3, 3, 3, 3, 3, 4, 3, 3]\n",
      "------------------------------------\n",
      "| approxkl           | 0.005018241 |\n",
      "| clipfrac           | 0.05        |\n",
      "| explained_variance | 0           |\n",
      "| fps                | 0           |\n",
      "| n_updates          | 24          |\n",
      "| policy_entropy     | 0.7991293   |\n",
      "| policy_loss        | 0.004862821 |\n",
      "| serial_timesteps   | 720         |\n",
      "| time_elapsed       | 975         |\n",
      "| total_timesteps    | 720         |\n",
      "| value_loss         | 69322.516   |\n",
      "------------------------------------\n",
      "Ep: 25  tStep: 1 Z difference 0.003060312966257417  Reward: -1.9969396870337426\n",
      "Ep: 25  tStep: 2 Z difference 0.00016030490137630693  Reward: -1.9998396950986237\n",
      "Ep: 25  tStep: 3 Z difference 0.002433613200857998  Reward: -1.997566386799142\n",
      "Ep: 25  tStep: 4 Z difference 0.002199389113485495  Reward: -1.9978006108865145\n",
      "Ep: 25  tStep: 5 Z difference 0.002875221671164052  Reward: -1.997124778328836\n",
      "Ep: 25  tStep: 6 Z difference 0.0020694438777861812  Reward: -1.9979305561222138\n",
      "Ep: 25  tStep: 7 Z difference 0.003027313329651893  Reward: -1.996972686670348\n",
      "Ep: 25  tStep: 8 Z difference 0.004553509856387805  Reward: -1.9954464901436122\n",
      "Ep: 25  tStep: 9 Z difference 0.0026254510883241267  Reward: -1.9973745489116759\n",
      "Ep: 25  tStep: 10 Z difference 0.003116192350908875  Reward: -1.9968838076490911\n",
      "Ep: 25  tStep: 11 Z difference 0.005253542147576784  Reward: -1.9947464578524232\n",
      "Ep: 25  tStep: 12 Z difference 0.00561961811631928  Reward: -1.9943803818836807\n",
      "Ep: 25  tStep: 13 Z difference 0.005705417171493199  Reward: -1.9942945828285068\n",
      "Ep: 25  tStep: 14 Z difference 0.0024975591633467076  Reward: -1.9975024408366533\n",
      "Ep: 25  tStep: 15 Z difference 0.0060650398779658055  Reward: -1.9939349601220342\n",
      "Ep: 25  tStep: 16 Z difference 0.05335953240059288  Reward: -1.9466404675994071\n",
      "Ep: 25  tStep: 17 Z difference 0.056854120584577394  Reward: -1.9431458794154226\n",
      "Ep: 25  tStep: 18 Z difference 0.08720791965946528  Reward: -1.9127920803405347\n",
      "Ep: 25  tStep: 19 Z difference 0.09053912297598998  Reward: -1.90946087702401\n",
      "Ep: 25  tStep: 20 Z difference 0.08841115974262337  Reward: -100\n",
      "Ep: 25  tStep: 21 Z difference 0.09424446883909399  Reward: -1.905755531160906\n",
      "Ep: 25  tStep: 22 Z difference 0.09528828401118528  Reward: -100\n",
      "Ep: 25  tStep: 23 Z difference 0.10230943336039777  Reward: -100\n",
      "Ep: 25  tStep: 24 Z difference 0.10086360928192706  Reward: -100\n",
      "Ep: 25  tStep: 25 Z difference 0.10219855458140348  Reward: -100\n",
      "Ep: 25  tStep: 26 Z difference 0.10215660837665208  Reward: -100\n",
      "Ep: 25  tStep: 27 Z difference 0.10170136005654928  Reward: -100\n",
      "Ep: 25  tStep: 28 Z difference 0.10181986541822541  Reward: -100\n",
      "Ep: 25  tStep: 29 Z difference 0.105185681686923  Reward: -100\n",
      "Ep: 25  tStep: 30 Z difference 0.10450207588151095  Reward: -100\n",
      "   Actionlist: [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 1, 3, 4, 3, 4, 3, 1, 4, 3, 4, 3, 3, 3, 3, 3, 4, 3]\n",
      "-------------------------------------\n",
      "| approxkl           | 0.06659092   |\n",
      "| clipfrac           | 0.17500001   |\n",
      "| explained_variance | -1.19e-07    |\n",
      "| fps                | 0            |\n",
      "| n_updates          | 25           |\n",
      "| policy_entropy     | 0.47703254   |\n",
      "| policy_loss        | -0.028686859 |\n",
      "| serial_timesteps   | 750          |\n",
      "| time_elapsed       | 1.02e+03     |\n",
      "| total_timesteps    | 750          |\n",
      "| value_loss         | 103142.72    |\n",
      "-------------------------------------\n",
      "Ep: 26  tStep: 1 Z difference 0.0020528707269575897  Reward: -1.9979471292730424\n",
      "Ep: 26  tStep: 2 Z difference -0.0013455051831900988  Reward: -2.00134550518319\n",
      "Ep: 26  tStep: 3 Z difference 0.0016144888877867025  Reward: -1.9983855111122133\n",
      "Ep: 26  tStep: 4 Z difference 0.0014106244660911926  Reward: -1.9985893755339088\n",
      "Ep: 26  tStep: 5 Z difference 0.0007210053935646776  Reward: -1.9992789946064353\n",
      "Ep: 26  tStep: 6 Z difference 0.0022480819106101  Reward: -1.99775191808939\n",
      "Ep: 26  tStep: 7 Z difference 0.00355765415616327  Reward: -1.9964423458438367\n",
      "Ep: 26  tStep: 8 Z difference 0.0019433119334277826  Reward: -1.9980566880665722\n",
      "Ep: 26  tStep: 9 Z difference -0.0002692770346999218  Reward: -2.0002692770347\n",
      "Ep: 26  tStep: 10 Z difference 0.00016118489168581718  Reward: -1.9998388151083142\n",
      "Ep: 26  tStep: 11 Z difference -0.0018558995626869468  Reward: -2.001855899562687\n",
      "Ep: 26  tStep: 12 Z difference -0.0012847858518361122  Reward: -2.001284785851836\n",
      "Ep: 26  tStep: 13 Z difference -0.00029333010315912844  Reward: -2.000293330103159\n",
      "Ep: 26  tStep: 14 Z difference -2.478639371705782e-05  Reward: -2.000024786393717\n",
      "Ep: 26  tStep: 15 Z difference -0.001583102566748984  Reward: -2.001583102566749\n",
      "Ep: 26  tStep: 16 Z difference -0.0003383562739940338  Reward: -100\n",
      "Ep: 26  tStep: 17 Z difference 0.0007535650350152245  Reward: -100\n",
      "Ep: 26  tStep: 18 Z difference 0.00024155733995145923  Reward: -100\n",
      "Ep: 26  tStep: 19 Z difference 0.001407837830111447  Reward: -100\n",
      "Ep: 26  tStep: 20 Z difference 0.002609171267598853  Reward: -100\n",
      "Ep: 26  tStep: 21 Z difference 0.0023539740778506513  Reward: -100\n",
      "Ep: 26  tStep: 22 Z difference 0.0022862148240205826  Reward: -100\n",
      "Ep: 26  tStep: 23 Z difference 0.0006090999592092139  Reward: -100\n",
      "Ep: 26  tStep: 24 Z difference -0.0007126454856249964  Reward: -100\n",
      "Ep: 26  tStep: 25 Z difference -0.0012859591722489405  Reward: -100\n",
      "Ep: 26  tStep: 26 Z difference -0.0014437707677483758  Reward: -100\n",
      "Ep: 26  tStep: 27 Z difference -0.0010876680225133661  Reward: -100\n",
      "Ep: 26  tStep: 28 Z difference -0.0014744237635286872  Reward: -100\n",
      "Ep: 26  tStep: 29 Z difference -0.00204700412489478  Reward: -100\n",
      "Ep: 26  tStep: 30 Z difference -0.0027012769199910025  Reward: -100\n",
      "   Actionlist: [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\n",
      "--------------------------------------\n",
      "| approxkl           | 0.00016197687 |\n",
      "| clipfrac           | 0.0           |\n",
      "| explained_variance | 5.96e-08      |\n",
      "| fps                | 0             |\n",
      "| n_updates          | 26            |\n",
      "| policy_entropy     | 0.17455998    |\n",
      "| policy_loss        | 4.808108e-07  |\n",
      "| serial_timesteps   | 780           |\n",
      "| time_elapsed       | 1.06e+03      |\n",
      "| total_timesteps    | 780           |\n",
      "| value_loss         | 210912.05     |\n",
      "--------------------------------------\n",
      "Ep: 27  tStep: 1 Z difference 0.002411173447966597  Reward: -1.9975888265520334\n",
      "Ep: 27  tStep: 2 Z difference 0.0010209354240444668  Reward: -1.9989790645759555\n",
      "Ep: 27  tStep: 3 Z difference 0.002279761561751137  Reward: -1.9977202384382489\n",
      "Ep: 27  tStep: 4 Z difference 0.0033026036314662832  Reward: -1.9966973963685337\n",
      "Ep: 27  tStep: 5 Z difference 0.0015515695806591623  Reward: -1.9984484304193408\n",
      "Ep: 27  tStep: 6 Z difference 0.0006890324123203229  Reward: -1.9993109675876797\n",
      "Ep: 27  tStep: 7 Z difference -0.00044952838309120224  Reward: -2.000449528383091\n",
      "Ep: 27  tStep: 8 Z difference -0.0008584305468950504  Reward: -2.000858430546895\n",
      "Ep: 27  tStep: 9 Z difference -0.000521687588468378  Reward: -2.0005216875884684\n",
      "Ep: 27  tStep: 10 Z difference -0.0001410917796196287  Reward: -2.0001410917796196\n",
      "Ep: 27  tStep: 11 Z difference 0.00045656830556684014  Reward: -1.9995434316944332\n",
      "Ep: 27  tStep: 12 Z difference 0.0005755136623974977  Reward: -1.9994244863376025\n",
      "Ep: 27  tStep: 13 Z difference 0.0017045412294565132  Reward: -1.9982954587705435\n",
      "Ep: 27  tStep: 14 Z difference 0.0025547585334626532  Reward: -1.9974452414665373\n",
      "Ep: 27  tStep: 15 Z difference 0.0029907937318083277  Reward: -100\n",
      "Ep: 27  tStep: 16 Z difference 0.0032379243437197225  Reward: -100\n",
      "Ep: 27  tStep: 17 Z difference 0.002932714371382872  Reward: -100\n",
      "Ep: 27  tStep: 18 Z difference 0.0038616908080872925  Reward: -100\n",
      "Ep: 27  tStep: 19 Z difference 0.002907047987356748  Reward: -100\n",
      "Ep: 27  tStep: 20 Z difference 0.0013328919887540813  Reward: -100\n",
      "Ep: 27  tStep: 21 Z difference 0.00019917114004464054  Reward: -100\n",
      "Ep: 27  tStep: 22 Z difference 0.0002560771800577122  Reward: -100\n",
      "Ep: 27  tStep: 23 Z difference -0.001886112563312281  Reward: -100\n",
      "Ep: 27  tStep: 24 Z difference -0.0022344420608133575  Reward: -100\n",
      "Ep: 27  tStep: 25 Z difference -0.0027806227128954752  Reward: -100\n",
      "Ep: 27  tStep: 26 Z difference -0.0005684737399223039  Reward: -100\n",
      "Ep: 27  tStep: 27 Z difference -0.002261135100201095  Reward: -100\n",
      "Ep: 27  tStep: 28 Z difference -0.004380151765421392  Reward: -100\n",
      "Ep: 27  tStep: 29 Z difference -0.0028951681181790256  Reward: -100\n",
      "Ep: 27  tStep: 30 Z difference -0.0017557273324579903  Reward: -100\n",
      "   Actionlist: [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\n",
      "closing SnS socket\n",
      "socket DC Closed\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-2d6938391fee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;31m#model.learn(total_timesteps=timesteps,b_log_name=\"first_run\", reset_num_timesteps=False)#50000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimesteps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreset_num_timesteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#50000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0mtoday\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoday\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/thesis/lib/python3.6/site-packages/stable_baselines/ppo2/ppo2.py\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps)\u001b[0m\n\u001b[1;32m    334\u001b[0m                 \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_rollout_start\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m                 \u001b[0;31m# true_reward is the reward without discount\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 336\u001b[0;31m                 \u001b[0mrollout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    337\u001b[0m                 \u001b[0;31m# Unpack\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m                 \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneglogpacs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mep_infos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrue_reward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrollout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/thesis/lib/python3.6/site-packages/stable_baselines/common/runners.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, callback)\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontinue_training\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mabstractmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/thesis/lib/python3.6/site-packages/stable_baselines/ppo2/ppo2.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    480\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBox\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m                 \u001b[0mclipped_actions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhigh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdones\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclipped_actions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_timesteps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_envs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/thesis/lib/python3.6/site-packages/stable_baselines/common/vec_env/base_vec_env.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    148\u001b[0m         \"\"\"\n\u001b[1;32m    149\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/thesis/lib/python3.6/site-packages/stable_baselines/common/vec_env/dummy_vec_env.py\u001b[0m in \u001b[0;36mstep_wait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     46\u001b[0m                 \u001b[0;31m# save final observation where user can get it, then reset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuf_infos\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0menv_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'terminal_observation'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m                 \u001b[0mobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menvs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0menv_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save_obs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         return (self._obs_from_buf(), np.copy(self.buf_rews), np.copy(self.buf_dones),\n",
      "\u001b[0;32m~/ScheragaThesis/UR5-RL/UR5_RL/envs/ur5_env_0.py\u001b[0m in \u001b[0;36mreset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    231\u001b[0m         \u001b[0;31m#if self.totalstepstaken>=410:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0;31m#    print(\"reset\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 233\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresetEnvironment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    234\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_observation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_observation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#you *have* to compute and return the observation from reset()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepisodeinitialpose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrentpose\u001b[0m \u001b[0;31m#contains just initial xyz poses IN INCHES\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ScheragaThesis/UR5-RL/UR5_RL/envs/ur5_env_0.py\u001b[0m in \u001b[0;36mresetEnvironment\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 259\u001b[0;31m         \u001b[0mdata2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msock_DC\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#receive the \"done\" command\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    260\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mdata2\u001b[0m\u001b[0;34m==\u001b[0m \u001b[0;34mb''\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m             \u001b[0mdata2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msock_DC\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m#48 bytes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#https://github.com/ZhizhenQin/BalancingBot/blob/master/balance-bot/balance_bot/balancebot_task.py\n",
    "\n",
    "\n",
    "#%load_ext tensorboard\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore',category=FutureWarning)\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "\n",
    "import gym\n",
    "from stable_baselines import DQN, PPO2\n",
    "from stable_baselines import results_plotter\n",
    "from stable_baselines.bench import Monitor\n",
    "from stable_baselines.results_plotter import load_results, ts2xy\n",
    "from stable_baselines.common.noise import AdaptiveParamNoiseSpec\n",
    "from stable_baselines.common.callbacks import BaseCallback\n",
    "from stable_baselines.deepq.policies import MlpPolicy\n",
    "from stable_baselines.common.policies import FeedForwardPolicy, register_policy,LstmPolicy\n",
    "from stable_baselines.common.tf_layers import conv, linear, conv_to_fc, lstm\n",
    "from stable_baselines.common.vec_env import DummyVecEnv\n",
    "from stable_baselines.common.env_checker import check_env\n",
    "from datetime import date\n",
    "import csv\n",
    "#import balance_bot\n",
    "import socket\n",
    "import numpy as np\n",
    "from time import sleep\n",
    "import UR5_RL\n",
    "\n",
    "HOST_SnS = '192.168.0.103'\n",
    "PORT_SnS= 65499\n",
    "\n",
    "\n",
    "\n",
    "try:\n",
    "    # Create a TCP/IP socket\n",
    "    sock_SnS = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "    #sock_SnS.setblocking(False)\n",
    "    # Connect the socket to the port where the server is listening\n",
    "    server_address_SnS = (HOST_SnS, PORT_SnS)\n",
    "    print('connecting to {} port {}'.format(*server_address_SnS))\n",
    "    sock_SnS.connect(server_address_SnS)\n",
    "\n",
    "\n",
    "    #def callback(lcl, glb):\n",
    "         #stop training if reward exceeds 199\n",
    "    #    is_solved = lcl['t'] > 1000 and sum(lcl['episode_rewards'][-101:-1]) / 100 >= 1\n",
    "    #    return is_solved\n",
    "\n",
    "    #https://stable-baselines.readthedocs.io/en/master/guide/custom_policy.html\n",
    "    #Layers of 20 and 15\n",
    "\n",
    "    #Do this only after restarting the notebook!! \n",
    "    #register_policy('ScottLSTMPolicy', ScottLSTMPolicy)    \n",
    "    #print(\"lstm registered\")\n",
    "\n",
    "    #try:\n",
    "    \n",
    "    #code stopped at ep 36 at 10steps per ep, 80 ep\n",
    "    StepsPerEpisode=30 #was 10\n",
    "    TotalEpisodes=700  #was 80\n",
    "    env= gym.make(\"ur5-rl-v0\",StepsPerEpisode=StepsPerEpisode,TotalEpisodes=TotalEpisodes,\n",
    "                 continuousactionspace=False)\n",
    "    env = DummyVecEnv([lambda: env])\n",
    "\n",
    "    timesteps=(StepsPerEpisode*TotalEpisodes)\n",
    "    print(\"Total timesteps:\",timesteps)\n",
    "    \n",
    "    class ScottLSTMPolicy(LstmPolicy):\n",
    "        def __init__(self, sess, ob_space, ac_space, n_env=1, n_steps=StepsPerEpisode,\n",
    "                     n_batch=StepsPerEpisode, n_lstm=StepsPerEpisode, reuse=False,  **_kwargs):\n",
    "            super().__init__(sess, ob_space, ac_space, n_env, n_steps, n_batch, n_lstm, reuse,\n",
    "                             net_arch=[7,'lstm',dict(vf=[20, 15],pi=[20,15])],\n",
    "                             layer_norm=True, feature_extraction=\"mlp\", **_kwargs)\n",
    "            \n",
    "    #model = DQN(\"LnMlpPolicy\", env, learning_rate=1e-3, prioritized_replay=True,gamma=1 , buffer_size=50000,param_noise=False,\n",
    "    # exploration_initial_eps=0.1, exploration_final_eps=0.1,learning_starts=1, verbose=1)\n",
    "\n",
    "\n",
    "    #model = PPO2(\"MlpPolicy\", env,verboThe new research shows that \"we must expect extreme event records to be broken - not just by small marse=0)\n",
    "    #model = PPO2(\"MlpLstmPolicy\", env,nminibatches=1, n_steps=80, #exploration_initial_eps=1, exploration_final_eps=0.1,\n",
    "    #             verbose=0,tensorboard_log=\"./ScottPPOLstm/\") #n_lstm=2, n_batch=80, nminibatches=10,\n",
    "\n",
    "    model = PPO2(ScottLSTMPolicy, env,nminibatches=1, n_steps=StepsPerEpisode,learning_rate=0.1,\n",
    "                 verbose=2)\n",
    "    \n",
    "    # DEFAULT learning_rate=0.00025  #n_lstm=2, n_batch=80, nminibatches=10,\n",
    "    ##exploration_initial_eps=1, exploration_final_eps=0.1,\n",
    "\n",
    "    #model.learn(total_timesteps=timesteps,b_log_name=\"first_run\", reset_num_timesteps=False)#50000\n",
    "    model.learn(total_timesteps=timesteps,reset_num_timesteps=False)#50000\n",
    "    \n",
    "    today = date.today()\n",
    "    todaydate = today.strftime(\"%m_%d_%Y\")\n",
    "    RLmodelfilename=\"UR5-RL_savedpolicy_\"+todaydate\n",
    "    model.save(RLmodelfilename)# save trained model\n",
    "    \n",
    "    del model\n",
    "    print(\"training complete\")\n",
    "    \n",
    "\n",
    "finally:\n",
    "    endmsg='end'\n",
    "    data1=endmsg.encode('ascii')    \n",
    "    sock_SnS.sendall(data1)\n",
    "    sock_SnS.sendall(data1)\n",
    "    print('closing SnS socket')\n",
    "    sock_SnS.close()\n",
    "    \n",
    "    HOST2 = '192.168.0.103'\n",
    "    PORT2= PORT_SnS-10 #65481\n",
    "    sock_DC = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "    server_address_DC = (HOST2, PORT2)\n",
    "    sock_DC.close()\n",
    "    print(\"socket DC Closed\")\n",
    "      \n",
    "    #gitkraken\n",
    "    \n",
    "    #check_env(env)\n",
    "    #https://stable-baselines.readthedocs.io/en/master/modules/dqn.html\n",
    "    #model.learn(total_timesteps=25000)\n",
    "    #del model # remove to demonstrate saving and loading\n",
    "    #model = DQN.load(\"deepq_cartpole\")\n",
    "\n",
    "    \n",
    "    #Starting at 11:02AM\n",
    "    #80 episodes in 45 min\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved!\n"
     ]
    }
   ],
   "source": [
    "model.save(\"UR5-RL_savedpolicy-8-5to6-2021\")\n",
    "print(\"saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HOST2 = '192.168.0.103'learning_rate=0.00025\n",
    "PORT2= 65485\n",
    "sock_DC = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "server_address_DC = (HOST2, PORT2)\n",
    "sock_DC.close()\n",
    "print(\"socket DC Closed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[Errno 9] Bad file descriptor",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-5c92c6e2c5c7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mendmsg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'end'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdata1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mendmsg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ascii'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0msock_SnS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msendall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'closing SnS socket'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0msock_SnS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 9] Bad file descriptor"
     ]
    }
   ],
   "source": [
    "endmsg='end'\n",
    "data1=endmsg.encode('ascii')    \n",
    "sock_SnS.sendall(data1)\n",
    "\n",
    "\n",
    "#print('closing SnS socket')\n",
    "#sock_SnS.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "In /home/scott/.local/lib/python3.6/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The text.latex.preview rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /home/scott/.local/lib/python3.6/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The mathtext.fallback_to_cm rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /home/scott/.local/lib/python3.6/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: Support for setting the 'mathtext.fallback_to_cm' rcParam is deprecated since 3.3 and will be removed two minor releases later; use 'mathtext.fallback : 'cm' instead.\n",
      "In /home/scott/.local/lib/python3.6/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The validate_bool_maybe_none function was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /home/scott/.local/lib/python3.6/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The savefig.jpeg_quality rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /home/scott/.local/lib/python3.6/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The keymap.all_axes rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /home/scott/.local/lib/python3.6/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The animation.avconv_path rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /home/scott/.local/lib/python3.6/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The animation.avconv_args rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "/home/scott/.local/lib/python3.6/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/scott/.local/lib/python3.6/site-packages/stable_baselines/common/policies.py:346: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/scott/.local/lib/python3.6/site-packages/stable_baselines/common/policies.py:442: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.flatten instead.\n",
      "WARNING:tensorflow:From /home/scott/.local/lib/python3.6/site-packages/stable_baselines/common/tf_layers.py:121: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/scott/.local/lib/python3.6/site-packages/stable_baselines/common/tf_layers.py:123: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/scott/.local/lib/python3.6/site-packages/stable_baselines/common/distributions.py:326: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/scott/.local/lib/python3.6/site-packages/stable_baselines/common/distributions.py:327: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/scott/.local/lib/python3.6/site-packages/stable_baselines/ppo2/ppo2.py:190: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/scott/.local/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /home/scott/.local/lib/python3.6/site-packages/stable_baselines/ppo2/ppo2.py:206: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/scott/.local/lib/python3.6/site-packages/stable_baselines/ppo2/ppo2.py:242: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore',category=FutureWarning)\n",
    "import tensorflow as tf\n",
    "#import tensorflow.contrib.layers as layers\n",
    "import tensorflow as tf\n",
    "#from tensorflow import keras\n",
    "#from tensorflow.keras import layers\n",
    "#from keras.utils import to_categorical\n",
    "#from keras.models import Sequential\n",
    "#from keras.layers import Dense, Dropout, Embedding, LSTM, GlobalMaxPooling1D, SpatialDropout1D\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "\n",
    "import gym\n",
    "from stable_baselines import DQN,PPO2\n",
    "from stable_baselines import results_plotter\n",
    "from stable_baselines.bench import Monitor\n",
    "from stable_baselines.results_plotter import load_results, ts2xy\n",
    "from stable_baselines.common.noise import AdaptiveParamNoiseSpec\n",
    "from stable_baselines.common.callbacks import BaseCallback\n",
    "from stable_baselines.deepq.policies import MlpPolicy\n",
    "from stable_baselines.common.policies import FeedForwardPolicy, register_policy,LstmPolicy\n",
    "from stable_baselines.common.tf_layers import conv, linear, conv_to_fc, lstm\n",
    "#from stable_baselines.common import get_vec_normalize_env\n",
    "from stable_baselines.common.env_checker import check_env\n",
    "from stable_baselines.common.vec_env import DummyVecEnv\n",
    "#import balance_bot\n",
    "import MainEnv_RL\n",
    "\n",
    "env= gym.make(\"UR5-RL-env\", render=True)\n",
    "env = DummyVecEnv([lambda: env])\n",
    "\n",
    "#env=get_vec_normalize_env(env) \n",
    "#model = DQN.load(\"MainScott_RL\")\n",
    "model = PPO2.load(\"UR5-RL_savedpolicy\")\n",
    "#env=model.get_env()\n",
    "#obs = env.reset()\n",
    "done = [False for _ in range(1)] #env.num_envs\n",
    "state=None\n",
    "\n",
    "while True:\n",
    "    obs = env.reset()\n",
    "    #env._seed()\n",
    "    for i in range(80):\n",
    "        action, _states = model.predict(obs,state=state,mask=done)\n",
    "        #actionlist.append(action)\n",
    "        obs, rewards, dones, info = env.step(action)\n",
    "    print(rewards)\n",
    "        \n",
    "\"\"\"\n",
    "\n",
    "while True:\n",
    "    obs = env.reset()\n",
    "    env._seed()\n",
    "    actionlist=[]\n",
    "    #print(\"reset\")\n",
    "    for i in range(80):\n",
    "        action, _states = model.predict(obs)\n",
    "        actionlist.append(action)\n",
    "        obs, rewards, dones, info = env.step(action)\n",
    "    print(rewards)       \n",
    "\"\"\"       \n",
    "        \n",
    "        \n",
    "    #print(\"actionlist\",actionlist)\n",
    "    #env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"UR5-RL_savedpolicy-9-13-2021\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
