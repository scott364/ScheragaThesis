{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfor env in gym.envs.registry.env_specs:\\n     if 'MainEnvRL-v0' in env:\\n        print('Remove {} from registry'.format(env))\\n        del gym.registry.env_specs[env]\\n\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#To register MainEnvRL with local copy of Gym\n",
    "\"\"\"\n",
    "from gym.envs.registration import register\n",
    " \n",
    "register(\n",
    "    id='MainEnvRL-v2',\n",
    "    #entry_point='balance_bot.envs:BalancebotEnv',\n",
    "    entry_point='MainEnv_RL.envs:MainEnvRL',\n",
    ")\n",
    "\n",
    "\"\"\"\n",
    "#import gym\n",
    "#for env in gym.envs.registry.env_specs:\n",
    "#    print(env)\n",
    "    \n",
    "    #if 'MainEnvRL-v1' == env:\n",
    "    #     print('Remove {} from registry'.format(env))\n",
    "         #del gym.envs.registry.env_specs[env]\n",
    "            \n",
    "            \n",
    "#import MainEnv_RL\n",
    "\n",
    "#env = gym.make('MainEnvRL-v2')\n",
    "#env.reset()\n",
    "\"\"\"\n",
    "for env in gym.envs.registry.env_specs:\n",
    "     if 'MainEnvRL-v0' in env:\n",
    "        print('Remove {} from registry'.format(env))\n",
    "        del gym.registry.env_specs[env]\n",
    "\"\"\"                \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "connecting to 192.168.0.103 port 65498\n",
      "port= /dev/ttyACM0\n",
      "the red  robot is being used. Please change the bot identity variable if this is incorrect\n",
      "Connected to http://192.168.0.103:10000\n",
      "DC socket connecting to 192.168.0.103 port 65488\n",
      "TotalEpisodes: 700    StepsPerEpisode: 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/scott/anaconda3/envs/thesis/lib/python3.6/site-packages/stable_baselines/__init__.py:33: UserWarning: stable-baselines is in maintenance mode, please use [Stable-Baselines3 (SB3)](https://github.com/DLR-RM/stable-baselines3) for an up-to-date version. You can find a [migration guide](https://stable-baselines3.readthedocs.io/en/master/guide/migration.html) in SB3 documentation.\n",
      "  \"stable-baselines is in maintenance mode, please use [Stable-Baselines3 (SB3)](https://github.com/DLR-RM/stable-baselines3) for an up-to-date version. You can find a [migration guide](https://stable-baselines3.readthedocs.io/en/master/guide/migration.html) in SB3 documentation.\"\n",
      "/home/scott/anaconda3/envs/thesis/lib/python3.6/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total timesteps: 21000\n",
      "WARNING:tensorflow:From /home/scott/anaconda3/envs/thesis/lib/python3.6/site-packages/stable_baselines/common/tf_util.py:191: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/scott/anaconda3/envs/thesis/lib/python3.6/site-packages/stable_baselines/common/tf_util.py:200: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/scott/anaconda3/envs/thesis/lib/python3.6/site-packages/stable_baselines/common/policies.py:116: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/scott/anaconda3/envs/thesis/lib/python3.6/site-packages/stable_baselines/common/input.py:25: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/scott/anaconda3/envs/thesis/lib/python3.6/site-packages/stable_baselines/common/policies.py:442: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.flatten instead.\n",
      "WARNING:tensorflow:From /home/scott/anaconda3/envs/thesis/lib/python3.6/site-packages/tensorflow_core/python/layers/core.py:332: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n",
      "WARNING:tensorflow:From /home/scott/anaconda3/envs/thesis/lib/python3.6/site-packages/stable_baselines/common/tf_layers.py:123: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/scott/anaconda3/envs/thesis/lib/python3.6/site-packages/stable_baselines/common/distributions.py:326: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/scott/anaconda3/envs/thesis/lib/python3.6/site-packages/stable_baselines/common/distributions.py:327: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/scott/anaconda3/envs/thesis/lib/python3.6/site-packages/stable_baselines/ppo2/ppo2.py:190: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/scott/anaconda3/envs/thesis/lib/python3.6/site-packages/stable_baselines/ppo2/ppo2.py:198: The name tf.trainable_variables is deprecated. Please use tf.compat.v1.trainable_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/scott/anaconda3/envs/thesis/lib/python3.6/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /home/scott/anaconda3/envs/thesis/lib/python3.6/site-packages/stable_baselines/ppo2/ppo2.py:206: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/scott/anaconda3/envs/thesis/lib/python3.6/site-packages/stable_baselines/ppo2/ppo2.py:240: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/scott/anaconda3/envs/thesis/lib/python3.6/site-packages/stable_baselines/ppo2/ppo2.py:242: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n",
      "\n",
      "Ep: 1  tStep: 1 Z difference -0.0003295563708993754  Reward: -2.0003295563708994\n",
      "Ep: 1  tStep: 2 Z difference 0.0010457218177615246  Reward: -1.9989542781822385\n",
      "Ep: 1  tStep: 3 Z difference 0.00211579003408513  Reward: -1.9978842099659149\n",
      "Ep: 1  tStep: 4 Z difference 0.0032157779209311954  Reward: -1.9967842220790688\n",
      "Ep: 1  tStep: 5 Z difference 0.0033178567968308315  Reward: -1.9966821432031692\n",
      "Ep: 1  tStep: 6 Z difference 0.0019271787777541682  Reward: -1.9980728212222458\n",
      "Ep: 1  tStep: 7 Z difference 0.05330526633150878  Reward: -1.9466947336684912\n",
      "Ep: 1  tStep: 8 Z difference 0.05587117140889131  Reward: -1.9441288285911087\n",
      "Ep: 1  tStep: 9 Z difference 0.05418848327212  Reward: -1.94581151672788\n",
      "Ep: 1  tStep: 10 Z difference 0.10565456985682253  Reward: -1.8943454301431775\n",
      "Ep: 1  tStep: 11 Z difference 0.10715700664520256  Reward: -1.8928429933547974\n",
      "Ep: 1  tStep: 12 Z difference 0.10592736685276005  Reward: -1.89407263314724\n",
      "Ep: 1  tStep: 13 Z difference 0.10464859426803885  Reward: -1.8953514057319611\n",
      "Ep: 1  tStep: 14 Z difference 0.10283229426927853  Reward: -1.8971677057307215\n",
      "Ep: 1  tStep: 15 Z difference 0.15214416457638125  Reward: -1.8478558354236188\n",
      "Ep: 1  tStep: 16 Z difference 0.15429354090727854  Reward: -1.8457064590927215\n",
      "Ep: 1  tStep: 17 Z difference 0.15029882489740842  Reward: -1.8497011751025916\n",
      "Ep: 1  tStep: 18 Z difference 0.15017914621531947  Reward: -1.8498208537846805\n",
      "Ep: 1  tStep: 19 Z difference 0.15110504268594083  Reward: -1.8488949573140592\n",
      "Ep: 1  tStep: 20 Z difference 0.15158199743367717  Reward: -1.8484180025663228\n",
      "Ep: 1  tStep: 21 Z difference 0.1508516054768112  Reward: -1.8491483945231888\n",
      "Ep: 1  tStep: 22 Z difference 0.1954539143126457  Reward: -1.8045460856873543\n",
      "Ep: 1  tStep: 23 Z difference 0.20484282425455724  Reward: -1.7951571757454428\n",
      "Ep: 1  tStep: 24 Z difference 0.20332836093194784  Reward: -1.7966716390680522\n",
      "Ep: 1  tStep: 25 Z difference 0.2106072007767854  Reward: -1.7893927992232146\n",
      "Ep: 1  tStep: 26 Z difference 0.20940865397527775  Reward: -1.7905913460247223\n",
      "Ep: 1  tStep: 27 Z difference 0.21708510277494764  Reward: -1.7829148972250524\n",
      "Ep: 1  tStep: 28 Z difference 0.21921365266852044  Reward: -1.7807863473314796\n",
      "Ep: 1  tStep: 29 Z difference 0.21773174898736158  Reward: -1.7822682510126384\n",
      "Ep: 1  tStep: 30 Z difference 0.21791214700080452  Reward: -1.7820878529991955\n",
      "   Actionlist: [0, 2, 3, 1, 1, 3, 4, 2, 0, 4, 1, 3, 3, 3, 4, 0, 0, 1, 1, 1, 0, 4, 3, 0, 4, 1, 4, 2, 0, 3]\n",
      "-----------------------------------\n",
      "| approxkl           | 0.5344357  |\n",
      "| clipfrac           | 0.64166665 |\n",
      "| explained_variance | 0.0642     |\n",
      "| fps                | 0          |\n",
      "| n_updates          | 1          |\n",
      "| policy_entropy     | 1.2987467  |\n",
      "| policy_loss        | 0.14940467 |\n",
      "| serial_timesteps   | 30         |\n",
      "| time_elapsed       | 1.45e-05   |\n",
      "| total_timesteps    | 30         |\n",
      "| value_loss         | 166.51543  |\n",
      "-----------------------------------\n",
      "Ep: 2  tStep: 1 Z difference 0.05004006228819513  Reward: -1.9499599377118049\n",
      "Ep: 2  tStep: 2 Z difference 0.053857313585654154  Reward: -1.9461426864143458\n",
      "Ep: 2  tStep: 3 Z difference 0.0473942247577015  Reward: -1.9526057752422985\n",
      "Ep: 2  tStep: 4 Z difference 0.053476571111753746  Reward: -1.9465234288882463\n",
      "Ep: 2  tStep: 5 Z difference 0.10232747316174207  Reward: -1.897672526838258\n",
      "Ep: 2  tStep: 6 Z difference 0.15456560457795865  Reward: -1.8454343954220414\n",
      "Ep: 2  tStep: 7 Z difference 0.15137769301682713  Reward: -1.8486223069831729\n",
      "Ep: 2  tStep: 8 Z difference 0.15417958216220162  Reward: -1.8458204178377984\n",
      "Ep: 2  tStep: 9 Z difference 0.2010686924822629  Reward: -1.798931307517737\n",
      "Ep: 2  tStep: 10 Z difference 0.2505526008948684  Reward: -1.7494473991051316\n",
      "Ep: 2  tStep: 11 Z difference 0.25602760727033047  Reward: -1.7439723927296695\n",
      "Ep: 2  tStep: 12 Z difference 0.25371044612042626  Reward: -1.7462895538795737\n",
      "Ep: 2  tStep: 13 Z difference 0.25493069934956747  Reward: -1.7450693006504325\n",
      "Ep: 2  tStep: 14 Z difference 0.2567546259310096  Reward: -1.7432453740689904\n",
      "Ep: 2  tStep: 15 Z difference 0.3042281964816156  Reward: -1.6957718035183844\n",
      "Ep: 2  tStep: 16 Z difference 0.30682020793817966  Reward: -1.6931797920618203\n",
      "Ep: 2  tStep: 17 Z difference 0.3543933640588075  Reward: -1.6456066359411925\n",
      "Ep: 2  tStep: 18 Z difference 0.35755839587189264  Reward: -1.6424416041281074\n",
      "Ep: 2  tStep: 19 Z difference 0.35474976013414583  Reward: -1.6452502398658542\n",
      "Ep: 2  tStep: 20 Z difference 0.3549498112645  Reward: -1.6450501887355\n",
      "Ep: 2  tStep: 21 Z difference 0.4044766925372185  Reward: -1.5955233074627815\n",
      "Ep: 2  tStep: 22 Z difference 0.4580506492432208  Reward: -1.5419493507567792\n",
      "Ep: 2  tStep: 23 Z difference 0.4579875832710416  Reward: -1.5420124167289584\n",
      "Ep: 2  tStep: 24 Z difference 0.4547021394506099  Reward: -1.5452978605493901\n",
      "Ep: 2  tStep: 25 Z difference 0.45648015987090784  Reward: -1.5435198401290922\n",
      "Ep: 2  tStep: 26 Z difference 0.4556770220484585  Reward: -1.5443229779515415\n",
      "Ep: 2  tStep: 27 Z difference 0.45729869752377295  Reward: -1.542701302476227\n",
      "Ep: 2  tStep: 28 Z difference 0.4539441744640471  Reward: -1.5460558255359529\n",
      "Ep: 2  tStep: 29 Z difference 0.5037542659513654  Reward: -1.4962457340486346\n",
      "Ep: 2  tStep: 30 Z difference 0.5080911515265705  Reward: -1.4919088484734295\n",
      "   Actionlist: [4, 3, 0, 3, 4, 4, 2, 3, 4, 4, 3, 2, 0, 3, 4, 3, 4, 3, 0, 2, 4, 4, 3, 0, 3, 2, 3, 2, 4, 2]\n",
      "------------------------------------\n",
      "| approxkl           | 0.06776563  |\n",
      "| clipfrac           | 0.57500005  |\n",
      "| explained_variance | -0.00277    |\n",
      "| fps                | 0           |\n",
      "| n_updates          | 2           |\n",
      "| policy_entropy     | 1.3606454   |\n",
      "| policy_loss        | 0.031489357 |\n",
      "| serial_timesteps   | 60          |\n",
      "| time_elapsed       | 63.8        |\n",
      "| total_timesteps    | 60          |\n",
      "| value_loss         | 126.14808   |\n",
      "------------------------------------\n",
      "Ep: 3  tStep: 1 Z difference 0.05258382094278957  Reward: -1.9474161790572104\n",
      "Ep: 3  tStep: 2 Z difference 0.05004944885149598  Reward: -1.949950551148504\n",
      "Ep: 3  tStep: 3 Z difference 0.05023703345246622  Reward: -1.9497629665475338\n",
      "Ep: 3  tStep: 4 Z difference 0.04972766572833054  Reward: -1.9502723342716695\n",
      "Ep: 3  tStep: 5 Z difference 0.0456006578419359  Reward: -1.954399342158064\n",
      "Ep: 3  tStep: 6 Z difference 0.05034776556640885  Reward: -1.9496522344335911\n",
      "Ep: 3  tStep: 7 Z difference 0.09745687346383924  Reward: -1.9025431265361608\n",
      "Ep: 3  tStep: 8 Z difference 0.10078749012015775  Reward: -1.8992125098798422\n",
      "Ep: 3  tStep: 9 Z difference 0.10070345104560285  Reward: -1.8992965489543971\n",
      "Ep: 3  tStep: 10 Z difference 0.10097346140556063  Reward: -1.8990265385944394\n",
      "Ep: 3  tStep: 11 Z difference 0.10032036193087723  Reward: -1.8996796380691228\n",
      "Ep: 3  tStep: 12 Z difference 0.14768393369279798  Reward: -1.852316066307202\n",
      "Ep: 3  tStep: 13 Z difference 0.15275781115219011  Reward: -1.8472421888478099\n",
      "Ep: 3  tStep: 14 Z difference 0.14954628651775437  Reward: -1.8504537134822456\n",
      "Ep: 3  tStep: 15 Z difference 0.14950126034691946  Reward: -1.8504987396530805\n",
      "Ep: 3  tStep: 16 Z difference 0.19152373092547093  Reward: -1.808476269074529\n",
      "Ep: 3  tStep: 17 Z difference 0.20269784787520795  Reward: -1.797302152124792\n",
      "Ep: 3  tStep: 18 Z difference 0.20507778166718804  Reward: -1.794922218332812\n",
      "Ep: 3  tStep: 19 Z difference 0.2133350240711125  Reward: -1.7866649759288875\n",
      "Ep: 3  tStep: 20 Z difference 0.21435991945154997  Reward: -1.78564008054845\n",
      "Ep: 3  tStep: 21 Z difference 0.21856964642703547  Reward: -1.7814303535729645\n",
      "Ep: 3  tStep: 22 Z difference 0.2192781852912158  Reward: -1.7807218147087842\n",
      "Ep: 3  tStep: 23 Z difference 0.216542442084104  Reward: -1.783457557915896\n",
      "Ep: 3  tStep: 24 Z difference 0.22005565672963856  Reward: -1.7799443432703614\n",
      "Ep: 3  tStep: 25 Z difference 0.2188094437863679  Reward: -1.781190556213632\n",
      "Ep: 3  tStep: 26 Z difference 0.21935973105989426  Reward: -1.7806402689401057\n",
      "Ep: 3  tStep: 27 Z difference 0.21996355107724685  Reward: -1.7800364489227531\n",
      "Ep: 3  tStep: 28 Z difference 0.22430996988080443  Reward: -1.7756900301191956\n",
      "Ep: 3  tStep: 29 Z difference 0.22465316610150055  Reward: -1.7753468338984995\n",
      "Ep: 3  tStep: 30 Z difference 0.22307035686485488  Reward: -1.7769296431351451\n",
      "   Actionlist: [4, 0, 1, 2, 0, 3, 4, 0, 2, 3, 3, 4, 2, 1, 3, 4, 3, 4, 4, 2, 4, 3, 3, 2, 3, 0, 0, 4, 3, 1]\n",
      "------------------------------------\n",
      "| approxkl           | 0.016837772 |\n",
      "| clipfrac           | 0.31666666  |\n",
      "| explained_variance | 0.000262    |\n",
      "| fps                | 0           |\n",
      "| n_updates          | 3           |\n",
      "| policy_entropy     | 1.3321909   |\n",
      "| policy_loss        | 0.006015587 |\n",
      "| serial_timesteps   | 90          |\n",
      "| time_elapsed       | 105         |\n",
      "| total_timesteps    | 90          |\n",
      "| value_loss         | 142.84137   |\n",
      "------------------------------------\n",
      "Ep: 4  tStep: 1 Z difference 0.0007644182488322215  Reward: -1.9992355817511678\n",
      "Ep: 4  tStep: 2 Z difference -0.0008720703966917931  Reward: -2.000872070396692\n",
      "Ep: 4  tStep: 3 Z difference -0.0007097121845931476  Reward: -2.000709712184593\n",
      "Ep: 4  tStep: 4 Z difference 0.0006780325334516668  Reward: -1.9993219674665483\n",
      "Ep: 4  tStep: 5 Z difference 0.0016702216073869458  Reward: -1.998329778392613\n",
      "Ep: 4  tStep: 6 Z difference 0.04847030624113957  Reward: -1.9515296937588604\n",
      "Ep: 4  tStep: 7 Z difference 0.05640561885684692  Reward: -1.943594381143153\n",
      "Ep: 4  tStep: 8 Z difference 0.10052847963906819  Reward: -1.8994715203609318\n",
      "Ep: 4  tStep: 9 Z difference 0.10351076679788518  Reward: -1.8964892332021148\n",
      "Ep: 4  tStep: 10 Z difference 0.10192546425536264  Reward: -1.8980745357446374\n",
      "Ep: 4  tStep: 11 Z difference 0.1019907302033154  Reward: -1.8980092697966846\n",
      "Ep: 4  tStep: 12 Z difference 0.10271716220378879  Reward: -1.8972828377962112\n",
      "Ep: 4  tStep: 13 Z difference 0.10339959468878801  Reward: -1.896600405311212\n",
      "Ep: 4  tStep: 14 Z difference 0.10479511265456676  Reward: -1.8952048873454332\n",
      "Ep: 4  tStep: 15 Z difference 0.10455678194575002  Reward: -1.89544321805425\n",
      "Ep: 4  tStep: 16 Z difference 0.10395809520520238  Reward: -1.8960419047947976\n",
      "Ep: 4  tStep: 17 Z difference 0.10618417735807562  Reward: -1.8938158226419244\n",
      "Ep: 4  tStep: 18 Z difference 0.10264426967315377  Reward: -1.8973557303268462\n",
      "Ep: 4  tStep: 19 Z difference 0.15447173894494748  Reward: -1.8455282610550525\n",
      "Ep: 4  tStep: 20 Z difference 0.1559583359077572  Reward: -1.8440416640922428\n",
      "Ep: 4  tStep: 21 Z difference 0.191139908485487  Reward: -1.808860091514513\n",
      "Ep: 4  tStep: 22 Z difference 0.19443209889829127  Reward: -1.8055679011017087\n",
      "Ep: 4  tStep: 23 Z difference 0.1924332009103149  Reward: -1.807566799089685\n",
      "Ep: 4  tStep: 24 Z difference 0.19281526336967936  Reward: -1.8071847366303206\n",
      "Ep: 4  tStep: 25 Z difference 0.19306488728746762  Reward: -1.8069351127125324\n",
      "Ep: 4  tStep: 26 Z difference 0.1930421542044729  Reward: -1.806957845795527\n",
      "Ep: 4  tStep: 27 Z difference 0.1917224620703606  Reward: -1.8082775379296394\n",
      "Ep: 4  tStep: 28 Z difference 0.1921370841711756  Reward: -1.8078629158288244\n",
      "Ep: 4  tStep: 29 Z difference 0.19255317292250673  Reward: -100\n",
      "Ep: 4  tStep: 30 Z difference 0.19381639901176095  Reward: -100\n",
      "   Actionlist: [3, 3, 3, 0, 2, 4, 3, 4, 3, 2, 3, 0, 1, 3, 3, 3, 3, 3, 4, 3, 4, 3, 3, 2, 3, 0, 3, 3, 3, 3]\n",
      "-------------------------------------\n",
      "| approxkl           | 0.025978912  |\n",
      "| clipfrac           | 0.24166666   |\n",
      "| explained_variance | -0.000107    |\n",
      "| fps                | 0            |\n",
      "| n_updates          | 4            |\n",
      "| policy_entropy     | 1.3236015    |\n",
      "| policy_loss        | 0.0068420805 |\n",
      "| serial_timesteps   | 120          |\n",
      "| time_elapsed       | 147          |\n",
      "| total_timesteps    | 120          |\n",
      "| value_loss         | 6440.177     |\n",
      "-------------------------------------\n",
      "Ep: 5  tStep: 1 Z difference 0.0016105289313941284  Reward: -1.9983894710686059\n",
      "Ep: 5  tStep: 2 Z difference -0.0026795704923570085  Reward: -2.002679570492357\n",
      "Ep: 5  tStep: 3 Z difference 0.048715823537483605  Reward: -1.9512841764625164\n",
      "Ep: 5  tStep: 4 Z difference 0.05064857558719815  Reward: -1.9493514244128018\n",
      "Ep: 5  tStep: 5 Z difference 0.05064329564534109  Reward: -1.949356704354659\n",
      "Ep: 5  tStep: 6 Z difference 0.05282171165645089  Reward: -1.9471782883435491\n",
      "Ep: 5  tStep: 7 Z difference 0.05005018217675383  Reward: -1.9499498178232462\n",
      "Ep: 5  tStep: 8 Z difference 0.10148752241134629  Reward: -1.8985124775886537\n",
      "Ep: 5  tStep: 9 Z difference 0.1516591432508081  Reward: -1.848340856749192\n",
      "Ep: 5  tStep: 10 Z difference 0.15485658804029212  Reward: -1.8451434119597079\n",
      "Ep: 5  tStep: 11 Z difference 0.15157627749666558  Reward: -1.8484237225033344\n",
      "Ep: 5  tStep: 12 Z difference 0.15088607176393243  Reward: -1.8491139282360676\n",
      "Ep: 5  tStep: 13 Z difference 0.15054463552385577  Reward: -1.8494553644761442\n",
      "Ep: 5  tStep: 14 Z difference 0.19301414117962112  Reward: -1.8069858588203789\n",
      "Ep: 5  tStep: 15 Z difference 0.20323918858058754  Reward: -1.7967608114194125\n",
      "Ep: 5  tStep: 16 Z difference 0.20648019289039077  Reward: -1.7935198071096092\n",
      "Ep: 5  tStep: 17 Z difference 0.2134096765823661  Reward: -1.786590323417634\n",
      "Ep: 5  tStep: 18 Z difference 0.2124196874842048  Reward: -1.7875803125157952\n",
      "Ep: 5  tStep: 19 Z difference 0.2145379708241668  Reward: -1.7854620291758332\n",
      "Ep: 5  tStep: 20 Z difference 0.21221391641683862  Reward: -1.7877860835831614\n",
      "Ep: 5  tStep: 21 Z difference 0.21686759850345538  Reward: -1.7831324014965446\n",
      "Ep: 5  tStep: 22 Z difference 0.218071278581768  Reward: -1.781928721418232\n",
      "Ep: 5  tStep: 23 Z difference 0.22242385731749215  Reward: -1.7775761426825079\n",
      "Ep: 5  tStep: 24 Z difference 0.22198400882780556  Reward: -1.7780159911721944\n",
      "Ep: 5  tStep: 25 Z difference 0.22856750966310502  Reward: -1.771432490336895\n",
      "Ep: 5  tStep: 26 Z difference 0.22835968528501693  Reward: -1.771640314714983\n",
      "Ep: 5  tStep: 27 Z difference 0.23023714461028577  Reward: -1.7697628553897142\n",
      "Ep: 5  tStep: 28 Z difference 0.22984408227205266  Reward: -1.7701559177279473\n",
      "Ep: 5  tStep: 29 Z difference 0.22963420458324224  Reward: -1.7703657954167578\n",
      "Ep: 5  tStep: 30 Z difference 0.22832463233768907  Reward: -1.771675367662311\n",
      "   Actionlist: [3, 0, 4, 1, 3, 3, 3, 4, 4, 2, 1, 1, 0, 4, 2, 4, 4, 0, 3, 0, 4, 0, 4, 1, 4, 3, 1, 0, 3, 3]\n",
      "--------------------------------------\n",
      "| approxkl           | 0.018701598   |\n",
      "| clipfrac           | 0.34166667    |\n",
      "| explained_variance | 0.00359       |\n",
      "| fps                | 0             |\n",
      "| n_updates          | 5             |\n",
      "| policy_entropy     | 1.4599478     |\n",
      "| policy_loss        | -0.0015596867 |\n",
      "| serial_timesteps   | 150           |\n",
      "| time_elapsed       | 189           |\n",
      "| total_timesteps    | 150           |\n",
      "| value_loss         | 130.71207     |\n",
      "--------------------------------------\n",
      "Ep: 6  tStep: 1 Z difference 0.001368384931236477  Reward: -1.9986316150687635\n",
      "Ep: 6  tStep: 2 Z difference 0.050055462118610894  Reward: -1.949944537881389\n",
      "Ep: 6  tStep: 3 Z difference 0.1006815979529172  Reward: -1.8993184020470828\n",
      "Ep: 6  tStep: 4 Z difference 0.10013879059702147  Reward: -1.8998612094029785\n",
      "Ep: 6  tStep: 5 Z difference 0.14956007303260277  Reward: -1.8504399269673972\n",
      "Ep: 6  tStep: 6 Z difference 0.15127986742742383  Reward: -1.8487201325725762\n",
      "Ep: 6  tStep: 7 Z difference 0.1518207681376489  Reward: -1.8481792318623511\n",
      "Ep: 6  tStep: 8 Z difference 0.197918180509284  Reward: -1.802081819490716\n",
      "Ep: 6  tStep: 9 Z difference 0.2050952348083257  Reward: -1.7949047651916743\n",
      "Ep: 6  tStep: 10 Z difference 0.20466418622173377  Reward: -1.7953358137782662\n",
      "Ep: 6  tStep: 11 Z difference 0.2060963704504073  Reward: -1.7939036295495927\n",
      "Ep: 6  tStep: 12 Z difference 0.20752752802371965  Reward: -1.7924724719762803\n",
      "Ep: 6  tStep: 13 Z difference 0.2112080874931066  Reward: -1.7887919125068934\n",
      "Ep: 6  tStep: 14 Z difference 0.21413141530118907  Reward: -1.785868584698811\n",
      "Ep: 6  tStep: 15 Z difference 0.21153691053874812  Reward: -1.7884630894612519\n",
      "Ep: 6  tStep: 16 Z difference 0.2112913932424041  Reward: -1.788708606757596\n",
      "Ep: 6  tStep: 17 Z difference 0.21235823482759297  Reward: -1.787641765172407\n",
      "Ep: 6  tStep: 18 Z difference 0.21755721757598234  Reward: -1.7824427824240177\n",
      "Ep: 6  tStep: 19 Z difference 0.21716562188826494  Reward: -1.782834378111735\n",
      "Ep: 6  tStep: 20 Z difference 0.21505540512613974  Reward: -1.7849445948738603\n",
      "Ep: 6  tStep: 21 Z difference 0.21883129687905312  Reward: -1.7811687031209469\n",
      "Ep: 6  tStep: 22 Z difference 0.2225400160383435  Reward: -1.7774599839616565\n",
      "Ep: 6  tStep: 23 Z difference 0.22307153018526726  Reward: -1.7769284698147327\n",
      "Ep: 6  tStep: 24 Z difference 0.2235374850541354  Reward: -1.7764625149458646\n",
      "Ep: 6  tStep: 25 Z difference 0.22409613223560143  Reward: -1.7759038677643986\n",
      "Ep: 6  tStep: 26 Z difference 0.2223138585288078  Reward: -1.7776861414711922\n",
      "Ep: 6  tStep: 27 Z difference 0.22927546186707914  Reward: -1.7707245381329209\n",
      "Ep: 6  tStep: 28 Z difference 0.2302361179549246  Reward: -1.7697638820450754\n",
      "Ep: 6  tStep: 29 Z difference 0.2281201812557878  Reward: -1.7718798187442122\n",
      "Ep: 6  tStep: 30 Z difference 0.2293771007478238  Reward: -1.7706228992521762\n",
      "   Actionlist: [3, 4, 4, 3, 4, 1, 0, 4, 1, 0, 2, 0, 4, 2, 1, 3, 0, 4, 4, 3, 4, 4, 3, 1, 3, 3, 4, 3, 3, 2]\n",
      "------------------------------------\n",
      "| approxkl           | 0.049087532 |\n",
      "| clipfrac           | 0.47500002  |\n",
      "| explained_variance | -0.000138   |\n",
      "| fps                | 0           |\n",
      "| n_updates          | 6           |\n",
      "| policy_entropy     | 1.2953348   |\n",
      "| policy_loss        | 0.056901164 |\n",
      "| serial_timesteps   | 180         |\n",
      "| time_elapsed       | 230         |\n",
      "| total_timesteps    | 180         |\n",
      "| value_loss         | 124.52614   |\n",
      "------------------------------------\n",
      "Ep: 7  tStep: 1 Z difference 0.04974761217534551  Reward: -1.9502523878246545\n",
      "Ep: 7  tStep: 2 Z difference 0.051316194901988244  Reward: -1.9486838050980118\n",
      "Ep: 7  tStep: 3 Z difference 0.047749447512627  Reward: -1.952250552487373\n",
      "Ep: 7  tStep: 4 Z difference 0.09802520053870989  Reward: -1.90197479946129\n",
      "Ep: 7  tStep: 5 Z difference 0.09884711148776137  Reward: -1.9011528885122386\n",
      "Ep: 7  tStep: 6 Z difference 0.09774023034349089  Reward: -1.9022597696565091\n",
      "Ep: 7  tStep: 7 Z difference 0.14273164156116547  Reward: -1.8572683584388345\n",
      "Ep: 7  tStep: 8 Z difference 0.18993490842171035  Reward: -1.8100650915782897\n",
      "Ep: 7  tStep: 9 Z difference 0.19618621291518235  Reward: -1.8038137870848177\n",
      "Ep: 7  tStep: 10 Z difference 0.19281379671916365  Reward: -1.8071862032808363\n",
      "Ep: 7  tStep: 11 Z difference 0.2048442909050734  Reward: -1.7951557090949266\n",
      "Ep: 7  tStep: 12 Z difference 0.20098920002430676  Reward: -1.7990107999756932\n",
      "Ep: 7  tStep: 13 Z difference 0.20330709449946882  Reward: -1.7966929055005312\n",
      "Ep: 7  tStep: 14 Z difference 0.20273583412356677  Reward: -1.7972641658764332\n",
      "Ep: 7  tStep: 15 Z difference 0.20158055351227544  Reward: -1.7984194464877246\n",
      "Ep: 7  tStep: 16 Z difference 0.20911371055655215  Reward: -1.7908862894434479\n",
      "Ep: 7  tStep: 17 Z difference 0.21431254663988986  Reward: -1.7856874533601101\n",
      "Ep: 7  tStep: 18 Z difference 0.21469915571585307  Reward: -1.785300844284147\n",
      "Ep: 7  tStep: 19 Z difference 0.2137295530598613  Reward: -1.7862704469401387\n",
      "Ep: 7  tStep: 20 Z difference 0.214331319766492  Reward: -1.785668680233508\n",
      "Ep: 7  tStep: 21 Z difference 0.21330730437636403  Reward: -1.786692695623636\n",
      "Ep: 7  tStep: 22 Z difference 0.2131865990389139  Reward: -1.786813400961086\n",
      "Ep: 7  tStep: 23 Z difference 0.21344091623835304  Reward: -1.786559083761647\n",
      "Ep: 7  tStep: 24 Z difference 0.21428937356174016  Reward: -1.7857106264382598\n",
      "Ep: 7  tStep: 25 Z difference 0.2153801215503366  Reward: -1.7846198784496634\n",
      "Ep: 7  tStep: 26 Z difference 0.21854735333919528  Reward: -1.7814526466608047\n",
      "Ep: 7  tStep: 27 Z difference 0.219943311300129  Reward: -1.780056688699871\n",
      "Ep: 7  tStep: 28 Z difference 0.2189217892158779  Reward: -1.781078210784122\n",
      "Ep: 7  tStep: 29 Z difference 0.22820730029642577  Reward: -1.7717926997035742\n",
      "Ep: 7  tStep: 30 Z difference 0.22580287344083194  Reward: -1.774197126559168\n",
      "   Actionlist: [4, 3, 3, 4, 3, 3, 4, 4, 2, 3, 4, 3, 3, 3, 3, 4, 4, 0, 3, 2, 0, 3, 3, 3, 3, 4, 2, 3, 4, 1]\n",
      "-----------------------------------\n",
      "| approxkl           | 0.08329657 |\n",
      "| clipfrac           | 0.7        |\n",
      "| explained_variance | -5.32e-05  |\n",
      "| fps                | 0          |\n",
      "| n_updates          | 7          |\n",
      "| policy_entropy     | 1.0596372  |\n",
      "| policy_loss        | 0.04198432 |\n",
      "| serial_timesteps   | 210        |\n",
      "| time_elapsed       | 272        |\n",
      "| total_timesteps    | 210        |\n",
      "| value_loss         | 120.95242  |\n",
      "-----------------------------------\n",
      "Ep: 8  tStep: 1 Z difference 0.0022379620220513985  Reward: -1.9977620379779486\n",
      "Ep: 8  tStep: 2 Z difference 0.002404426855593833  Reward: -1.9975955731444062\n",
      "Ep: 8  tStep: 3 Z difference 0.0019525518316774182  Reward: -1.9980474481683226\n",
      "Ep: 8  tStep: 4 Z difference 0.0025373053923249955  Reward: -1.997462694607675\n",
      "Ep: 8  tStep: 5 Z difference 0.0018179133143276793  Reward: -1.9981820866856723\n",
      "Ep: 8  tStep: 6 Z difference -0.0007710181761533264  Reward: -2.0007710181761533\n",
      "Ep: 8  tStep: 7 Z difference 0.0030927259426563047  Reward: -1.9969072740573437\n",
      "Ep: 8  tStep: 8 Z difference -0.0001930112078785129  Reward: -2.0001930112078785\n",
      "Ep: 8  tStep: 9 Z difference 0.00516891641281525  Reward: -1.9948310835871848\n",
      "Ep: 8  tStep: 10 Z difference 0.002061817295104351  Reward: -1.9979381827048956\n",
      "Ep: 8  tStep: 11 Z difference 0.05415196367427688  Reward: -1.9458480363257231\n",
      "Ep: 8  tStep: 12 Z difference 0.10622583023272458  Reward: -1.8937741697672754\n",
      "Ep: 8  tStep: 13 Z difference 0.10668459851406498  Reward: -1.893315401485935\n",
      "Ep: 8  tStep: 14 Z difference 0.1049994170714168  Reward: -1.8950005829285832\n",
      "Ep: 8  tStep: 15 Z difference 0.10614003117755066  Reward: -1.8938599688224493\n",
      "Ep: 8  tStep: 16 Z difference 0.10556481084525604  Reward: -1.894435189154744\n",
      "Ep: 8  tStep: 17 Z difference 0.10593059348389522  Reward: -1.8940694065161048\n",
      "Ep: 8  tStep: 18 Z difference 0.10550819813534629  Reward: -1.8944918018646537\n",
      "Ep: 8  tStep: 19 Z difference 0.15469041653685256  Reward: -1.8453095834631474\n",
      "Ep: 8  tStep: 20 Z difference 0.15613242732398236  Reward: -1.8438675726760176\n",
      "Ep: 8  tStep: 21 Z difference 0.18870570862442237  Reward: -1.8112942913755776\n",
      "Ep: 8  tStep: 22 Z difference 0.20050271204821746  Reward: -1.7994972879517825\n",
      "Ep: 8  tStep: 23 Z difference 0.20641184697635495  Reward: -1.793588153023645\n",
      "Ep: 8  tStep: 24 Z difference 0.20731178373284642  Reward: -1.7926882162671536\n",
      "Ep: 8  tStep: 25 Z difference 0.21135152591355144  Reward: -1.7886484740864486\n",
      "Ep: 8  tStep: 26 Z difference 0.21048062883727248  Reward: -1.7895193711627275\n",
      "Ep: 8  tStep: 27 Z difference 0.2112444604258985  Reward: -1.7887555395741015\n",
      "Ep: 8  tStep: 28 Z difference 0.2114198718275877  Reward: -1.7885801281724123\n",
      "Ep: 8  tStep: 29 Z difference 0.21137968560345488  Reward: -1.7886203143965451\n",
      "Ep: 8  tStep: 30 Z difference 0.21663528106175356  Reward: -1.7833647189382464\n",
      "   Actionlist: [3, 3, 2, 3, 3, 3, 0, 3, 0, 3, 4, 4, 3, 3, 1, 3, 1, 3, 4, 3, 4, 4, 4, 0, 4, 2, 2, 3, 2, 4]\n",
      "-------------------------------------\n",
      "| approxkl           | 0.073547915  |\n",
      "| clipfrac           | 0.75         |\n",
      "| explained_variance | -7.87e-06    |\n",
      "| fps                | 0            |\n",
      "| n_updates          | 8            |\n",
      "| policy_entropy     | 1.4498636    |\n",
      "| policy_loss        | -0.032494497 |\n",
      "| serial_timesteps   | 240          |\n",
      "| time_elapsed       | 316          |\n",
      "| total_timesteps    | 240          |\n",
      "| value_loss         | 131.86493    |\n",
      "-------------------------------------\n",
      "Ep: 9  tStep: 1 Z difference 0.003353056409209909  Reward: -1.99664694359079\n",
      "Ep: 9  tStep: 2 Z difference 0.0008046044729650426  Reward: -1.999195395527035\n",
      "Ep: 9  tStep: 3 Z difference 0.0014889436036349402  Reward: -1.998511056396365\n",
      "Ep: 9  tStep: 4 Z difference 0.054261229137703815  Reward: -1.9457387708622962\n",
      "Ep: 9  tStep: 5 Z difference 0.05966788959912961  Reward: -1.9403321104008704\n",
      "Ep: 9  tStep: 6 Z difference 0.10355989959016432  Reward: -1.8964401004098357\n",
      "Ep: 9  tStep: 7 Z difference 0.10812602264098814  Reward: -1.8918739773590119\n",
      "Ep: 9  tStep: 8 Z difference 0.15504549262672684  Reward: -1.8449545073732732\n",
      "Ep: 9  tStep: 9 Z difference 0.15725558828897768  Reward: -1.8427444117110223\n",
      "Ep: 9  tStep: 10 Z difference 0.1561902133543045  Reward: -1.8438097866456955\n",
      "Ep: 9  tStep: 11 Z difference 0.15743070636056355  Reward: -1.8425692936394364\n",
      "Ep: 9  tStep: 12 Z difference 0.1566478083152325  Reward: -1.8433521916847675\n",
      "Ep: 9  tStep: 13 Z difference 0.1561142408575864  Reward: -1.8438857591424136\n",
      "Ep: 9  tStep: 14 Z difference 0.19925253914855423  Reward: -1.8007474608514458\n",
      "Ep: 9  tStep: 15 Z difference 0.20917164325192594  Reward: -1.790828356748074\n",
      "Ep: 9  tStep: 16 Z difference 0.2075732875198124  Reward: -1.7924267124801876\n",
      "Ep: 9  tStep: 17 Z difference 0.20889767293557515  Reward: -1.7911023270644248\n",
      "Ep: 9  tStep: 18 Z difference 0.20911928382851208  Reward: -1.790880716171488\n",
      "Ep: 9  tStep: 19 Z difference 0.209103590667993  Reward: -1.790896409332007\n",
      "Ep: 9  tStep: 20 Z difference 0.2171907016120853  Reward: -1.7828092983879147\n",
      "Ep: 9  tStep: 21 Z difference 0.21569735805690282  Reward: -1.7843026419430972\n",
      "Ep: 9  tStep: 22 Z difference 0.21619132595062274  Reward: -1.7838086740493773\n",
      "Ep: 9  tStep: 23 Z difference 0.21988332529403287  Reward: -1.7801166747059671\n",
      "Ep: 9  tStep: 24 Z difference 0.21978197974339153  Reward: -1.7802180202566085\n",
      "Ep: 9  tStep: 25 Z difference 0.21971847377605735  Reward: -1.7802815262239426\n",
      "Ep: 9  tStep: 26 Z difference 0.21900993491187704  Reward: -1.780990065088123\n",
      "Ep: 9  tStep: 27 Z difference 0.21627873832136402  Reward: -1.783721261678636\n",
      "Ep: 9  tStep: 28 Z difference 0.22206218130029765  Reward: -1.7779378186997024\n",
      "Ep: 9  tStep: 29 Z difference 0.22646095952726908  Reward: -1.773539040472731\n",
      "Ep: 9  tStep: 30 Z difference 0.22545102398209282  Reward: -1.7745489760179072\n",
      "   Actionlist: [2, 0, 2, 4, 1, 4, 3, 4, 0, 0, 2, 3, 3, 4, 2, 0, 3, 0, 3, 4, 3, 3, 4, 3, 2, 3, 3, 4, 4, 0]\n",
      "-------------------------------------\n",
      "| approxkl           | 0.02192859   |\n",
      "| clipfrac           | 0.23333333   |\n",
      "| explained_variance | -7.03e-06    |\n",
      "| fps                | 0            |\n",
      "| n_updates          | 9            |\n",
      "| policy_entropy     | 1.4883921    |\n",
      "| policy_loss        | -0.011377652 |\n",
      "| serial_timesteps   | 270          |\n",
      "| time_elapsed       | 357          |\n",
      "| total_timesteps    | 270          |\n",
      "| value_loss         | 120.25928    |\n",
      "-------------------------------------\n",
      "Ep: 10  tStep: 1 Z difference -0.0009355763640255255  Reward: -2.0009355763640255\n",
      "Ep: 10  tStep: 2 Z difference -0.0004420484654605872  Reward: -2.0004420484654606\n",
      "Ep: 10  tStep: 3 Z difference 0.04693413649089617  Reward: -1.9530658635091038\n",
      "Ep: 10  tStep: 4 Z difference 0.04736914503388112  Reward: -1.9526308549661189\n",
      "Ep: 10  tStep: 5 Z difference 0.0971009173836559  Reward: -1.902899082616344\n",
      "Ep: 10  tStep: 6 Z difference 0.09939461212530709  Reward: -1.900605387874693\n",
      "Ep: 10  tStep: 7 Z difference 0.0988510714441535  Reward: -1.9011489285558465\n",
      "Ep: 10  tStep: 8 Z difference 0.14602852525562016  Reward: -1.8539714747443798\n",
      "Ep: 10  tStep: 9 Z difference 0.14930326252728676  Reward: -1.8506967374727132\n",
      "Ep: 10  tStep: 10 Z difference 0.14219748744331273  Reward: -1.8578025125566873\n",
      "Ep: 10  tStep: 11 Z difference 0.19143558522947135  Reward: -1.8085644147705286\n",
      "Ep: 10  tStep: 12 Z difference 0.19849516082219765  Reward: -1.8015048391778024\n",
      "Ep: 10  tStep: 13 Z difference 0.19565704540908335  Reward: -1.8043429545909166\n",
      "Ep: 10  tStep: 14 Z difference 0.1961913461919873  Reward: -1.8038086538080127\n",
      "Ep: 10  tStep: 15 Z difference 0.19810297847427405  Reward: -1.801897021525726\n",
      "Ep: 10  tStep: 16 Z difference 0.19605641434453425  Reward: -1.8039435856554658\n",
      "Ep: 10  tStep: 17 Z difference 0.19533936890736214  Reward: -1.8046606310926379\n",
      "Ep: 10  tStep: 18 Z difference 0.20144078171811985  Reward: -1.7985592182818801\n",
      "closing SnS socket\n",
      "socket DC Closed\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-29da2fa18dda>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;31m#model.learn(total_timesteps=timesteps,b_log_name=\"first_run\", reset_num_timesteps=False)#50000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimesteps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreset_num_timesteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#50000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0mtoday\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoday\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/thesis/lib/python3.6/site-packages/stable_baselines/ppo2/ppo2.py\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps)\u001b[0m\n\u001b[1;32m    334\u001b[0m                 \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_rollout_start\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m                 \u001b[0;31m# true_reward is the reward without discount\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 336\u001b[0;31m                 \u001b[0mrollout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    337\u001b[0m                 \u001b[0;31m# Unpack\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m                 \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneglogpacs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mep_infos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrue_reward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrollout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/thesis/lib/python3.6/site-packages/stable_baselines/common/runners.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, callback)\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontinue_training\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mabstractmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/thesis/lib/python3.6/site-packages/stable_baselines/ppo2/ppo2.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    480\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBox\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m                 \u001b[0mclipped_actions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhigh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdones\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclipped_actions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_timesteps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_envs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/thesis/lib/python3.6/site-packages/stable_baselines/common/vec_env/base_vec_env.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    148\u001b[0m         \"\"\"\n\u001b[1;32m    149\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/thesis/lib/python3.6/site-packages/stable_baselines/common/vec_env/dummy_vec_env.py\u001b[0m in \u001b[0;36mstep_wait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0menv_idx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_envs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuf_rews\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0menv_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuf_dones\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0menv_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuf_infos\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0menv_idx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menvs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0menv_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0menv_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuf_dones\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0menv_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m                 \u001b[0;31m# save final observation where user can get it, then reset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ScheragaThesis/UR5-RL/UR5_RL/envs/ur5_env_0.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m         \u001b[0;31m# we can still check if we violate a torque limit etc and if that happens, we can collect an observation, reward and make sure done is now true so we move onto a new action or terminate the current policy rollout.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmotionselector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_observation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_observation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m         \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_reward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ScheragaThesis/UR5-RL/UR5_RL/envs/ur5_env_0.py\u001b[0m in \u001b[0;36mmotionselector\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    210\u001b[0m             \u001b[0msock_DC\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactionlist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m             \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.05\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontinuousactionspace\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#https://github.com/ZhizhenQin/BalancingBot/blob/master/balance-bot/balance_bot/balancebot_task.py\n",
    "\n",
    "\n",
    "#%load_ext tensorboard\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore',category=FutureWarning)\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "\n",
    "import gym\n",
    "from stable_baselines import DQN, PPO2\n",
    "from stable_baselines import results_plotter\n",
    "from stable_baselines.bench import Monitor\n",
    "from stable_baselines.results_plotter import load_results, ts2xy\n",
    "from stable_baselines.common.noise import AdaptiveParamNoiseSpec\n",
    "from stable_baselines.common.callbacks import BaseCallback\n",
    "from stable_baselines.deepq.policies import MlpPolicy\n",
    "from stable_baselines.common.policies import FeedForwardPolicy, register_policy,LstmPolicy\n",
    "from stable_baselines.common.tf_layers import conv, linear, conv_to_fc, lstm\n",
    "from stable_baselines.common.vec_env import DummyVecEnv\n",
    "from stable_baselines.common.env_checker import check_env\n",
    "from datetime import date\n",
    "import csv\n",
    "#import balance_bot\n",
    "import socket\n",
    "import numpy as np\n",
    "from time import sleep\n",
    "import UR5_RL\n",
    "\n",
    "HOST_SnS = '192.168.0.103'\n",
    "PORT_SnS= 65498\n",
    "\n",
    "\n",
    "\n",
    "try:\n",
    "    # Create a TCP/IP socket\n",
    "    sock_SnS = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "    #sock_SnS.setblocking(False)\n",
    "    # Connect the socket to the port where the server is listening\n",
    "    server_address_SnS = (HOST_SnS, PORT_SnS)\n",
    "    print('connecting to {} port {}'.format(*server_address_SnS))\n",
    "    sock_SnS.connect(server_address_SnS)\n",
    "\n",
    "\n",
    "    #def callback(lcl, glb):\n",
    "         #stop training if reward exceeds 199\n",
    "    #    is_solved = lcl['t'] > 1000 and sum(lcl['episode_rewards'][-101:-1]) / 100 >= 1\n",
    "    #    return is_solved\n",
    "\n",
    "    #https://stable-baselines.readthedocs.io/en/master/guide/custom_policy.html\n",
    "    #Layers of 20 and 15\n",
    "\n",
    "    #Do this only after restarting the notebook!! \n",
    "    #register_policy('ScottLSTMPolicy', ScottLSTMPolicy)    \n",
    "    #print(\"lstm registered\")\n",
    "\n",
    "    #try:\n",
    "    \n",
    "    #code stopped at ep 36 at 10steps per ep, 80 ep\n",
    "    StepsPerEpisode=30 #was 10\n",
    "    TotalEpisodes=700  #was 80\n",
    "    env= gym.make(\"ur5-rl-v0\",StepsPerEpisode=StepsPerEpisode,TotalEpisodes=TotalEpisodes,\n",
    "                 continuousactionspace=False)\n",
    "    env = DummyVecEnv([lambda: env])\n",
    "\n",
    "    timesteps=(StepsPerEpisode*TotalEpisodes)\n",
    "    print(\"Total timesteps:\",timesteps)\n",
    "    \n",
    "    class ScottLSTMPolicy(LstmPolicy):\n",
    "        def __init__(self, sess, ob_space, ac_space, n_env=1, n_steps=StepsPerEpisode,\n",
    "                     n_batch=StepsPerEpisode, n_lstm=StepsPerEpisode, reuse=False,  **_kwargs):\n",
    "            super().__init__(sess, ob_space, ac_space, n_env, n_steps, n_batch, n_lstm, reuse,\n",
    "                             net_arch=[7,'lstm',dict(vf=[20, 15],pi=[20,15])],\n",
    "                             layer_norm=True, feature_extraction=\"mlp\", **_kwargs)\n",
    "            \n",
    "    #model = DQN(\"LnMlpPolicy\", env, learning_rate=1e-3, prioritized_replay=True,gamma=1 , buffer_size=50000,param_noise=False,\n",
    "    # exploration_initial_eps=0.1, exploration_final_eps=0.1,learning_starts=1, verbose=1)\n",
    "\n",
    "\n",
    "    #model = PPO2(\"MlpPolicy\", env,verboThe new research shows that \"we must expect extreme event records to be broken - not just by small marse=0)\n",
    "    #model = PPO2(\"MlpLstmPolicy\", env,nminibatches=1, n_steps=80, #exploration_initial_eps=1, exploration_final_eps=0.1,\n",
    "    #             verbose=0,tensorboard_log=\"./ScottPPOLstm/\") #n_lstm=2, n_batch=80, nminibatches=10,\n",
    "\n",
    "    model = PPO2(ScottLSTMPolicy, env,nminibatches=1, n_steps=StepsPerEpisode,learning_rate=0.1,\n",
    "                 verbose=2)# DEFAULT learning_rate=0.00025  #n_lstm=2, n_batch=80, nminibatches=10,\n",
    "    ##exploration_initial_eps=1, exploration_final_eps=0.1,\n",
    "\n",
    "    #model.learn(total_timesteps=timesteps,b_log_name=\"first_run\", reset_num_timesteps=False)#50000\n",
    "    model.learn(total_timesteps=timesteps,reset_num_timesteps=False)#50000\n",
    "    \n",
    "    today = date.today()\n",
    "    todaydate = today.strftime(\"%m_%d_%Y\")\n",
    "    RLmodelfilename=\"UR5-RL_savedpolicy_\"+todaydate\n",
    "    model.save(RLmodelfilename)# save trained model\n",
    "    \n",
    "    del model\n",
    "    print(\"training complete\")\n",
    "    \n",
    "\n",
    "finally:\n",
    "    endmsg='end'\n",
    "    data1=endmsg.encode('ascii')    \n",
    "    sock_SnS.sendall(data1)\n",
    "    sock_SnS.sendall(data1)\n",
    "    print('closing SnS socket')\n",
    "    sock_SnS.close()\n",
    "    \n",
    "    HOST2 = '192.168.0.103'\n",
    "    PORT2= PORT_SnS-10 #65481\n",
    "    sock_DC = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "    server_address_DC = (HOST2, PORT2)\n",
    "    sock_DC.close()\n",
    "    print(\"socket DC Closed\")\n",
    "      \n",
    "    #gitkraken\n",
    "    \n",
    "    #check_env(env)\n",
    "    #https://stable-baselines.readthedocs.io/en/master/modules/dqn.html\n",
    "    #model.learn(total_timesteps=25000)\n",
    "    #del model # remove to demonstrate saving and loading\n",
    "    #model = DQN.load(\"deepq_cartpole\")\n",
    "\n",
    "    \n",
    "    #Starting at 11:02AM\n",
    "    #80 episodes in 45 min\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved!\n"
     ]
    }
   ],
   "source": [
    "model.save(\"UR5-RL_savedpolicy-8-5to6-2021\")\n",
    "print(\"saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HOST2 = '192.168.0.103'learning_rate=0.00025\n",
    "PORT2= 65485\n",
    "sock_DC = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "server_address_DC = (HOST2, PORT2)\n",
    "sock_DC.close()\n",
    "print(\"socket DC Closed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[Errno 9] Bad file descriptor",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-5c92c6e2c5c7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mendmsg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'end'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdata1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mendmsg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ascii'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0msock_SnS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msendall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'closing SnS socket'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0msock_SnS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 9] Bad file descriptor"
     ]
    }
   ],
   "source": [
    "endmsg='end'\n",
    "data1=endmsg.encode('ascii')    \n",
    "sock_SnS.sendall(data1)\n",
    "\n",
    "\n",
    "#print('closing SnS socket')\n",
    "#sock_SnS.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "In /home/scott/.local/lib/python3.6/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The text.latex.preview rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /home/scott/.local/lib/python3.6/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The mathtext.fallback_to_cm rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /home/scott/.local/lib/python3.6/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: Support for setting the 'mathtext.fallback_to_cm' rcParam is deprecated since 3.3 and will be removed two minor releases later; use 'mathtext.fallback : 'cm' instead.\n",
      "In /home/scott/.local/lib/python3.6/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The validate_bool_maybe_none function was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /home/scott/.local/lib/python3.6/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The savefig.jpeg_quality rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /home/scott/.local/lib/python3.6/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The keymap.all_axes rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /home/scott/.local/lib/python3.6/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The animation.avconv_path rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /home/scott/.local/lib/python3.6/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The animation.avconv_args rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "/home/scott/.local/lib/python3.6/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/scott/.local/lib/python3.6/site-packages/stable_baselines/common/policies.py:346: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/scott/.local/lib/python3.6/site-packages/stable_baselines/common/policies.py:442: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.flatten instead.\n",
      "WARNING:tensorflow:From /home/scott/.local/lib/python3.6/site-packages/stable_baselines/common/tf_layers.py:121: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/scott/.local/lib/python3.6/site-packages/stable_baselines/common/tf_layers.py:123: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/scott/.local/lib/python3.6/site-packages/stable_baselines/common/distributions.py:326: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/scott/.local/lib/python3.6/site-packages/stable_baselines/common/distributions.py:327: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/scott/.local/lib/python3.6/site-packages/stable_baselines/ppo2/ppo2.py:190: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/scott/.local/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /home/scott/.local/lib/python3.6/site-packages/stable_baselines/ppo2/ppo2.py:206: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/scott/.local/lib/python3.6/site-packages/stable_baselines/ppo2/ppo2.py:242: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore',category=FutureWarning)\n",
    "import tensorflow as tf\n",
    "#import tensorflow.contrib.layers as layers\n",
    "import tensorflow as tf\n",
    "#from tensorflow import keras\n",
    "#from tensorflow.keras import layers\n",
    "#from keras.utils import to_categorical\n",
    "#from keras.models import Sequential\n",
    "#from keras.layers import Dense, Dropout, Embedding, LSTM, GlobalMaxPooling1D, SpatialDropout1D\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "\n",
    "import gym\n",
    "from stable_baselines import DQN,PPO2\n",
    "from stable_baselines import results_plotter\n",
    "from stable_baselines.bench import Monitor\n",
    "from stable_baselines.results_plotter import load_results, ts2xy\n",
    "from stable_baselines.common.noise import AdaptiveParamNoiseSpec\n",
    "from stable_baselines.common.callbacks import BaseCallback\n",
    "from stable_baselines.deepq.policies import MlpPolicy\n",
    "from stable_baselines.common.policies import FeedForwardPolicy, register_policy,LstmPolicy\n",
    "from stable_baselines.common.tf_layers import conv, linear, conv_to_fc, lstm\n",
    "#from stable_baselines.common import get_vec_normalize_env\n",
    "from stable_baselines.common.env_checker import check_env\n",
    "from stable_baselines.common.vec_env import DummyVecEnv\n",
    "#import balance_bot\n",
    "import MainEnv_RL\n",
    "\n",
    "env= gym.make(\"UR5-RL-env\", render=True)\n",
    "env = DummyVecEnv([lambda: env])\n",
    "\n",
    "#env=get_vec_normalize_env(env) \n",
    "#model = DQN.load(\"MainScott_RL\")\n",
    "model = PPO2.load(\"UR5-RL_savedpolicy\")\n",
    "#env=model.get_env()\n",
    "#obs = env.reset()\n",
    "done = [False for _ in range(1)] #env.num_envs\n",
    "state=None\n",
    "\n",
    "while True:\n",
    "    obs = env.reset()\n",
    "    #env._seed()\n",
    "    for i in range(80):\n",
    "        action, _states = model.predict(obs,state=state,mask=done)\n",
    "        #actionlist.append(action)\n",
    "        obs, rewards, dones, info = env.step(action)\n",
    "    print(rewards)\n",
    "        \n",
    "\"\"\"\n",
    "\n",
    "while True:\n",
    "    obs = env.reset()\n",
    "    env._seed()\n",
    "    actionlist=[]\n",
    "    #print(\"reset\")\n",
    "    for i in range(80):\n",
    "        action, _states = model.predict(obs)\n",
    "        actionlist.append(action)\n",
    "        obs, rewards, dones, info = env.step(action)\n",
    "    print(rewards)       \n",
    "\"\"\"       \n",
    "        \n",
    "        \n",
    "    #print(\"actionlist\",actionlist)\n",
    "    #env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"UR5-RL_savedpolicy-9-13-2021\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
