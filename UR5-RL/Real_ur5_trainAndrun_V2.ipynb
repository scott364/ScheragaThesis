{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfor env in gym.envs.registry.env_specs:\\n     if 'MainEnvRL-v0' in env:\\n        print('Remove {} from registry'.format(env))\\n        del gym.registry.env_specs[env]\\n\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#To register MainEnvRL with local copy of Gym\n",
    "\"\"\"\n",
    "from gym.envs.registration import register\n",
    " \n",
    "register(\n",
    "    id='MainEnvRL-v2',\n",
    "    #entry_point='balance_bot.envs:BalancebotEnv',\n",
    "    entry_point='MainEnv_RL.envs:MainEnvRL',\n",
    ")\n",
    "\n",
    "\"\"\"\n",
    "#import gym\n",
    "#for env in gym.envs.registry.env_specs:\n",
    "#    print(env)\n",
    "    \n",
    "    #if 'MainEnvRL-v1' == env:\n",
    "    #     print('Remove {} from registry'.format(env))\n",
    "         #del gym.envs.registry.env_specs[env]\n",
    "            \n",
    "            \n",
    "#import MainEnv_RL\n",
    "\n",
    "#env = gym.make('MainEnvRL-v2')\n",
    "#env.reset()\n",
    "\"\"\"\n",
    "for env in gym.envs.registry.env_specs:\n",
    "     if 'MainEnvRL-v0' in env:\n",
    "        print('Remove {} from registry'.format(env))\n",
    "        del gym.registry.env_specs[env]\n",
    "\"\"\"                \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "connecting to 192.168.0.103 port 65495\n",
      "port= /dev/ttyACM1\n",
      "the red  robot is being used. Please change the bot identity variable if this is incorrect\n",
      "Connected to http://192.168.0.103:10000\n",
      "DC socket connecting to 192.168.0.103 port 65485\n",
      "TotalEpisodes: 700    StepsPerEpisode: 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/scott/anaconda3/envs/thesis/lib/python3.6/site-packages/stable_baselines/__init__.py:33: UserWarning: stable-baselines is in maintenance mode, please use [Stable-Baselines3 (SB3)](https://github.com/DLR-RM/stable-baselines3) for an up-to-date version. You can find a [migration guide](https://stable-baselines3.readthedocs.io/en/master/guide/migration.html) in SB3 documentation.\n",
      "  \"stable-baselines is in maintenance mode, please use [Stable-Baselines3 (SB3)](https://github.com/DLR-RM/stable-baselines3) for an up-to-date version. You can find a [migration guide](https://stable-baselines3.readthedocs.io/en/master/guide/migration.html) in SB3 documentation.\"\n",
      "/home/scott/anaconda3/envs/thesis/lib/python3.6/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total timesteps: 14000\n",
      "WARNING:tensorflow:From /home/scott/anaconda3/envs/thesis/lib/python3.6/site-packages/stable_baselines/common/tf_util.py:191: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/scott/anaconda3/envs/thesis/lib/python3.6/site-packages/stable_baselines/common/tf_util.py:200: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/scott/anaconda3/envs/thesis/lib/python3.6/site-packages/stable_baselines/common/policies.py:116: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/scott/anaconda3/envs/thesis/lib/python3.6/site-packages/stable_baselines/common/input.py:25: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/scott/anaconda3/envs/thesis/lib/python3.6/site-packages/stable_baselines/common/policies.py:442: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.flatten instead.\n",
      "WARNING:tensorflow:From /home/scott/anaconda3/envs/thesis/lib/python3.6/site-packages/tensorflow_core/python/layers/core.py:332: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n",
      "WARNING:tensorflow:From /home/scott/anaconda3/envs/thesis/lib/python3.6/site-packages/stable_baselines/common/tf_layers.py:123: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/scott/anaconda3/envs/thesis/lib/python3.6/site-packages/stable_baselines/common/distributions.py:418: The name tf.random_normal is deprecated. Please use tf.random.normal instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/scott/anaconda3/envs/thesis/lib/python3.6/site-packages/stable_baselines/ppo2/ppo2.py:190: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/scott/anaconda3/envs/thesis/lib/python3.6/site-packages/stable_baselines/ppo2/ppo2.py:198: The name tf.trainable_variables is deprecated. Please use tf.compat.v1.trainable_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/scott/anaconda3/envs/thesis/lib/python3.6/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /home/scott/anaconda3/envs/thesis/lib/python3.6/site-packages/stable_baselines/ppo2/ppo2.py:206: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/scott/anaconda3/envs/thesis/lib/python3.6/site-packages/stable_baselines/ppo2/ppo2.py:240: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/scott/anaconda3/envs/thesis/lib/python3.6/site-packages/stable_baselines/ppo2/ppo2.py:242: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n",
      "\n",
      "Ep: 1  tStep: 1 Z difference 0.005326288013160152  Reward: -0.6621467247121036\n",
      "Ep: 1  tStep: 2 Z difference -0.0005526339143515635  Reward: -0.6680256466396153\n",
      "Ep: 1  tStep: 3 Z difference 0.005210422622412114  Reward: -0.6622625901028516\n",
      "Ep: 1  tStep: 4 Z difference 0.0025598918102680557  Reward: -0.6649131209149957\n",
      "Ep: 1  tStep: 5 Z difference 0.0031694317646322467  Reward: -0.6643035809606315\n",
      "Ep: 1  tStep: 6 Z difference 0.0035261211700734485  Reward: -0.6639468915551903\n",
      "Ep: 1  tStep: 7 Z difference 0.007108855050057095  Reward: -0.6603641576752066\n",
      "Ep: 1  tStep: 8 Z difference 0.003233084397017638  Reward: -0.6642399283282461\n",
      "Ep: 1  tStep: 9 Z difference 0.005053784347325507  Reward: -0.6624192283779382\n",
      "Ep: 1  tStep: 10 Z difference 0.007956579048186363  Reward: -0.6595164336770774\n",
      "Ep: 1  tStep: 11 Z difference 0.005621378096937857  Reward: -0.6618516346283259\n",
      "Ep: 1  tStep: 12 Z difference 0.0039678763054307176  Reward: -0.663505136419833\n",
      "Ep: 1  tStep: 13 Z difference 0.005941987899690471  Reward: -0.6615310248255732\n",
      "Ep: 1  tStep: 14 Z difference 0.004279099544882481  Reward: -0.6631939131803812\n",
      "Ep: 1  tStep: 15 Z difference 0.005402847170084435  Reward: -0.6620701655551793\n",
      "Ep: 1  tStep: 16 Z difference 0.00442019132450211  Reward: -0.6630528214007616\n",
      "Ep: 1  tStep: 17 Z difference 0.005418980325758049  Reward: -0.6620540323995057\n",
      "Ep: 1  tStep: 18 Z difference 0.00557385862022608  Reward: -0.6618991541050376\n",
      "Ep: 1  tStep: 19 Z difference 0.00741978495940554  Reward: -0.6600532277658582\n",
      "Ep: 1  tStep: 20 Z difference 0.007054588980972554  Reward: -0.6604184237442912\n",
      "-------------------------------------\n",
      "| approxkl           | 0.00663827   |\n",
      "| clipfrac           | 0.075        |\n",
      "| explained_variance | -0.0279      |\n",
      "| fps                | 0            |\n",
      "| n_updates          | 1            |\n",
      "| policy_entropy     | 2.8358717    |\n",
      "| policy_loss        | -0.023640078 |\n",
      "| serial_timesteps   | 20           |\n",
      "| time_elapsed       | 1.03e-05     |\n",
      "| total_timesteps    | 20           |\n",
      "| value_loss         | 13.975975    |\n",
      "-------------------------------------\n",
      "Ep: 2  tStep: 1 Z difference 0.003746118747443017  Reward: -0.6633352982901037\n",
      "Ep: 2  tStep: 2 Z difference -0.002036297576129442  Reward: -0.6691177146136762\n",
      "Ep: 2  tStep: 3 Z difference 0.0020855770334602397  Reward: -0.6649958400040865\n",
      "Ep: 2  tStep: 4 Z difference 0.00047372811660162384  Reward: -0.6666076889209451\n",
      "Ep: 2  tStep: 5 Z difference 0.000594580119103405  Reward: -0.6664868369184433\n",
      "Ep: 2  tStep: 6 Z difference 0.0018371264360843576  Reward: -0.6652442906014624\n",
      "BUTTON PRESSED! Episode over!\n",
      "*****Success condition achieved at tStep 7 Total Successes: 1 *****\n",
      "Ep: 2  tStep: 7 Z difference -0.0009902824282645994  Reward: 0.28192830053418866\n",
      "Ep: 3  tStep: 1 Z difference 0.0038036114476622807  Reward: -0.6629750889234245\n",
      "Ep: 3  tStep: 2 Z difference -0.00178022039607173  Reward: -0.6685589207671585\n",
      "Ep: 3  tStep: 3 Z difference -3.549294248195167e-05  Reward: -0.6668141933135687\n",
      "Ep: 3  tStep: 4 Z difference 0.00019653116911655388  Reward: -0.6665821692019702\n",
      "Ep: 3  tStep: 5 Z difference -0.0009483362235127579  Reward: -0.6677270365945995\n",
      "Ep: 3  tStep: 6 Z difference 0.00040567553266912526  Reward: -0.6663730248384176\n",
      "Ep: 3  tStep: 7 Z difference 0.0037308655820789127  Reward: -0.6630478347890079\n",
      "Ep: 3  tStep: 8 Z difference 0.0022738949596883273  Reward: -0.6645048054113984\n",
      "Ep: 3  tStep: 9 Z difference 0.004299925982206965  Reward: -0.6624787743888798\n",
      "Ep: 3  tStep: 10 Z difference 0.003940303275734358  Reward: -0.6628383970953524\n",
      "Ep: 3  tStep: 11 Z difference 0.005775376401096821  Reward: -0.66100332396999\n",
      "Ep: 3  tStep: 12 Z difference 0.005117143649608025  Reward: -0.6616615567214787\n",
      "Ep: 3  tStep: 13 Z difference 0.006932856988161706  Reward: -0.6598458433829251\n",
      "-------------------------------------\n",
      "| approxkl           | 0.0074160807 |\n",
      "| clipfrac           | 0.0875       |\n",
      "| explained_variance | -0.066       |\n",
      "| fps                | 0            |\n",
      "| n_updates          | 2            |\n",
      "| policy_entropy     | 2.830532     |\n",
      "| policy_loss        | -0.019471515 |\n",
      "| serial_timesteps   | 40           |\n",
      "| time_elapsed       | 56.1         |\n",
      "| total_timesteps    | 40           |\n",
      "| value_loss         | 5.7224092    |\n",
      "-------------------------------------\n",
      "Ep: 3  tStep: 14 Z difference 0.009818345212936563  Reward: -0.6569603551581502\n",
      "Ep: 3  tStep: 15 Z difference 0.0069771498337387605  Reward: -0.659801550537348\n",
      "Ep: 3  tStep: 16 Z difference 0.0054010871894658585  Reward: -0.6613776131816209\n",
      "Ep: 3  tStep: 17 Z difference 0.00630542389750488  Reward: -0.6604732764735819\n",
      "Ep: 3  tStep: 18 Z difference 0.007282799801230588  Reward: -0.6594959005698562\n",
      "Ep: 3  tStep: 19 Z difference 0.007303919568657946  Reward: -0.6594747808024288\n",
      "BUTTON PRESSED! Episode over!\n",
      "*****Success condition achieved at tStep 20 Total Successes: 2 *****\n",
      "Ep: 3  tStep: 20 Z difference 0.008970914544910169  Reward: -0.3578077858261766\n",
      "Ep: 4  tStep: 1 Z difference 0.0024349331863224855  Reward: -0.6636570814132692\n",
      "Ep: 4  tStep: 2 Z difference -0.0005153809912501472  Reward: -0.6666073955908418\n",
      "Ep: 4  tStep: 3 Z difference 0.0007268719956279313  Reward: -0.6653651426039637\n",
      "Ep: 4  tStep: 4 Z difference 0.00044674174711101244  Reward: -0.6656452728524807\n",
      "Ep: 4  tStep: 5 Z difference 0.002010191196948341  Reward: -0.6640818234026433\n",
      "Ep: 4  tStep: 6 Z difference 0.0015523029059174576  Reward: -0.6645397116936742\n",
      "Ep: 4  tStep: 7 Z difference 0.003388549351692305  Reward: -0.6627034652478994\n",
      "Ep: 4  tStep: 8 Z difference 0.002353094087541141  Reward: -0.6637389205120505\n",
      "Ep: 4  tStep: 9 Z difference 0.0037637185536327777  Reward: -0.6623282960459589\n",
      "Ep: 4  tStep: 10 Z difference 4.546616598988251e-05  Reward: -0.6660465484336018\n",
      "Ep: 4  tStep: 11 Z difference 0.002491545896232239  Reward: -0.6636004687033594\n",
      "Ep: 4  tStep: 12 Z difference 0.0012302264526486972  Reward: -0.664861788146943\n",
      "Ep: 4  tStep: 13 Z difference 0.0005450073316692894  Reward: -1\n",
      "------------------------------------\n",
      "| approxkl           | 0.005439094 |\n",
      "| clipfrac           | 0.0625      |\n",
      "| explained_variance | -0.0258     |\n",
      "| fps                | 0           |\n",
      "| n_updates          | 3           |\n",
      "| policy_entropy     | 2.8255117   |\n",
      "| policy_loss        | -0.03305257 |\n",
      "| serial_timesteps   | 60          |\n",
      "| time_elapsed       | 90.5        |\n",
      "| total_timesteps    | 60          |\n",
      "| value_loss         | 6.107706    |\n",
      "------------------------------------\n",
      "Ep: 4  tStep: 14 Z difference 0.0044143247224393  Reward: -1\n",
      "Ep: 4  tStep: 15 Z difference 0.0008480173282325865  Reward: -1\n",
      "Ep: 4  tStep: 16 Z difference 0.0030001802951096224  Reward: -1\n",
      "BUTTON PRESSED! Episode over!\n",
      "*****Success condition achieved at tStep 17 Total Successes: 3 *****\n",
      "Ep: 4  tStep: 17 Z difference 0.005402847170084879  Reward: -0.55\n",
      "Ep: 5  tStep: 1 Z difference -0.0009946823798121507  Reward: -0.6662190265342596\n",
      "Ep: 5  tStep: 2 Z difference -0.00033820960894237473  Reward: -0.6655625537633898\n",
      "Ep: 5  tStep: 3 Z difference 0.0008248442500828901  Reward: -0.6643994999043645\n",
      "BUTTON PRESSED! Episode over!\n",
      "*****Success condition achieved at tStep 4 Total Successes: 4 *****\n",
      "Ep: 5  tStep: 4 Z difference 0.00253935870304689  Reward: 0.4373150145485995\n",
      "Ep: 6  tStep: 1 Z difference 0.0033248967193064694  Reward: -0.6627363182194532\n",
      "Ep: 6  tStep: 2 Z difference 0.002170056103169671  Reward: -0.66389115883559\n",
      "Ep: 6  tStep: 3 Z difference 0.002149229665845631  Reward: -0.6639119852729141\n",
      "Ep: 6  tStep: 4 Z difference 0.0004822346895929641  Reward: -0.6655789802491667\n",
      "Ep: 6  tStep: 5 Z difference -0.0006479661978784357  Reward: -0.6667091811366381\n",
      "Ep: 6  tStep: 6 Z difference 3.3439631760057154e-05  Reward: -0.6660277753069996\n",
      "Ep: 6  tStep: 7 Z difference -0.0005681804098189858  Reward: -0.6666293953485787\n",
      "Ep: 6  tStep: 8 Z difference -0.0016253421016041436  Reward: -0.6676865570403638\n",
      "Ep: 6  tStep: 9 Z difference -0.0008964167952538737  Reward: -0.6669576317340136\n",
      "Ep: 6  tStep: 10 Z difference 0.0002581304907796067  Reward: -0.6658030844479801\n",
      "Ep: 6  tStep: 11 Z difference 0.0008257242403924003  Reward: -0.6652354906983673\n",
      "Ep: 6  tStep: 12 Z difference -0.0015461429737508858  Reward: -0.6676073579125106\n",
      "-------------------------------------\n",
      "| approxkl           | 0.0095859375 |\n",
      "| clipfrac           | 0.15         |\n",
      "| explained_variance | 0.00135      |\n",
      "| fps                | 0            |\n",
      "| n_updates          | 4            |\n",
      "| policy_entropy     | 2.8259177    |\n",
      "| policy_loss        | -0.029352626 |\n",
      "| serial_timesteps   | 80           |\n",
      "| time_elapsed       | 125          |\n",
      "| total_timesteps    | 80           |\n",
      "| value_loss         | 4.338813     |\n",
      "-------------------------------------\n",
      "Ep: 6  tStep: 13 Z difference -0.00025226388871679717  Reward: -0.6663134788274765\n",
      "Ep: 6  tStep: 14 Z difference -0.0015206232547759768  Reward: -0.6675818381935357\n",
      "Ep: 6  tStep: 15 Z difference 0.0011354808293280172  Reward: -0.6649257341094317\n",
      "Ep: 6  tStep: 16 Z difference -0.001364571639895562  Reward: -0.6674257865786553\n",
      "Ep: 6  tStep: 17 Z difference -0.0005244742244485678  Reward: -0.6665856891632083\n",
      "Ep: 6  tStep: 18 Z difference 0.0011439874023198016  Reward: -0.6649172275364399\n",
      "Ep: 6  tStep: 19 Z difference -0.000609246624261317  Reward: -0.666670461563021\n",
      "Ep: 6  tStep: 20 Z difference 0.0012228932000692971  Reward: -0.6648383217386904\n",
      "Ep: 7  tStep: 1 Z difference 0.0040640885792671  Reward: -0.6623203761331737\n",
      "Ep: 7  tStep: 2 Z difference 0.0029978336542844097  Reward: -0.6633866310581564\n",
      "Ep: 7  tStep: 3 Z difference 0.003010153518617109  Reward: -0.6633743111938237\n",
      "Ep: 7  tStep: 4 Z difference 0.004338058895617447  Reward: -0.6620464058168234\n",
      "Ep: 7  tStep: 5 Z difference 0.0034208156630395337  Reward: -0.6629636490494013\n",
      "Ep: 7  tStep: 6 Z difference 0.005617858135699816  Reward: -0.660766606576741\n",
      "Ep: 7  tStep: 7 Z difference 0.005585591824352587  Reward: -0.6607988728880883\n",
      "Ep: 7  tStep: 8 Z difference 0.004269419651478312  Reward: -0.6621150450609625\n",
      "Ep: 7  tStep: 9 Z difference 0.005049091065674638  Reward: -0.6613353736467662\n",
      "Ep: 7  tStep: 10 Z difference 0.0070994684867558  Reward: -0.659284996225685\n",
      "Ep: 7  tStep: 11 Z difference 0.00629721065461597  Reward: -0.6600872540578249\n",
      "Ep: 7  tStep: 12 Z difference 0.006181051933765058  Reward: -0.6602034127786758\n",
      "--------------------------------------\n",
      "| approxkl           | 0.0033353786  |\n",
      "| clipfrac           | 0.0375        |\n",
      "| explained_variance | 0.0679        |\n",
      "| fps                | 0             |\n",
      "| n_updates          | 5             |\n",
      "| policy_entropy     | 2.8284147     |\n",
      "| policy_loss        | -0.0015122741 |\n",
      "| serial_timesteps   | 100           |\n",
      "| time_elapsed       | 178           |\n",
      "| total_timesteps    | 100           |\n",
      "| value_loss         | 4.5574236     |\n",
      "--------------------------------------\n",
      "Ep: 7  tStep: 13 Z difference 0.007956285718083045  Reward: -0.6584281789943578\n",
      "Ep: 7  tStep: 14 Z difference 0.006233558022230579  Reward: -0.6601509066902103\n",
      "Ep: 7  tStep: 15 Z difference 0.007626289352029136  Reward: -0.6587581753604117\n",
      "Ep: 7  tStep: 16 Z difference 0.008002925204485312  Reward: -0.6583815395079555\n",
      "Ep: 7  tStep: 17 Z difference 0.011686564639955765  Reward: -1\n",
      "Ep: 7  tStep: 18 Z difference 0.009727119550853924  Reward: -1\n",
      "Ep: 7  tStep: 19 Z difference 0.009141632664948496  Reward: -1\n",
      "Ep: 7  tStep: 20 Z difference 0.007922846086322988  Reward: -1\n",
      "Ep: 8  tStep: 1 Z difference 0.002966740663349565  Reward: -0.6630146884873511\n",
      "Ep: 8  tStep: 2 Z difference -0.0017195010647177433  Reward: -0.6677009302154184\n",
      "Ep: 8  tStep: 3 Z difference 0.0005916468180715562  Reward: -0.6653897823326291\n",
      "Ep: 8  tStep: 4 Z difference -0.0019268854476512942  Reward: -0.667908314598352\n",
      "Ep: 8  tStep: 5 Z difference -0.00020855770334593515  Reward: -0.6661899868540466\n",
      "Ep: 8  tStep: 6 Z difference -0.0017063012100755337  Reward: -0.6676877303607762\n",
      "Ep: 8  tStep: 7 Z difference 0.0006614593826235193  Reward: -0.6653199697680772\n",
      "Ep: 8  tStep: 8 Z difference 0.0030198334120212778  Reward: -0.6629615957386794\n",
      "Ep: 8  tStep: 9 Z difference 0.001977924885600668  Reward: -0.6640035042651\n",
      "Ep: 8  tStep: 10 Z difference 0.0019679516620936255  Reward: -0.6640134774886071\n",
      "Ep: 8  tStep: 11 Z difference 0.003753452000021973  Reward: -0.6622279771506787\n",
      "Ep: 8  tStep: 12 Z difference 0.0033331099621953797  Reward: -0.6626483191885053\n",
      "--------------------------------------\n",
      "| approxkl           | 0.0009113368  |\n",
      "| clipfrac           | 0.0125        |\n",
      "| explained_variance | 0.0335        |\n",
      "| fps                | 0             |\n",
      "| n_updates          | 6             |\n",
      "| policy_entropy     | 2.8284671     |\n",
      "| policy_loss        | -0.0030026408 |\n",
      "| serial_timesteps   | 120           |\n",
      "| time_elapsed       | 213           |\n",
      "| total_timesteps    | 120           |\n",
      "| value_loss         | 5.292679      |\n",
      "--------------------------------------\n",
      "Ep: 8  tStep: 13 Z difference 0.0068020317621528825  Reward: -0.6591793973885478\n",
      "Ep: 8  tStep: 14 Z difference 0.004118061318248323  Reward: -0.6618633678324524\n",
      "Ep: 8  tStep: 15 Z difference 0.004424004615843469  Reward: -1\n",
      "Ep: 8  tStep: 16 Z difference 0.005558312124759102  Reward: -1\n",
      "Ep: 8  tStep: 17 Z difference 0.006540381310135235  Reward: -1\n",
      "Ep: 8  tStep: 18 Z difference 0.007662368954718168  Reward: -1\n",
      "Ep: 8  tStep: 19 Z difference 0.006127665854990472  Reward: -1\n",
      "Ep: 8  tStep: 20 Z difference 0.0075790632054211216  Reward: -1\n",
      "Ep: 9  tStep: 1 Z difference 0.0018107267268003824  Reward: -0.6649647470131517\n",
      "Ep: 9  tStep: 2 Z difference 0.00045436832979328656  Reward: -0.6663211054101588\n",
      "Ep: 9  tStep: 3 Z difference 0.0006036733523013815  Reward: -0.6661718003876507\n",
      "Ep: 9  tStep: 4 Z difference -0.0016752082191406892  Reward: -0.6684506819590927\n",
      "Ep: 9  tStep: 5 Z difference 0.001959151758998967  Reward: -0.6648163219809531\n",
      "Ep: 9  tStep: 6 Z difference -0.0008183909878134443  Reward: -0.6675938647277655\n",
      "Ep: 9  tStep: 7 Z difference 0.0023319743201137833  Reward: -0.6644434994198383\n",
      "Ep: 9  tStep: 8 Z difference 0.004138007765263296  Reward: -0.6626374659746888\n",
      "Ep: 9  tStep: 9 Z difference 0.0022240288421513377  Reward: -0.6645514448978007\n",
      "Ep: 9  tStep: 10 Z difference 0.003085539355129008  Reward: -0.663689934384823\n",
      "Ep: 9  tStep: 11 Z difference 0.0017379808612170144  Reward: -0.665037492878735\n",
      "Ep: 9  tStep: 12 Z difference 0.0034885749168696023  Reward: -0.6632868988230824\n",
      "-------------------------------------\n",
      "| approxkl           | 0.0018576091 |\n",
      "| clipfrac           | 0.0          |\n",
      "| explained_variance | -0.00787     |\n",
      "| fps                | 0            |\n",
      "| n_updates          | 7            |\n",
      "| policy_entropy     | 2.8283288    |\n",
      "| policy_loss        | -0.024527142 |\n",
      "| serial_timesteps   | 140          |\n",
      "| time_elapsed       | 248          |\n",
      "| total_timesteps    | 140          |\n",
      "| value_loss         | 5.4841995    |\n",
      "-------------------------------------\n",
      "Ep: 9  tStep: 13 Z difference 0.0028189022913576167  Reward: -0.6639565714485944\n",
      "Ep: 9  tStep: 14 Z difference 0.001433504214138015  Reward: -0.665341969525814\n",
      "Ep: 9  tStep: 15 Z difference 0.003440175449848315  Reward: -0.6633352982901037\n",
      "Ep: 9  tStep: 16 Z difference 0.005267035332322312  Reward: -0.6615084384076297\n",
      "Ep: 9  tStep: 17 Z difference 0.005557138804346273  Reward: -0.6612183349356058\n",
      "Ep: 9  tStep: 18 Z difference 0.004174087367951884  Reward: -0.6626013863720002\n",
      "Ep: 9  tStep: 19 Z difference 0.0075682099916041246  Reward: -0.6592072637483479\n",
      "Ep: 9  tStep: 20 Z difference 0.006518088222295049  Reward: -0.660257385517657\n",
      "Ep: 10  tStep: 1 Z difference 0.004367098575830397  Reward: -0.6625216005839407\n",
      "Ep: 10  tStep: 2 Z difference 0.001656141762435226  Reward: -0.6652325573973359\n",
      "Ep: 10  tStep: 3 Z difference 0.003589480472355966  Reward: -0.6632992186874151\n",
      "Ep: 10  tStep: 4 Z difference 0.0020456841394302927  Reward: -0.6648430150203408\n",
      "Ep: 10  tStep: 5 Z difference 0.005236235671490341  Reward: -0.6616524634882808\n",
      "BUTTON PRESSED! Episode over!\n",
      "*****Success condition achieved at tStep 6 Total Successes: 5 *****\n",
      "Ep: 10  tStep: 6 Z difference 0.002367760592699053  Reward: 0.3354790614329279\n",
      "Ep: 11  tStep: 1 Z difference 0.0008820436201988358  Reward: -0.66532114308849\n",
      "Ep: 11  tStep: 2 Z difference 0.0030702861897649036  Reward: -0.6631329005189239\n",
      "Ep: 11  tStep: 3 Z difference 0.0023102678924797893  Reward: -0.663892918816209\n",
      "Ep: 11  tStep: 4 Z difference 0.004012169151008216  Reward: -0.6621910175576806\n",
      "Ep: 11  tStep: 5 Z difference 0.005074317454546673  Reward: -0.6611288692541422\n",
      "Ep: 11  tStep: 6 Z difference 0.006442115725576958  Reward: -0.6597610709831119\n",
      "-------------------------------------\n",
      "| approxkl           | 0.006855868  |\n",
      "| clipfrac           | 0.112500004  |\n",
      "| explained_variance | -0.0498      |\n",
      "| fps                | 0            |\n",
      "| n_updates          | 8            |\n",
      "| policy_entropy     | 2.830047     |\n",
      "| policy_loss        | -0.011944977 |\n",
      "| serial_timesteps   | 160          |\n",
      "| time_elapsed       | 283          |\n",
      "| total_timesteps    | 160          |\n",
      "| value_loss         | 1.6015826    |\n",
      "-------------------------------------\n",
      "Ep: 11  tStep: 7 Z difference 0.0068859241716565656  Reward: -0.6593172625370323\n",
      "Ep: 11  tStep: 8 Z difference 0.00528815509974967  Reward: -0.6609150316089392\n",
      "Ep: 11  tStep: 9 Z difference 0.006722832634300069  Reward: -0.6594803540743888\n",
      "Ep: 11  tStep: 10 Z difference 0.004630509008467065  Reward: -0.6615726777002218\n",
      "Ep: 11  tStep: 11 Z difference 0.00925075146332377  Reward: -0.6569524352453651\n",
      "Ep: 11  tStep: 12 Z difference 0.009334057212620817  Reward: -0.656869129496068\n",
      "Ep: 11  tStep: 13 Z difference 0.007329732617735729  Reward: -0.6588734540909531\n",
      "Ep: 11  tStep: 14 Z difference 0.009501842031627739  Reward: -0.6567013446770611\n",
      "Ep: 11  tStep: 15 Z difference 0.00820708295628414  Reward: -0.6579961037524047\n",
      "Ep: 11  tStep: 16 Z difference 0.0076007696330546715  Reward: -0.6586024170756342\n",
      "Ep: 11  tStep: 17 Z difference 0.009737679434567603  Reward: -0.6564655072741212\n",
      "Ep: 11  tStep: 18 Z difference 0.00747229104787106  Reward: -0.6587308956608178\n",
      "Ep: 11  tStep: 19 Z difference 0.008877342242002317  Reward: -0.6573258444666865\n",
      "Ep: 11  tStep: 20 Z difference 0.009476322312653274  Reward: -0.6567268643960356\n",
      "Ep: 12  tStep: 1 Z difference 0.0011762537136670304  Reward: -0.6644904322363439\n",
      "Ep: 12  tStep: 2 Z difference 0.001054521720856183  Reward: -0.6646121642291547\n",
      "Ep: 12  tStep: 3 Z difference 0.0003543427646159891  Reward: -0.6653123431853949\n",
      "Ep: 12  tStep: 4 Z difference 0.0011439874023198016  Reward: -0.6645226985476911\n",
      "Ep: 12  tStep: 5 Z difference -0.0009568427965045423  Reward: -0.6666235287465154\n",
      "Ep: 12  tStep: 6 Z difference 0.0011445740625259937  Reward: -0.6645221118874849\n",
      "--------------------------------------\n",
      "| approxkl           | 0.0029622817  |\n",
      "| clipfrac           | 0.0125        |\n",
      "| explained_variance | -0.0167       |\n",
      "| fps                | 0             |\n",
      "| n_updates          | 9             |\n",
      "| policy_entropy     | 2.8316944     |\n",
      "| policy_loss        | -0.0018297546 |\n",
      "| serial_timesteps   | 180           |\n",
      "| time_elapsed       | 337           |\n",
      "| total_timesteps    | 180           |\n",
      "| value_loss         | 3.5365467     |\n",
      "--------------------------------------\n",
      "Ep: 12  tStep: 7 Z difference 0.00019154455736236642  Reward: -0.6654751413926485\n",
      "Ep: 12  tStep: 8 Z difference 0.00017013145983213462  Reward: -0.6654965544901787\n",
      "Ep: 12  tStep: 9 Z difference 0.0017500073954459516  Reward: -0.6639166785545649\n",
      "Ep: 12  tStep: 10 Z difference 0.0026361576370894646  Reward: -0.6630305283129214\n",
      "Ep: 12  tStep: 11 Z difference 0.00312367226853949  Reward: -0.6625430136814714\n",
      "Ep: 12  tStep: 12 Z difference 0.004975758539885078  Reward: -0.6606909274101258\n",
      "Ep: 12  tStep: 13 Z difference 0.006066066533326975  Reward: -0.6596006194166839\n",
      "Ep: 12  tStep: 14 Z difference 0.004166754115372484  Reward: -0.6614999318346384\n",
      "Ep: 12  tStep: 15 Z difference 0.004571549657732099  Reward: -0.6610951362922788\n",
      "Ep: 12  tStep: 16 Z difference 0.004054702015965805  Reward: -0.6616119839340451\n",
      "Ep: 12  tStep: 17 Z difference 0.004805333749949625  Reward: -0.6608613522000613\n",
      "Ep: 12  tStep: 18 Z difference 0.004682135106623075  Reward: -1\n",
      "Ep: 12  tStep: 19 Z difference 0.0027238633379336186  Reward: -1\n",
      "Ep: 12  tStep: 20 Z difference 0.005259408749639594  Reward: -1\n",
      "Ep: 13  tStep: 1 Z difference 0.004692988320440072  Reward: -0.6615289715148509\n",
      "Ep: 13  tStep: 2 Z difference 0.001944778583943929  Reward: -0.664277181251347\n",
      "Ep: 13  tStep: 3 Z difference -0.0004751947671173262  Reward: -0.6666971546024083\n",
      "Ep: 13  tStep: 4 Z difference 0.002510025692731066  Reward: -0.6637119341425599\n",
      "Ep: 13  tStep: 5 Z difference 0.002779302727430988  Reward: -0.66344265710786\n",
      "Ep: 13  tStep: 6 Z difference 0.005092503920942626  Reward: -0.6611294559143484\n",
      "-------------------------------------\n",
      "| approxkl           | 0.0034650182 |\n",
      "| clipfrac           | 0.0375       |\n",
      "| explained_variance | -0.0115      |\n",
      "| fps                | 0            |\n",
      "| n_updates          | 10           |\n",
      "| policy_entropy     | 2.829381     |\n",
      "| policy_loss        | -0.004807426 |\n",
      "| serial_timesteps   | 200          |\n",
      "| time_elapsed       | 372          |\n",
      "| total_timesteps    | 200          |\n",
      "| value_loss         | 4.3229547    |\n",
      "-------------------------------------\n",
      "Ep: 13  tStep: 7 Z difference 0.004254166486114208  Reward: -0.6619677933491768\n",
      "Ep: 13  tStep: 8 Z difference 0.006735445828735642  Reward: -0.6594865140065553\n",
      "Ep: 13  tStep: 9 Z difference 0.005807056052237858  Reward: -0.6604149037830531\n",
      "Ep: 13  tStep: 10 Z difference 0.006989763028174778  Reward: -0.6592321968071162\n",
      "closing SnS socket\n",
      "socket DC Closed\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-6bf4884dabd2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0;31m#model.learn(total_timesteps=timesteps,b_log_name=\"first_run\", reset_num_timesteps=False)#50000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimesteps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreset_num_timesteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#50000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"UR5-RL_savedpolicy-9-11-2021\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m# save trained model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mdel\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/thesis/lib/python3.6/site-packages/stable_baselines/ppo2/ppo2.py\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps)\u001b[0m\n\u001b[1;32m    334\u001b[0m                 \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_rollout_start\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m                 \u001b[0;31m# true_reward is the reward without discount\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 336\u001b[0;31m                 \u001b[0mrollout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    337\u001b[0m                 \u001b[0;31m# Unpack\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m                 \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneglogpacs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mep_infos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrue_reward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrollout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/thesis/lib/python3.6/site-packages/stable_baselines/common/runners.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, callback)\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontinue_training\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mabstractmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/thesis/lib/python3.6/site-packages/stable_baselines/ppo2/ppo2.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    480\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBox\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m                 \u001b[0mclipped_actions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhigh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdones\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclipped_actions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_timesteps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_envs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/thesis/lib/python3.6/site-packages/stable_baselines/common/vec_env/base_vec_env.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    148\u001b[0m         \"\"\"\n\u001b[1;32m    149\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/thesis/lib/python3.6/site-packages/stable_baselines/common/vec_env/dummy_vec_env.py\u001b[0m in \u001b[0;36mstep_wait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0menv_idx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_envs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuf_rews\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0menv_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuf_dones\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0menv_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuf_infos\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0menv_idx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menvs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0menv_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0menv_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuf_dones\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0menv_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m                 \u001b[0;31m# save final observation where user can get it, then reset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ScheragaThesis/UR5-RL/UR5_RL/envs/ur5_env_0.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    166\u001b[0m         \u001b[0;31m# we can still check if we violate a torque limit etc and if that happens, we can collect an observation, reward and make sure done is now true so we move onto a new action or terminate the current policy rollout.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmotionselector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_observation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_observation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    169\u001b[0m         \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_reward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ScheragaThesis/UR5-RL/UR5_RL/envs/ur5_env_0.py\u001b[0m in \u001b[0;36m_compute_observation\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    280\u001b[0m         \u001b[0;31m#print(\"sent\",inputstring)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbot\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'red'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 282\u001b[0;31m             \u001b[0mdata2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msock_DC\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#64\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    283\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mdata2\u001b[0m\u001b[0;34m==\u001b[0m \u001b[0;34mb''\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m                 \u001b[0mdata2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msock_DC\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#64  #48 bytes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#https://github.com/ZhizhenQin/BalancingBot/blob/master/balance-bot/balance_bot/balancebot_task.py\n",
    "\n",
    "\n",
    "#%load_ext tensorboard\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore',category=FutureWarning)\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "\n",
    "import gym\n",
    "from stable_baselines import DQN, PPO2\n",
    "from stable_baselines import results_plotter\n",
    "from stable_baselines.bench import Monitor\n",
    "from stable_baselines.results_plotter import load_results, ts2xy\n",
    "from stable_baselines.common.noise import AdaptiveParamNoiseSpec\n",
    "from stable_baselines.common.callbacks import BaseCallback\n",
    "from stable_baselines.deepq.policies import MlpPolicy\n",
    "from stable_baselines.common.policies import FeedForwardPolicy, register_policy,LstmPolicy\n",
    "from stable_baselines.common.tf_layers import conv, linear, conv_to_fc, lstm\n",
    "from stable_baselines.common.vec_env import DummyVecEnv\n",
    "from stable_baselines.common.env_checker import check_env\n",
    "#import balance_bot\n",
    "import socket\n",
    "import numpy as np\n",
    "from time import sleep\n",
    "import UR5_RL\n",
    "\n",
    "HOST_SnS = '192.168.0.103'\n",
    "PORT_SnS= 65495\n",
    "try:\n",
    "    # Create a TCP/IP socket\n",
    "    sock_SnS = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "    #sock_SnS.setblocking(False)\n",
    "    # Connect the socket to the port where the server is listening\n",
    "    server_address_SnS = (HOST_SnS, PORT_SnS)\n",
    "    print('connecting to {} port {}'.format(*server_address_SnS))\n",
    "    sock_SnS.connect(server_address_SnS)\n",
    "\n",
    "\n",
    "    #def callback(lcl, glb):\n",
    "         #stop training if reward exceeds 199\n",
    "    #    is_solved = lcl['t'] > 1000 and sum(lcl['episode_rewards'][-101:-1]) / 100 >= 1\n",
    "    #    return is_solved\n",
    "\n",
    "    #https://stable-baselines.readthedocs.io/en/master/guide/custom_policy.html\n",
    "    #Layers of 20 and 15\n",
    "\n",
    "    #Do this only after restarting the notebook!! \n",
    "    #register_policy('ScottLSTMPolicy', ScottLSTMPolicy)    \n",
    "    #print(\"lstm registered\")\n",
    "\n",
    "    #try:\n",
    "    \n",
    "    #code stopped at ep 36 at 10steps per ep, 80 ep\n",
    "    StepsPerEpisode=20 #was 10\n",
    "    TotalEpisodes=700  #was 80\n",
    "    env= gym.make(\"ur5-rl-v0\",StepsPerEpisode=StepsPerEpisode,TotalEpisodes=TotalEpisodes)\n",
    "    env = DummyVecEnv([lambda: env])\n",
    "\n",
    "    timesteps=(StepsPerEpisode*TotalEpisodes)\n",
    "    print(\"Total timesteps:\",timesteps)\n",
    "    \n",
    "    class ScottLSTMPolicy(LstmPolicy):\n",
    "        def __init__(self, sess, ob_space, ac_space, n_env=1, n_steps=StepsPerEpisode,\n",
    "                     n_batch=StepsPerEpisode, n_lstm=StepsPerEpisode, reuse=False,  **_kwargs):\n",
    "            super().__init__(sess, ob_space, ac_space, n_env, n_steps, n_batch, n_lstm, reuse,\n",
    "                             net_arch=[7,'lstm',dict(vf=[20, 15],pi=[20,15])],\n",
    "                             layer_norm=True, feature_extraction=\"mlp\", **_kwargs)\n",
    "            \n",
    "    #model = DQN(\"LnMlpPolicy\", env, learning_rate=1e-3, prioritized_replay=True,gamma=1 , buffer_size=50000,param_noise=False,\n",
    "    # exploration_initial_eps=0.1, exploration_final_eps=0.1,learning_starts=1, verbose=1)\n",
    "\n",
    "\n",
    "    #model = PPO2(\"MlpPolicy\", env,verboThe new research shows that \"we must expect extreme event records to be broken - not just by small marse=0)\n",
    "    #model = PPO2(\"MlpLstmPolicy\", env,nminibatches=1, n_steps=80, #exploration_initial_eps=1, exploration_final_eps=0.1,\n",
    "    #             verbose=0,tensorboard_log=\"./ScottPPOLstm/\") #n_lstm=2, n_batch=80, nminibatches=10,\n",
    "\n",
    "    model = PPO2(ScottLSTMPolicy, env,nminibatches=1, n_steps=StepsPerEpisode,learning_rate=0.001,\n",
    "                 verbose=2)# DEFAULT learning_rate=0.00025  #n_lstm=2, n_batch=80, nminibatches=10, #exploration_initial_eps=1, exploration_final_eps=0.1,\n",
    "\n",
    "    #model.learn(total_timesteps=timesteps,b_log_name=\"first_run\", reset_num_timesteps=False)#50000\n",
    "    model.learn(total_timesteps=timesteps,reset_num_timesteps=False)#50000\n",
    "    model.save(\"UR5-RL_savedpolicy-9-11-2021\")# save trained model\n",
    "    del model\n",
    "    print(\"training complete\")\n",
    "    \n",
    "\n",
    "finally:\n",
    "    endmsg='end'\n",
    "    data1=endmsg.encode('ascii')    \n",
    "    sock_SnS.sendall(data1)\n",
    "    sock_SnS.sendall(data1)\n",
    "    print('closing SnS socket')\n",
    "    sock_SnS.close()\n",
    "    \n",
    "    HOST2 = '192.168.0.103'\n",
    "    PORT2= PORT_SnS-10 #65481\n",
    "    sock_DC = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "    server_address_DC = (HOST2, PORT2)\n",
    "    sock_DC.close()\n",
    "    print(\"socket DC Closed\")\n",
    "      \n",
    "    #gitkraken\n",
    "    \n",
    "    #check_env(env)\n",
    "    #https://stable-baselines.readthedocs.io/en/master/modules/dqn.html\n",
    "    #model.learn(total_timesteps=25000)\n",
    "    #del model # remove to demonstrate saving and loading\n",
    "    #model = DQN.load(\"deepq_cartpole\")\n",
    "\n",
    "    \n",
    "    #Starting at 11:02AM\n",
    "    #80 episodes in 45 min\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved!\n"
     ]
    }
   ],
   "source": [
    "model.save(\"UR5-RL_savedpolicy-8-5to6-2021\")\n",
    "print(\"saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HOST2 = '192.168.0.103'learning_rate=0.00025\n",
    "PORT2= 65485\n",
    "sock_DC = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "server_address_DC = (HOST2, PORT2)\n",
    "sock_DC.close()\n",
    "print(\"socket DC Closed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[Errno 9] Bad file descriptor",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-5c92c6e2c5c7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mendmsg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'end'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdata1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mendmsg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ascii'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0msock_SnS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msendall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'closing SnS socket'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0msock_SnS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 9] Bad file descriptor"
     ]
    }
   ],
   "source": [
    "endmsg='end'\n",
    "data1=endmsg.encode('ascii')    \n",
    "sock_SnS.sendall(data1)\n",
    "\n",
    "\n",
    "#print('closing SnS socket')\n",
    "#sock_SnS.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "In /home/scott/.local/lib/python3.6/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The text.latex.preview rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /home/scott/.local/lib/python3.6/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The mathtext.fallback_to_cm rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /home/scott/.local/lib/python3.6/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: Support for setting the 'mathtext.fallback_to_cm' rcParam is deprecated since 3.3 and will be removed two minor releases later; use 'mathtext.fallback : 'cm' instead.\n",
      "In /home/scott/.local/lib/python3.6/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The validate_bool_maybe_none function was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /home/scott/.local/lib/python3.6/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The savefig.jpeg_quality rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /home/scott/.local/lib/python3.6/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The keymap.all_axes rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /home/scott/.local/lib/python3.6/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The animation.avconv_path rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /home/scott/.local/lib/python3.6/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The animation.avconv_args rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "/home/scott/.local/lib/python3.6/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/scott/.local/lib/python3.6/site-packages/stable_baselines/common/policies.py:346: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/scott/.local/lib/python3.6/site-packages/stable_baselines/common/policies.py:442: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.flatten instead.\n",
      "WARNING:tensorflow:From /home/scott/.local/lib/python3.6/site-packages/stable_baselines/common/tf_layers.py:121: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/scott/.local/lib/python3.6/site-packages/stable_baselines/common/tf_layers.py:123: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/scott/.local/lib/python3.6/site-packages/stable_baselines/common/distributions.py:326: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/scott/.local/lib/python3.6/site-packages/stable_baselines/common/distributions.py:327: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/scott/.local/lib/python3.6/site-packages/stable_baselines/ppo2/ppo2.py:190: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/scott/.local/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /home/scott/.local/lib/python3.6/site-packages/stable_baselines/ppo2/ppo2.py:206: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/scott/.local/lib/python3.6/site-packages/stable_baselines/ppo2/ppo2.py:242: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore',category=FutureWarning)\n",
    "import tensorflow as tf\n",
    "#import tensorflow.contrib.layers as layers\n",
    "import tensorflow as tf\n",
    "#from tensorflow import keras\n",
    "#from tensorflow.keras import layers\n",
    "#from keras.utils import to_categorical\n",
    "#from keras.models import Sequential\n",
    "#from keras.layers import Dense, Dropout, Embedding, LSTM, GlobalMaxPooling1D, SpatialDropout1D\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "\n",
    "import gym\n",
    "from stable_baselines import DQN,PPO2\n",
    "from stable_baselines import results_plotter\n",
    "from stable_baselines.bench import Monitor\n",
    "from stable_baselines.results_plotter import load_results, ts2xy\n",
    "from stable_baselines.common.noise import AdaptiveParamNoiseSpec\n",
    "from stable_baselines.common.callbacks import BaseCallback\n",
    "from stable_baselines.deepq.policies import MlpPolicy\n",
    "from stable_baselines.common.policies import FeedForwardPolicy, register_policy,LstmPolicy\n",
    "from stable_baselines.common.tf_layers import conv, linear, conv_to_fc, lstm\n",
    "#from stable_baselines.common import get_vec_normalize_env\n",
    "from stable_baselines.common.env_checker import check_env\n",
    "from stable_baselines.common.vec_env import DummyVecEnv\n",
    "#import balance_bot\n",
    "import MainEnv_RL\n",
    "\n",
    "env= gym.make(\"UR5-RL-env\", render=True)\n",
    "env = DummyVecEnv([lambda: env])\n",
    "\n",
    "#env=get_vec_normalize_env(env) \n",
    "#model = DQN.load(\"MainScott_RL\")\n",
    "model = PPO2.load(\"UR5-RL_savedpolicy\")\n",
    "#env=model.get_env()\n",
    "#obs = env.reset()\n",
    "done = [False for _ in range(1)] #env.num_envs\n",
    "state=None\n",
    "\n",
    "while True:\n",
    "    obs = env.reset()\n",
    "    #env._seed()\n",
    "    for i in range(80):\n",
    "        action, _states = model.predict(obs,state=state,mask=done)\n",
    "        #actionlist.append(action)\n",
    "        obs, rewards, dones, info = env.step(action)\n",
    "    print(rewards)\n",
    "        \n",
    "\"\"\"\n",
    "\n",
    "while True:\n",
    "    obs = env.reset()\n",
    "    env._seed()\n",
    "    actionlist=[]\n",
    "    #print(\"reset\")\n",
    "    for i in range(80):\n",
    "        action, _states = model.predict(obs)\n",
    "        actionlist.append(action)\n",
    "        obs, rewards, dones, info = env.step(action)\n",
    "    print(rewards)       \n",
    "\"\"\"       \n",
    "        \n",
    "        \n",
    "    #print(\"actionlist\",actionlist)\n",
    "    #env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
