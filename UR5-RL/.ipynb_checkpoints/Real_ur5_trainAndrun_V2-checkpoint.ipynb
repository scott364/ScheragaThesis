{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfor env in gym.envs.registry.env_specs:\\n     if 'MainEnvRL-v0' in env:\\n        print('Remove {} from registry'.format(env))\\n        del gym.registry.env_specs[env]\\n\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#To register MainEnvRL with local copy of Gym\n",
    "\"\"\"\n",
    "from gym.envs.registration import register\n",
    " \n",
    "register(\n",
    "    id='MainEnvRL-v2',\n",
    "    #entry_point='balance_bot.envs:BalancebotEnv',\n",
    "    entry_point='MainEnv_RL.envs:MainEnvRL',\n",
    ")\n",
    "\n",
    "\"\"\"\n",
    "#import gym\n",
    "#for env in gym.envs.registry.env_specs:\n",
    "#    print(env)\n",
    "    \n",
    "    #if 'MainEnvRL-v1' == env:\n",
    "    #     print('Remove {} from registry'.format(env))\n",
    "         #del gym.envs.registry.env_specs[env]\n",
    "            \n",
    "            \n",
    "#import MainEnv_RL\n",
    "\n",
    "#env = gym.make('MainEnvRL-v2')\n",
    "#env.reset()\n",
    "\"\"\"\n",
    "for env in gym.envs.registry.env_specs:\n",
    "     if 'MainEnvRL-v0' in env:\n",
    "        print('Remove {} from registry'.format(env))\n",
    "        del gym.registry.env_specs[env]\n",
    "\"\"\"                \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "connecting to 192.168.0.103 port 65495\n",
      "port= /dev/ttyACM0\n",
      "the red  robot is being used. Please change the bot identity variable if this is incorrect\n",
      "Connected to http://192.168.0.103:10000\n",
      "DC socket connecting to 192.168.0.103 port 65485\n",
      "TotalEpisodes: 700    StepsPerEpisode: 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/scott/anaconda3/envs/thesis/lib/python3.6/site-packages/stable_baselines/__init__.py:33: UserWarning: stable-baselines is in maintenance mode, please use [Stable-Baselines3 (SB3)](https://github.com/DLR-RM/stable-baselines3) for an up-to-date version. You can find a [migration guide](https://stable-baselines3.readthedocs.io/en/master/guide/migration.html) in SB3 documentation.\n",
      "  \"stable-baselines is in maintenance mode, please use [Stable-Baselines3 (SB3)](https://github.com/DLR-RM/stable-baselines3) for an up-to-date version. You can find a [migration guide](https://stable-baselines3.readthedocs.io/en/master/guide/migration.html) in SB3 documentation.\"\n",
      "/home/scott/anaconda3/envs/thesis/lib/python3.6/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total timesteps: 14000\n",
      "WARNING:tensorflow:From /home/scott/anaconda3/envs/thesis/lib/python3.6/site-packages/stable_baselines/common/tf_util.py:191: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/scott/anaconda3/envs/thesis/lib/python3.6/site-packages/stable_baselines/common/tf_util.py:200: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/scott/anaconda3/envs/thesis/lib/python3.6/site-packages/stable_baselines/common/policies.py:116: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/scott/anaconda3/envs/thesis/lib/python3.6/site-packages/stable_baselines/common/input.py:25: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/scott/anaconda3/envs/thesis/lib/python3.6/site-packages/stable_baselines/common/policies.py:442: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.flatten instead.\n",
      "WARNING:tensorflow:From /home/scott/anaconda3/envs/thesis/lib/python3.6/site-packages/tensorflow_core/python/layers/core.py:332: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n",
      "WARNING:tensorflow:From /home/scott/anaconda3/envs/thesis/lib/python3.6/site-packages/stable_baselines/common/tf_layers.py:123: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/scott/anaconda3/envs/thesis/lib/python3.6/site-packages/stable_baselines/common/distributions.py:418: The name tf.random_normal is deprecated. Please use tf.random.normal instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/scott/anaconda3/envs/thesis/lib/python3.6/site-packages/stable_baselines/ppo2/ppo2.py:190: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/scott/anaconda3/envs/thesis/lib/python3.6/site-packages/stable_baselines/ppo2/ppo2.py:198: The name tf.trainable_variables is deprecated. Please use tf.compat.v1.trainable_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/scott/anaconda3/envs/thesis/lib/python3.6/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /home/scott/anaconda3/envs/thesis/lib/python3.6/site-packages/stable_baselines/ppo2/ppo2.py:206: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/scott/anaconda3/envs/thesis/lib/python3.6/site-packages/stable_baselines/ppo2/ppo2.py:240: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/scott/anaconda3/envs/thesis/lib/python3.6/site-packages/stable_baselines/ppo2/ppo2.py:242: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n",
      "\n",
      "action [1. 1.]\n",
      "Ep: 1  tStep: 1 Z difference 0.05176454996466662  Reward: -0.3595492969390004\n",
      "action [-1.         0.3018654]\n",
      "Ep: 1  tStep: 2 Z difference 0.09911008192524307  Reward: -0.31220376497842395\n",
      "action [0.8165612 1.       ]\n",
      "Ep: 1  tStep: 3 Z difference 0.14990092261247323  Reward: -0.2614129242911938\n",
      "action [0.96433824 1.        ]\n",
      "Ep: 1  tStep: 4 Z difference 0.18482935131117717  Reward: -0.22648449559248984\n",
      "action [0.89381325 0.23748815]\n",
      "Ep: 1  tStep: 5 Z difference 0.19451921793892968  Reward: -0.21679462896473733\n",
      "action [0.6042229  0.73855317]\n",
      "Ep: 1  tStep: 6 Z difference 0.1972867874622346  Reward: -0.2140270594414324\n",
      "action [0.6976272 1.       ]\n",
      "Ep: 1  tStep: 7 Z difference 0.20256130271218709  Reward: -0.20875254419147993\n",
      "action [ 1.        -0.2826767]\n",
      "Ep: 1  tStep: 8 Z difference 0.20199092232659455  Reward: -0.20932292457707247\n",
      "action [-0.05317385 -0.62896204]\n",
      "Ep: 1  tStep: 9 Z difference 0.2074968750279398  Reward: -0.20381697187572723\n",
      "action [-0.04576856 -0.70088816]\n",
      "Ep: 1  tStep: 10 Z difference 0.21150875084884468  Reward: -0.19980509605482233\n",
      "action [ 1.         -0.02481931]\n",
      "Ep: 1  tStep: 11 Z difference 0.21551388007737726  Reward: -0.19579996682628975\n",
      "action [0.43425536 0.9516107 ]\n",
      "Ep: 1  tStep: 12 Z difference 0.2174805117540064  Reward: -0.1938333351496606\n",
      "action [-0.7236644  1.       ]\n",
      "Ep: 1  tStep: 13 Z difference 0.2223135651987045  Reward: -0.18900028170496252\n",
      "action [-0.20072669  0.091167  ]\n",
      "Ep: 1  tStep: 14 Z difference 0.22969947053119544  Reward: -0.18161437637247158\n",
      "action [ 1. -1.]\n",
      "Ep: 1  tStep: 15 Z difference 0.23272854384146635  Reward: -0.17858530306220066\n",
      "action [-0.5022313  1.       ]\n",
      "Ep: 1  tStep: 16 Z difference 0.23464736271128084  Reward: -0.17666648419238618\n",
      "action [-0.03614882 -0.16241705]\n",
      "Ep: 1  tStep: 17 Z difference 0.24005974310971823  Reward: -0.1712541037939488\n",
      "action [0.2830121 1.       ]\n",
      "Ep: 1  tStep: 18 Z difference 0.24776215162351756  Reward: -0.16355169528014946\n",
      "action [-1.        -0.8312618]\n",
      "Ep: 1  tStep: 19 Z difference 0.25235614103414106  Reward: -0.15895770586952596\n",
      "action [1. 1.]\n",
      "Ep: 1  tStep: 20 Z difference 0.25641406968124203  Reward: -0.15489977722242498\n",
      "-------------------------------------\n",
      "| approxkl           | 0.0014201258 |\n",
      "| clipfrac           | 0.0          |\n",
      "| explained_variance | -0.146       |\n",
      "| fps                | 0            |\n",
      "| n_updates          | 1            |\n",
      "| policy_entropy     | 2.8407893    |\n",
      "| policy_loss        | -0.016967803 |\n",
      "| serial_timesteps   | 20           |\n",
      "| time_elapsed       | 1.03e-05     |\n",
      "| total_timesteps    | 20           |\n",
      "| value_loss         | 0.9654937    |\n",
      "-------------------------------------\n",
      "action [-0.3138468  0.6859873]\n",
      "Ep: 2  tStep: 1 Z difference 0.049917010309919796  Reward: -0.3623357862539591\n",
      "action [-0.04279746  0.17315662]\n",
      "Ep: 2  tStep: 2 Z difference 0.1031907436553392  Reward: -0.3090620529085397\n",
      "action [-1.          0.84240615]\n",
      "Ep: 2  tStep: 3 Z difference 0.15224477680176518  Reward: -0.26000801976211374\n",
      "action [-1.          0.98071706]\n",
      "Ep: 2  tStep: 4 Z difference 0.18286125298403233  Reward: -0.2293915435798466\n",
      "action [-0.52869165  0.6086589 ]\n",
      "Ep: 2  tStep: 5 Z difference 0.1886996953573079  Reward: -0.22355310120657101\n",
      "action [-0.32451934 -1.        ]\n",
      "Ep: 2  tStep: 6 Z difference 0.19153018418774037  Reward: -0.22072261237613855\n",
      "action [0.98272    0.34817466]\n",
      "Ep: 2  tStep: 7 Z difference 0.19514401105865842  Reward: -0.2171087855052205\n",
      "action [-1.  1.]\n",
      "Ep: 2  tStep: 8 Z difference 0.19666712761931127  Reward: -0.21558566894456765\n",
      "action [0.7951247  0.02439243]\n",
      "Ep: 2  tStep: 9 Z difference 0.2010719191133976  Reward: -0.2111808774504813\n",
      "action [ 1.       -0.719081]\n",
      "Ep: 2  tStep: 10 Z difference 0.20526991288475704  Reward: -0.20698288367912188\n",
      "action [-0.8725786  0.7098699]\n",
      "Ep: 2  tStep: 11 Z difference 0.21274689721427897  Reward: -0.19950589934959995\n",
      "action [0.7620425 1.       ]\n",
      "Ep: 2  tStep: 12 Z difference 0.21475239512957645  Reward: -0.19750040143430247\n",
      "action [-1.         -0.10119575]\n",
      "Ep: 2  tStep: 13 Z difference 0.2180324123431001  Reward: -0.1942203842207788\n",
      "action [-1.          0.55760694]\n",
      "Ep: 2  tStep: 14 Z difference 0.22438843568339983  Reward: -0.1878643608804791\n",
      "action [1. 1.]\n",
      "Ep: 2  tStep: 15 Z difference 0.22682292887456734  Reward: -0.18542986768931158\n",
      "action [ 0.8808632  -0.22324294]\n",
      "Ep: 2  tStep: 16 Z difference 0.23109308185130395  Reward: -0.18115971471257497\n",
      "action [-0.70324576  1.        ]\n",
      "Ep: 2  tStep: 17 Z difference 0.23565465828552856  Reward: -0.17659813827835036\n",
      "action [-1.  1.]\n",
      "Ep: 2  tStep: 18 Z difference 0.23769623580351507  Reward: -0.17455656076036385\n",
      "action [ 1.        -0.1852315]\n",
      "Ep: 2  tStep: 19 Z difference 0.24362561050876996  Reward: -0.16862718605510896\n",
      "action [ 0.6511863 -1.       ]\n",
      "Ep: 2  tStep: 20 Z difference 0.2481405474565923  Reward: -0.1641122491072866\n",
      "-------------------------------------\n",
      "| approxkl           | 0.0027836282 |\n",
      "| clipfrac           | 0.025        |\n",
      "| explained_variance | -0.0733      |\n",
      "| fps                | 0            |\n",
      "| n_updates          | 2            |\n",
      "| policy_entropy     | 2.8483229    |\n",
      "| policy_loss        | 0.0046833456 |\n",
      "| serial_timesteps   | 40           |\n",
      "| time_elapsed       | 60.3         |\n",
      "| total_timesteps    | 40           |\n",
      "| value_loss         | 0.83497643   |\n",
      "-------------------------------------\n",
      "action [-0.7383516  -0.82364166]\n",
      "Ep: 3  tStep: 1 Z difference 0.052520461640507055  Reward: -0.3606096852619203\n",
      "action [ 1.         -0.77344567]\n",
      "Ep: 3  tStep: 2 Z difference 0.10059887886382635  Reward: -0.312531268038601\n",
      "action [-0.23212141  1.        ]\n",
      "Ep: 3  tStep: 3 Z difference 0.15348424315266307  Reward: -0.25964590374976426\n",
      "action [-0.8804396  0.8728982]\n",
      "Ep: 3  tStep: 4 Z difference 0.20035311369560693  Reward: -0.2127770332068204\n",
      "action [0.841067  0.9823125]\n",
      "Ep: 3  tStep: 5 Z difference 0.25533285492099855  Reward: -0.15779729198142878\n",
      "action [-0.7475262  0.5648537]\n",
      "Ep: 3  tStep: 6 Z difference 0.30301190320886695  Reward: -0.11011824369356038\n",
      "action [0.79890424 0.58808357]\n",
      "Ep: 3  tStep: 7 Z difference 0.3580859500624243  Reward: -0.05504419684000306\n",
      "action [-0.6432835  1.       ]\n",
      "Ep: 3  tStep: 8 Z difference 0.4039256719384343  Reward: -0.009204474963993015\n",
      "action [0.63168603 1.        ]\n",
      "Ep: 3  tStep: 9 Z difference 0.46175951506756263  Reward: 0.0486293681651353\n",
      "action [ 1.         -0.08300966]\n",
      "Ep: 3  tStep: 10 Z difference 0.5072645472958686  Reward: 0.09413440039344123\n",
      "action [-0.01119209  1.        ]\n",
      "Ep: 3  tStep: 11 Z difference 0.5565661510493609  Reward: 0.14343600414693358\n",
      "action [1. 1.]\n",
      "Ep: 3  tStep: 12 Z difference 0.5682735421266409  Reward: 0.1551433952242136\n",
      "action [-1.          0.44150054]\n",
      "BUTTON PRESSED! Episode over!\n",
      "*****Success condition achieved at tStep 13 Total Successes: 1 *****\n",
      "Ep: 3  tStep: 13 Z difference 0.5806561791013929  Reward: 0.8175260321989655\n",
      "action [0.9011452 1.       ]\n",
      "Ep: 4  tStep: 1 Z difference 0.05145523337088509  Reward: -0.360318848464638\n",
      "action [1. 1.]\n",
      "Ep: 4  tStep: 2 Z difference 0.10253969749137726  Reward: -0.30923438434414585\n",
      "action [0.09169716 1.        ]\n",
      "Ep: 4  tStep: 3 Z difference 0.14989930929690587  Reward: -0.26187477253861724\n",
      "action [-0.39779285  1.        ]\n",
      "Ep: 4  tStep: 4 Z difference 0.1890524248063561  Reward: -0.22272165702916702\n",
      "action [0.2937448 1.       ]\n",
      "Ep: 4  tStep: 5 Z difference 0.2022321863364427  Reward: -0.20954189549908042\n",
      "action [-0.35096383  0.7064152 ]\n",
      "Ep: 4  tStep: 6 Z difference 0.20482449112310963  Reward: -0.20694959071241348\n",
      "action [-0.975006   -0.17462862]\n",
      "Ep: 4  tStep: 7 Z difference 0.21261504533290854  Reward: -0.19915903650261457\n",
      "-------------------------------------\n",
      "| approxkl           | 0.0063864635 |\n",
      "| clipfrac           | 0.075        |\n",
      "| explained_variance | -0.178       |\n",
      "| fps                | 0            |\n",
      "| n_updates          | 3            |\n",
      "| policy_entropy     | 2.8555398    |\n",
      "| policy_loss        | -0.008053973 |\n",
      "| serial_timesteps   | 60           |\n",
      "| time_elapsed       | 95.6         |\n",
      "| total_timesteps    | 60           |\n",
      "| value_loss         | 0.93174446   |\n",
      "-------------------------------------\n",
      "action [1.         0.67945415]\n",
      "Ep: 4  tStep: 8 Z difference 0.21429436017349346  Reward: -0.19747972166202965\n",
      "action [-0.16504057  0.7021784 ]\n",
      "Ep: 4  tStep: 9 Z difference 0.2192275858484205  Reward: -0.1925464959871026\n",
      "action [-1.  1.]\n",
      "Ep: 4  tStep: 10 Z difference 0.2237304962620139  Reward: -0.1880435855735092\n",
      "action [0.01679578 0.34652552]\n",
      "Ep: 4  tStep: 11 Z difference 0.22719721808619786  Reward: -0.18457686374932525\n",
      "action [1.         0.74773955]\n",
      "Ep: 4  tStep: 12 Z difference 0.23329291095994398  Reward: -0.17848117087557913\n",
      "action [-0.14494228 -0.95081717]\n",
      "Ep: 4  tStep: 13 Z difference 0.23696056390479203  Reward: -0.17481351793073108\n",
      "action [-1.  1.]\n",
      "Ep: 4  tStep: 14 Z difference 0.2420108282908795  Reward: -0.1697632535446436\n",
      "action [1. 1.]\n",
      "Ep: 4  tStep: 15 Z difference 0.24986679511368237  Reward: -0.16190728672184074\n",
      "action [0.46441472 1.        ]\n",
      "Ep: 4  tStep: 16 Z difference 0.25439463858604405  Reward: -0.15737944324947906\n",
      "action [-1.  1.]\n",
      "Ep: 4  tStep: 17 Z difference 0.2599453241281209  Reward: -0.15182875770740223\n",
      "action [-0.31537673  1.        ]\n",
      "Ep: 4  tStep: 18 Z difference 0.2631366089854388  Reward: -0.14863747285008433\n",
      "action [-0.39063722  0.3367709 ]\n",
      "Ep: 4  tStep: 19 Z difference 0.26932646082229894  Reward: -0.14244762101322417\n",
      "action [0.4691089  0.27413374]\n",
      "Ep: 4  tStep: 20 Z difference 0.27690757733844196  Reward: -0.13486650449708115\n",
      "action [0.11783174 0.63605905]\n",
      "Ep: 5  tStep: 1 Z difference 0.05125635556094332  Reward: -0.3607176307398827\n",
      "action [0.37320647 0.0279265 ]\n",
      "Ep: 5  tStep: 2 Z difference 0.10465974081195872  Reward: -0.3073142454888673\n",
      "action [1. 1.]\n",
      "Ep: 5  tStep: 3 Z difference 0.1534955363616346  Reward: -0.25847844993919145\n",
      "action [0.20698315 0.62711924]\n",
      "Ep: 5  tStep: 4 Z difference 0.194711642486602  Reward: -0.21726234381422405\n",
      "action [-0.01009753  1.        ]\n",
      "Ep: 5  tStep: 5 Z difference 0.20883563361875712  Reward: -0.20313835268206892\n",
      "action [ 0.11816059 -0.3163519 ]\n",
      "Ep: 5  tStep: 6 Z difference 0.21024596475474544  Reward: -0.2017280215460806\n",
      "action [-0.13107985  1.        ]\n",
      "Ep: 5  tStep: 7 Z difference 0.21745968531668192  Reward: -0.19451430098414413\n",
      "-------------------------------------\n",
      "| approxkl           | 0.001466461  |\n",
      "| clipfrac           | 0.0125       |\n",
      "| explained_variance | 0.0245       |\n",
      "| fps                | 0            |\n",
      "| n_updates          | 4            |\n",
      "| policy_entropy     | 2.8607116    |\n",
      "| policy_loss        | -0.009143435 |\n",
      "| serial_timesteps   | 80           |\n",
      "| time_elapsed       | 131          |\n",
      "| total_timesteps    | 80           |\n",
      "| value_loss         | 0.25607535   |\n",
      "-------------------------------------\n",
      "action [-0.3266633  1.       ]\n",
      "Ep: 5  tStep: 8 Z difference 0.21800087935700985  Reward: -0.1939731069438162\n",
      "action [ 0.2936448  -0.21061832]\n",
      "Ep: 5  tStep: 9 Z difference 0.22085498126074654  Reward: -0.1911190050400795\n",
      "action [ 1.         -0.39049006]\n",
      "Ep: 5  tStep: 10 Z difference 0.2273820160511879  Reward: -0.18459197024963814\n",
      "action [0.84327495 1.        ]\n",
      "Ep: 5  tStep: 11 Z difference 0.2333388171210884  Reward: -0.17863516917973765\n",
      "action [ 0.08304814 -0.58203495]\n",
      "Ep: 5  tStep: 12 Z difference 0.23570232442729155  Reward: -0.1762716618735345\n",
      "action [0.22767389 0.3581469 ]\n",
      "Ep: 5  tStep: 13 Z difference 0.24118173075430072  Reward: -0.17079225554652533\n",
      "action [-0.18383735 -0.171722  ]\n",
      "Ep: 5  tStep: 14 Z difference 0.24657915131747732  Reward: -0.16539483498334873\n",
      "action [-1.         0.6548788]\n",
      "Ep: 5  tStep: 15 Z difference 0.2507108524855224  Reward: -0.16126313381530366\n",
      "action [0.4523826  0.92588663]\n",
      "Ep: 5  tStep: 16 Z difference 0.26069287589602164  Reward: -0.1512811104048044\n",
      "action [0.30590934 1.        ]\n",
      "Ep: 5  tStep: 17 Z difference 0.25823007634989903  Reward: -0.15374390995092702\n",
      "action [-1.  1.]\n",
      "Ep: 5  tStep: 18 Z difference 0.2608668206471947  Reward: -0.15110716565363136\n",
      "action [1. 1.]\n",
      "Ep: 5  tStep: 19 Z difference 0.267485374429822  Reward: -0.14448861187100404\n",
      "action [-0.21532476  1.        ]\n",
      "Ep: 5  tStep: 20 Z difference 0.2700319197203962  Reward: -0.14194206658042985\n",
      "action [0.36656138 1.        ]\n",
      "Ep: 6  tStep: 1 Z difference 0.05230750398561357  Reward: -0.3603241284064951\n",
      "action [1.       0.909142]\n",
      "Ep: 6  tStep: 2 Z difference 0.10527705401405685  Reward: -0.3073545783780518\n",
      "action [-0.3161665  0.544138 ]\n",
      "Ep: 6  tStep: 3 Z difference 0.1519303269311787  Reward: -0.26070130546092996\n",
      "action [0.40608543 1.        ]\n",
      "Ep: 6  tStep: 4 Z difference 0.1916393029861152  Reward: -0.22099232940599345\n",
      "action [1. 1.]\n",
      "Ep: 6  tStep: 5 Z difference 0.19737111986689282  Reward: -0.21526051252521583\n",
      "action [-0.54873383 -0.24629873]\n",
      "Ep: 6  tStep: 6 Z difference 0.20184557726047947  Reward: -0.21078605513162918\n",
      "action [1. 1.]\n",
      "Ep: 6  tStep: 7 Z difference 0.20762946023456763  Reward: -0.20500217215754102\n",
      "-------------------------------------\n",
      "| approxkl           | 0.0037824954 |\n",
      "| clipfrac           | 0.05         |\n",
      "| explained_variance | 0.189        |\n",
      "| fps                | 0            |\n",
      "| n_updates          | 5            |\n",
      "| policy_entropy     | 2.862896     |\n",
      "| policy_loss        | -0.016446618 |\n",
      "| serial_timesteps   | 100          |\n",
      "| time_elapsed       | 167          |\n",
      "| total_timesteps    | 100          |\n",
      "| value_loss         | 0.21473302   |\n",
      "-------------------------------------\n",
      "action [-0.69790506 -0.62712705]\n",
      "Ep: 6  tStep: 8 Z difference 0.20985040911063546  Reward: -0.2027812232814732\n",
      "action [-0.35359043  1.        ]\n",
      "Ep: 6  tStep: 9 Z difference 0.21698991715647287  Reward: -0.19564171523563578\n",
      "action [1. 1.]\n",
      "Ep: 6  tStep: 10 Z difference 0.21822586354613316  Reward: -0.1944057688459755\n",
      "action [0.44687024 0.37697148]\n",
      "Ep: 6  tStep: 11 Z difference 0.22210603415071972  Reward: -0.19052559824138893\n",
      "action [ 1.         -0.04786313]\n",
      "Ep: 6  tStep: 12 Z difference 0.22735561634190393  Reward: -0.18527601605020472\n",
      "action [0.01381977 0.18937379]\n",
      "Ep: 6  tStep: 13 Z difference 0.23193801921345303  Reward: -0.18069361317865562\n",
      "action [0.9866433 1.       ]\n",
      "Ep: 6  tStep: 14 Z difference 0.23915071312002834  Reward: -0.17348091927208031\n",
      "action [1.         0.56161577]\n",
      "Ep: 6  tStep: 15 Z difference 0.24304951018616583  Reward: -0.16958212220594282\n",
      "action [-0.8780827  -0.25319982]\n",
      "Ep: 6  tStep: 16 Z difference 0.24629286113679427  Reward: -0.16633877125531438\n",
      "action [0.22691883 1.        ]\n",
      "Ep: 6  tStep: 17 Z difference 0.2542762798894196  Reward: -0.15835535250268906\n",
      "action [0.8604896 1.       ]\n",
      "Ep: 6  tStep: 18 Z difference 0.2566079608794305  Reward: -0.15602367151267815\n",
      "action [1. 1.]\n",
      "Ep: 6  tStep: 19 Z difference 0.25860788552276803  Reward: -0.15402374686934062\n",
      "action [-1.  1.]\n",
      "Ep: 6  tStep: 20 Z difference 0.2649731487613174  Reward: -0.14765848363079126\n",
      "action [5.8212876e-04 8.9429629e-01]\n",
      "Ep: 7  tStep: 1 Z difference 0.05091447932571169  Reward: -0.3599199195243421\n",
      "action [-0.26279312  0.8666798 ]\n",
      "Ep: 7  tStep: 2 Z difference 0.101898917881027  Reward: -0.3089354809690268\n",
      "action [1. 1.]\n",
      "Ep: 7  tStep: 3 Z difference 0.1505173558242623  Reward: -0.2603170430257915\n",
      "action [-0.81665003  1.        ]\n",
      "Ep: 7  tStep: 4 Z difference 0.18842205841466786  Reward: -0.22241234043538594\n",
      "action [1. 1.]\n",
      "Ep: 7  tStep: 5 Z difference 0.1986924253165725  Reward: -0.2121419735334813\n",
      "action [-1.  1.]\n",
      "Ep: 7  tStep: 6 Z difference 0.20030647420920467  Reward: -0.21052792464084913\n",
      "action [-1.  1.]\n",
      "Ep: 7  tStep: 7 Z difference 0.20539868480004397  Reward: -0.20543571405000982\n",
      "-------------------------------------\n",
      "| approxkl           | 0.0043581566 |\n",
      "| clipfrac           | 0.0625       |\n",
      "| explained_variance | 0.283        |\n",
      "| fps                | 0            |\n",
      "| n_updates          | 6            |\n",
      "| policy_entropy     | 2.8662088    |\n",
      "| policy_loss        | -0.013442306 |\n",
      "| serial_timesteps   | 120          |\n",
      "| time_elapsed       | 202          |\n",
      "| total_timesteps    | 120          |\n",
      "| value_loss         | 0.20039427   |\n",
      "-------------------------------------\n",
      "action [0.3799798 1.       ]\n",
      "Ep: 7  tStep: 8 Z difference 0.20913849695026876  Reward: -0.20169590189978504\n",
      "action [-0.5332081  -0.26585495]\n",
      "Ep: 7  tStep: 9 Z difference 0.21542822768725456  Reward: -0.19540617116279924\n",
      "action [0.22884968 1.        ]\n",
      "Ep: 7  tStep: 10 Z difference 0.22093799367994071  Reward: -0.18989640517011308\n",
      "action [-0.54044694  0.8909689 ]\n",
      "Ep: 7  tStep: 11 Z difference 0.22329006111212113  Reward: -0.18754433773793266\n",
      "action [-0.5789784  1.       ]\n",
      "Ep: 7  tStep: 12 Z difference 0.23078435191772906  Reward: -0.18005004693232474\n",
      "action [-0.42835632  0.7944028 ]\n",
      "Ep: 7  tStep: 13 Z difference 0.23253157267719526  Reward: -0.17830282617285853\n",
      "action [0.8118978 1.       ]\n",
      "Ep: 7  tStep: 14 Z difference 0.23786798057891456  Reward: -0.17296641827113923\n",
      "action [ 0.7280775  -0.09729826]\n",
      "Ep: 7  tStep: 15 Z difference 0.24155763328149948  Reward: -0.1692767655685543\n",
      "action [ 0.78691   -0.4917301]\n",
      "Ep: 7  tStep: 16 Z difference 0.2508598641779276  Reward: -0.1599745346721262\n",
      "action [-0.6239797  1.       ]\n",
      "Ep: 7  tStep: 17 Z difference 0.25556062574610117  Reward: -0.15527377310395263\n",
      "action [1. 1.]\n",
      "Ep: 7  tStep: 18 Z difference 0.26394517341479684  Reward: -0.14688922543525695\n",
      "action [-0.32736444  0.6566413 ]\n",
      "Ep: 7  tStep: 19 Z difference 0.2665163584340364  Reward: -0.14431804041601737\n",
      "action [1. 1.]\n",
      "Ep: 7  tStep: 20 Z difference 0.2712424930561337  Reward: -0.1395919057939201\n",
      "action [-0.57762843 -0.29400706]\n",
      "Ep: 8  tStep: 1 Z difference 0.048781529480591335  Reward: -0.3620401095099748\n",
      "action [0.8191279 1.       ]\n",
      "Ep: 8  tStep: 2 Z difference 0.10271422890275694  Reward: -0.3081074100878092\n",
      "action [1. 1.]\n",
      "Ep: 8  tStep: 3 Z difference 0.15138399961404492  Reward: -0.2594376393765212\n",
      "action [0.05633775 1.        ]\n",
      "Ep: 8  tStep: 4 Z difference 0.19400383694767953  Reward: -0.2168178020428866\n",
      "action [-0.1462749  1.       ]\n",
      "Ep: 8  tStep: 5 Z difference 0.21159278992339958  Reward: -0.19922884906716654\n",
      "action [1.        0.8629361]\n",
      "Ep: 8  tStep: 6 Z difference 0.21202999844215809  Reward: -0.19879164054840803\n",
      "action [0.39386967 1.        ]\n",
      "Ep: 8  tStep: 7 Z difference 0.2196624477263538  Reward: -0.19115919126421232\n",
      "-------------------------------------\n",
      "| approxkl           | 0.0061486596 |\n",
      "| clipfrac           | 0.0875       |\n",
      "| explained_variance | 0.347        |\n",
      "| fps                | 0            |\n",
      "| n_updates          | 7            |\n",
      "| policy_entropy     | 2.8705742    |\n",
      "| policy_loss        | -0.015736595 |\n",
      "| serial_timesteps   | 140          |\n",
      "| time_elapsed       | 238          |\n",
      "| total_timesteps    | 140          |\n",
      "| value_loss         | 0.17791685   |\n",
      "-------------------------------------\n",
      "action [1. 1.]\n",
      "Ep: 8  tStep: 8 Z difference 0.22085923454724243  Reward: -0.18996240444332368\n",
      "action [0.10534047 0.9933457 ]\n",
      "Ep: 8  tStep: 9 Z difference 0.2253354519214481  Reward: -0.185486187069118\n",
      "action [0.12264294 0.52898073]\n",
      "Ep: 8  tStep: 10 Z difference 0.22930538153760116  Reward: -0.18151625745296496\n",
      "action [-0.12049406 -0.08187068]\n",
      "Ep: 8  tStep: 11 Z difference 0.2355427528511731  Reward: -0.17527888613939302\n",
      "action [-0.48305216  1.        ]\n",
      "Ep: 8  tStep: 12 Z difference 0.23718378811329588  Reward: -0.17363785087727024\n",
      "action [-0.6602142  1.       ]\n",
      "Ep: 8  tStep: 13 Z difference 0.24456177353300124  Reward: -0.16625986545756488\n",
      "action [-0.75329536  1.        ]\n",
      "Ep: 8  tStep: 14 Z difference 0.24540671089515076  Reward: -0.16541492809541536\n",
      "action [0.6352453 1.       ]\n",
      "Ep: 8  tStep: 15 Z difference 0.2528586155008523  Reward: -0.1579630234897138\n",
      "action [-1.  1.]\n",
      "Ep: 8  tStep: 16 Z difference 0.25590470195710635  Reward: -0.15491693703345977\n",
      "action [-0.87518424  1.        ]\n",
      "Ep: 8  tStep: 17 Z difference 0.2606833426676691  Reward: -0.150138296322897\n",
      "action [1. 1.]\n",
      "Ep: 8  tStep: 18 Z difference 0.26675967575460646  Reward: -0.14406196323595966\n",
      "action [0.31869823 1.        ]\n",
      "Ep: 8  tStep: 19 Z difference 0.27018621135465803  Reward: -0.14063542763590808\n",
      "action [-0.5611153  1.       ]\n",
      "Ep: 8  tStep: 20 Z difference 0.2766792198531327  Reward: -0.1341424191374334\n",
      "action [0.02074334 1.        ]\n",
      "Ep: 9  tStep: 1 Z difference 0.051706910599395695  Reward: -0.3605797655913978\n",
      "action [-0.14569527  1.        ]\n",
      "Ep: 9  tStep: 2 Z difference 0.10359949915409095  Reward: -0.30868717703670256\n",
      "action [-1.  1.]\n",
      "Ep: 9  tStep: 3 Z difference 0.15047276964858147  Reward: -0.26181390654221204\n",
      "action [0.19956934 0.3921559 ]\n",
      "Ep: 9  tStep: 4 Z difference 0.1852576132617889  Reward: -0.2270290629290046\n",
      "action [-0.54497993  0.35341966]\n",
      "Ep: 9  tStep: 5 Z difference 0.19664967447817316  Reward: -0.21563700171262035\n",
      "action [-1.  1.]\n",
      "Ep: 9  tStep: 6 Z difference 0.20188371017388995  Reward: -0.21040296601690356\n",
      "action [0.83842915 1.        ]\n",
      "Ep: 9  tStep: 7 Z difference 0.2061506365194914  Reward: -0.20613603967130212\n",
      "--------------------------------------\n",
      "| approxkl           | 0.00040218185 |\n",
      "| clipfrac           | 0.0           |\n",
      "| explained_variance | 0.379         |\n",
      "| fps                | 0             |\n",
      "| n_updates          | 8             |\n",
      "| policy_entropy     | 2.8725135     |\n",
      "| policy_loss        | 7.02769e-05   |\n",
      "| serial_timesteps   | 160           |\n",
      "| time_elapsed       | 273           |\n",
      "| total_timesteps    | 160           |\n",
      "| value_loss         | 0.18542717    |\n",
      "--------------------------------------\n",
      "action [0.26852125 1.        ]\n",
      "Ep: 9  tStep: 8 Z difference 0.21301822755970035  Reward: -0.19926844863109316\n",
      "action [1. 1.]\n",
      "Ep: 9  tStep: 9 Z difference 0.21544406751282486  Reward: -0.19684260867796866\n",
      "action [ 0.2687246  -0.41163766]\n",
      "Ep: 9  tStep: 10 Z difference 0.21889876280277942  Reward: -0.1933879133880141\n",
      "action [1. 1.]\n",
      "Ep: 9  tStep: 11 Z difference 0.22460022001788005  Reward: -0.18768645617291346\n",
      "action [0.03308782 1.        ]\n",
      "Ep: 9  tStep: 12 Z difference 0.22640874676890643  Reward: -0.18587792942188708\n",
      "action [-0.83860207  1.        ]\n",
      "Ep: 9  tStep: 13 Z difference 0.2300838796313851  Reward: -0.1822027965594084\n",
      "action [-0.45428815  1.        ]\n",
      "Ep: 9  tStep: 14 Z difference 0.23486794694885615  Reward: -0.17741872924193736\n",
      "action [-0.83120286  1.        ]\n",
      "Ep: 9  tStep: 15 Z difference 0.23679292575083677  Reward: -0.17549375043995674\n",
      "action [-0.44003847 -0.98398316]\n",
      "Ep: 9  tStep: 16 Z difference 0.24342863934449843  Reward: -0.16885803684629508\n",
      "action [0.04987935 1.        ]\n",
      "Ep: 9  tStep: 17 Z difference 0.2542495868500323  Reward: -0.15803708934076122\n",
      "action [-0.003359    0.90371066]\n",
      "Ep: 9  tStep: 18 Z difference 0.25285832217074944  Reward: -0.15942835402004407\n",
      "action [-0.54826903  1.        ]\n",
      "Ep: 9  tStep: 19 Z difference 0.2589378818888215  Reward: -0.153348794301972\n",
      "action [0.18216358 1.        ]\n",
      "Ep: 9  tStep: 20 Z difference 0.2656035151530056  Reward: -0.1466831610377879\n",
      "action [1. 1.]\n",
      "Ep: 10  tStep: 1 Z difference 0.053863913512974815  Reward: -0.35793730135709056\n",
      "action [0.70786303 1.        ]\n",
      "Ep: 10  tStep: 2 Z difference 0.10322022333070624  Reward: -0.30858099153935914\n",
      "action [-1.          0.66685855]\n",
      "Ep: 10  tStep: 3 Z difference 0.14952238011434638  Reward: -0.262278834755719\n",
      "action [-1.  1.]\n",
      "Ep: 10  tStep: 4 Z difference 0.1877214394632727  Reward: -0.2240797754067927\n",
      "action [1.         0.40546042]\n",
      "Ep: 10  tStep: 5 Z difference 0.1999197184681889  Reward: -0.21188149640187648\n",
      "action [0.39939585 1.        ]\n",
      "Ep: 10  tStep: 6 Z difference 0.2018715369746089  Reward: -0.20992967789545647\n",
      "action [-0.9678232  1.       ]\n",
      "Ep: 10  tStep: 7 Z difference 0.2075606743253764  Reward: -0.204240540544689\n",
      "-------------------------------------\n",
      "| approxkl           | 0.0005128102 |\n",
      "| clipfrac           | 0.0          |\n",
      "| explained_variance | 0.467        |\n",
      "| fps                | 0            |\n",
      "| n_updates          | 9            |\n",
      "| policy_entropy     | 2.8718395    |\n",
      "| policy_loss        | -0.004644261 |\n",
      "| serial_timesteps   | 180          |\n",
      "| time_elapsed       | 308          |\n",
      "| total_timesteps    | 180          |\n",
      "| value_loss         | 0.16633397   |\n",
      "-------------------------------------\n",
      "action [-0.46241713  0.45778513]\n",
      "Ep: 10  tStep: 8 Z difference 0.21094189042448974  Reward: -0.20085932444557564\n",
      "action [-1.  1.]\n",
      "Ep: 10  tStep: 9 Z difference 0.21726623411364843  Reward: -0.19453498075641695\n",
      "action [-1.         0.8907066]\n",
      "Ep: 10  tStep: 10 Z difference 0.22081494170166538  Reward: -0.1909862731684\n",
      "action [1. 1.]\n",
      "Ep: 10  tStep: 11 Z difference 0.22607611043192444  Reward: -0.18572510443814094\n",
      "action [0.268639 1.      ]\n",
      "Ep: 10  tStep: 12 Z difference 0.23305868687257147  Reward: -0.1787425279974939\n",
      "action [1. 1.]\n",
      "Ep: 10  tStep: 13 Z difference 0.23821161679476477  Reward: -0.1735895980753006\n",
      "action [-1.  1.]\n",
      "Ep: 10  tStep: 14 Z difference 0.23985162540152638  Reward: -0.171949589468539\n",
      "action [-1.  1.]\n",
      "Ep: 10  tStep: 15 Z difference 0.2438921009074897  Reward: -0.16790911396257568\n",
      "action [1. 1.]\n",
      "Ep: 10  tStep: 16 Z difference 0.2525526722032576  Reward: -0.15924854266680777\n",
      "action [-0.7805465  -0.08124113]\n",
      "Ep: 10  tStep: 17 Z difference 0.2537872986074534  Reward: -0.15801391626261196\n",
      "action [-0.413151  1.      ]\n",
      "Ep: 10  tStep: 18 Z difference 0.2590317475218322  Reward: -0.15276946734823316\n",
      "action [0.43589404 1.        ]\n",
      "Ep: 10  tStep: 19 Z difference 0.2601497752100226  Reward: -0.1516514396600428\n",
      "action [-1.  1.]\n",
      "Ep: 10  tStep: 20 Z difference 0.2667202228557315  Reward: -0.1450809920143339\n",
      "action [-0.7688806  1.       ]\n",
      "Ep: 11  tStep: 1 Z difference 0.050057075434178255  Reward: -0.36112741289399564\n",
      "action [-0.36424682 -0.5840237 ]\n",
      "Ep: 11  tStep: 2 Z difference 0.10319162364564827  Reward: -0.3079928646825256\n",
      "action [-0.60058755  1.        ]\n",
      "Ep: 11  tStep: 3 Z difference 0.15299042192399526  Reward: -0.25819406640417863\n",
      "action [0.02780085 1.        ]\n",
      "Ep: 11  tStep: 4 Z difference 0.18795111693404642  Reward: -0.22323337139412747\n",
      "action [-0.23293237  0.9443737 ]\n",
      "Ep: 11  tStep: 5 Z difference 0.19490333370901647  Reward: -0.21628115461915742\n",
      "action [0.3960436 1.       ]\n",
      "Ep: 11  tStep: 6 Z difference 0.2003164474327117  Reward: -0.21086804089546218\n",
      "action [-0.8161699  1.       ]\n",
      "Ep: 11  tStep: 7 Z difference 0.20622294238992023  Reward: -0.20496154593825366\n",
      "--------------------------------------\n",
      "| approxkl           | 0.00062410044 |\n",
      "| clipfrac           | 0.0           |\n",
      "| explained_variance | 0.61          |\n",
      "| fps                | 0             |\n",
      "| n_updates          | 10            |\n",
      "| policy_entropy     | 2.8704247     |\n",
      "| policy_loss        | -0.008108761  |\n",
      "| serial_timesteps   | 200           |\n",
      "| time_elapsed       | 344           |\n",
      "| total_timesteps    | 200           |\n",
      "| value_loss         | 0.12501897    |\n",
      "--------------------------------------\n",
      "action [0.16311046 1.        ]\n",
      "Ep: 11  tStep: 8 Z difference 0.21163326947763572  Reward: -0.19955121885053817\n",
      "action [0.40134314 1.        ]\n",
      "Ep: 11  tStep: 9 Z difference 0.21447490485198806  Reward: -0.19670958347618583\n",
      "action [0.11419557 1.        ]\n",
      "Ep: 11  tStep: 10 Z difference 0.21961360826417797  Reward: -0.19157088006399592\n",
      "action [0.65302706 1.        ]\n",
      "Ep: 11  tStep: 11 Z difference 0.22320015543550253  Reward: -0.18798433289267136\n",
      "action [1. 1.]\n",
      "Ep: 11  tStep: 12 Z difference 0.22558800914026822  Reward: -0.18559647918790567\n",
      "action [0.7868969 1.       ]\n",
      "Ep: 11  tStep: 13 Z difference 0.231003322839737  Reward: -0.18018116548843688\n",
      "action [0.23279336 0.02252543]\n",
      "Ep: 11  tStep: 14 Z difference 0.23361645406372844  Reward: -0.17756803426444545\n",
      "action [0.1797137 0.9808622]\n",
      "Ep: 11  tStep: 15 Z difference 0.2398948915917427  Reward: -0.17128959673643118\n",
      "action [-0.8654659   0.40334904]\n",
      "Ep: 11  tStep: 16 Z difference 0.24507407455816876  Reward: -0.16611041377000513\n",
      "action [-1.          0.30951095]\n",
      "Ep: 11  tStep: 17 Z difference 0.24886712612211692  Reward: -0.16231736220605697\n",
      "action [-0.24621578  1.        ]\n",
      "Ep: 11  tStep: 18 Z difference 0.25284864227734527  Reward: -0.15833584605082862\n",
      "action [0.94125897 1.        ]\n",
      "Ep: 11  tStep: 19 Z difference 0.2606218900110573  Reward: -0.1505625983171166\n",
      "action [0.3594714 1.       ]\n",
      "Ep: 11  tStep: 20 Z difference 0.2631596353985368  Reward: -0.14802485292963707\n",
      "action [0.36736062 1.        ]\n",
      "Ep: 12  tStep: 1 Z difference 0.052123292680829714  Reward: -0.3598162273328751\n",
      "action [ 0.1476825 -0.4829265]\n",
      "Ep: 12  tStep: 2 Z difference 0.1005938922520726  Reward: -0.3113456277616322\n",
      "action [0.02432618 0.11337566]\n",
      "Ep: 12  tStep: 3 Z difference 0.15435147360265233  Reward: -0.2575880464110525\n",
      "action [1. 1.]\n",
      "Ep: 12  tStep: 4 Z difference 0.20012050292380135  Reward: -0.21181901708990347\n",
      "action [-0.85827285  1.        ]\n",
      "Ep: 12  tStep: 5 Z difference 0.21039409645684026  Reward: -0.20154542355686456\n",
      "action [-1.  1.]\n",
      "Ep: 12  tStep: 6 Z difference 0.2139461773410436  Reward: -0.19799334267266122\n",
      "action [-0.4249246  1.       ]\n",
      "Ep: 12  tStep: 7 Z difference 0.22101469950191666  Reward: -0.19092482051178816\n",
      "--------------------------------------\n",
      "| approxkl           | 0.00057354796 |\n",
      "| clipfrac           | 0.0           |\n",
      "| explained_variance | 0.767         |\n",
      "| fps                | 0             |\n",
      "| n_updates          | 11            |\n",
      "| policy_entropy     | 2.8718624     |\n",
      "| policy_loss        | -0.0021991357 |\n",
      "| serial_timesteps   | 220           |\n",
      "| time_elapsed       | 379           |\n",
      "| total_timesteps    | 220           |\n",
      "| value_loss         | 0.07880759    |\n",
      "--------------------------------------\n",
      "action [0.7078328 1.       ]\n",
      "Ep: 12  tStep: 8 Z difference 0.22651023898459943  Reward: -0.1854292810291054\n",
      "action [-0.11034741  0.7588697 ]\n",
      "Ep: 12  tStep: 9 Z difference 0.22716040515825142  Reward: -0.1847791148554534\n",
      "action [-0.6385405  1.       ]\n",
      "Ep: 12  tStep: 10 Z difference 0.23069781953729684  Reward: -0.18124170047640797\n",
      "action [-1.  1.]\n",
      "Ep: 12  tStep: 11 Z difference 0.23802476551905238  Reward: -0.17391475449465243\n",
      "action [0.06492001 1.        ]\n",
      "Ep: 12  tStep: 12 Z difference 0.23997922399640048  Reward: -0.17196029601730434\n",
      "action [0.3995427  0.92487353]\n",
      "Ep: 12  tStep: 13 Z difference 0.24646108595095573  Reward: -0.1654784340627491\n",
      "action [-1.  1.]\n",
      "Ep: 12  tStep: 14 Z difference 0.248159760578349  Reward: -0.16377975943535583\n",
      "action [-0.12677565  1.        ]\n",
      "Ep: 12  tStep: 15 Z difference 0.25304708009213206  Reward: -0.15889243992157276\n",
      "action [-0.2016125  1.       ]\n",
      "Ep: 12  tStep: 16 Z difference 0.2574828179121016  Reward: -0.15445670210160323\n",
      "action [0.014977 1.      ]\n",
      "Ep: 12  tStep: 17 Z difference 0.268315058626607  Reward: -0.14362446138709783\n",
      "action [-1.  1.]\n",
      "Ep: 12  tStep: 18 Z difference 0.26610936291590326  Reward: -0.14583015709780156\n",
      "action [0.45614743 1.        ]\n",
      "Ep: 12  tStep: 19 Z difference 0.2715867159321901  Reward: -0.14035280408151474\n",
      "action [0.10943811 0.8528733 ]\n",
      "Ep: 12  tStep: 20 Z difference 0.2764411824744193  Reward: -0.13549833753928553\n",
      "closing SnS socket\n",
      "socket DC Closed\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-02e54201fcca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0;31m#model.learn(total_timesteps=timesteps,b_log_name=\"first_run\", reset_num_timesteps=False)#50000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimesteps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreset_num_timesteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#50000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"UR5-RL_savedpolicy-9-13-2021\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m# save trained model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mdel\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/thesis/lib/python3.6/site-packages/stable_baselines/ppo2/ppo2.py\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps)\u001b[0m\n\u001b[1;32m    334\u001b[0m                 \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_rollout_start\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m                 \u001b[0;31m# true_reward is the reward without discount\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 336\u001b[0;31m                 \u001b[0mrollout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    337\u001b[0m                 \u001b[0;31m# Unpack\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m                 \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneglogpacs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mep_infos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrue_reward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrollout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/thesis/lib/python3.6/site-packages/stable_baselines/common/runners.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, callback)\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontinue_training\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mabstractmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/thesis/lib/python3.6/site-packages/stable_baselines/ppo2/ppo2.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    480\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBox\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m                 \u001b[0mclipped_actions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhigh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdones\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclipped_actions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_timesteps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_envs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/thesis/lib/python3.6/site-packages/stable_baselines/common/vec_env/base_vec_env.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    148\u001b[0m         \"\"\"\n\u001b[1;32m    149\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/thesis/lib/python3.6/site-packages/stable_baselines/common/vec_env/dummy_vec_env.py\u001b[0m in \u001b[0;36mstep_wait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     46\u001b[0m                 \u001b[0;31m# save final observation where user can get it, then reset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuf_infos\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0menv_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'terminal_observation'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m                 \u001b[0mobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menvs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0menv_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save_obs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         return (self._obs_from_buf(), np.copy(self.buf_rews), np.copy(self.buf_dones),\n",
      "\u001b[0;32m~/ScheragaThesis/UR5-RL/UR5_RL/envs/ur5_env_0.py\u001b[0m in \u001b[0;36mreset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0;31m#if self.totalstepstaken>=410:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m         \u001b[0;31m#    print(\"reset\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 253\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresetEnvironment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    254\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_observation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_observation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#you *have* to compute and return the observation from reset()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepisodeinitialpose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrentpose\u001b[0m \u001b[0;31m#contains just initial xyz poses IN INCHES\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ScheragaThesis/UR5-RL/UR5_RL/envs/ur5_env_0.py\u001b[0m in \u001b[0;36mresetEnvironment\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 279\u001b[0;31m         \u001b[0mdata2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msock_DC\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#receive the \"done\" command\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    280\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mdata2\u001b[0m\u001b[0;34m==\u001b[0m \u001b[0;34mb''\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m             \u001b[0mdata2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msock_DC\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m#48 bytes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#https://github.com/ZhizhenQin/BalancingBot/blob/master/balance-bot/balance_bot/balancebot_task.py\n",
    "\n",
    "\n",
    "#%load_ext tensorboard\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore',category=FutureWarning)\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "\n",
    "import gym\n",
    "from stable_baselines import DQN, PPO2\n",
    "from stable_baselines import results_plotter\n",
    "from stable_baselines.bench import Monitor\n",
    "from stable_baselines.results_plotter import load_results, ts2xy\n",
    "from stable_baselines.common.noise import AdaptiveParamNoiseSpec\n",
    "from stable_baselines.common.callbacks import BaseCallback\n",
    "from stable_baselines.deepq.policies import MlpPolicy\n",
    "from stable_baselines.common.policies import FeedForwardPolicy, register_policy,LstmPolicy\n",
    "from stable_baselines.common.tf_layers import conv, linear, conv_to_fc, lstm\n",
    "from stable_baselines.common.vec_env import DummyVecEnv\n",
    "from stable_baselines.common.env_checker import check_env\n",
    "from datetime import date\n",
    "import csv\n",
    "#import balance_bot\n",
    "import socket\n",
    "import numpy as np\n",
    "from time import sleep\n",
    "import UR5_RL\n",
    "\n",
    "HOST_SnS = '192.168.0.103'\n",
    "PORT_SnS= 65495\n",
    "\n",
    "\n",
    "\n",
    "try:\n",
    "    # Create a TCP/IP socket\n",
    "    sock_SnS = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "    #sock_SnS.setblocking(False)\n",
    "    # Connect the socket to the port where the server is listening\n",
    "    server_address_SnS = (HOST_SnS, PORT_SnS)\n",
    "    print('connecting to {} port {}'.format(*server_address_SnS))\n",
    "    sock_SnS.connect(server_address_SnS)\n",
    "\n",
    "\n",
    "    #def callback(lcl, glb):\n",
    "         #stop training if reward exceeds 199\n",
    "    #    is_solved = lcl['t'] > 1000 and sum(lcl['episode_rewards'][-101:-1]) / 100 >= 1\n",
    "    #    return is_solved\n",
    "\n",
    "    #https://stable-baselines.readthedocs.io/en/master/guide/custom_policy.html\n",
    "    #Layers of 20 and 15\n",
    "\n",
    "    #Do this only after restarting the notebook!! \n",
    "    #register_policy('ScottLSTMPolicy', ScottLSTMPolicy)    \n",
    "    #print(\"lstm registered\")\n",
    "\n",
    "    #try:\n",
    "    \n",
    "    #code stopped at ep 36 at 10steps per ep, 80 ep\n",
    "    StepsPerEpisode=20 #was 10\n",
    "    TotalEpisodes=700  #was 80\n",
    "    env= gym.make(\"ur5-rl-v0\",StepsPerEpisode=StepsPerEpisode,TotalEpisodes=TotalEpisodes)\n",
    "    env = DummyVecEnv([lambda: env])\n",
    "\n",
    "    timesteps=(StepsPerEpisode*TotalEpisodes)\n",
    "    print(\"Total timesteps:\",timesteps)\n",
    "    \n",
    "    class ScottLSTMPolicy(LstmPolicy):\n",
    "        def __init__(self, sess, ob_space, ac_space, n_env=1, n_steps=StepsPerEpisode,\n",
    "                     n_batch=StepsPerEpisode, n_lstm=StepsPerEpisode, reuse=False,  **_kwargs):\n",
    "            super().__init__(sess, ob_space, ac_space, n_env, n_steps, n_batch, n_lstm, reuse,\n",
    "                             net_arch=[7,'lstm',dict(vf=[20, 15],pi=[20,15])],\n",
    "                             layer_norm=True, feature_extraction=\"mlp\", **_kwargs)\n",
    "            \n",
    "    #model = DQN(\"LnMlpPolicy\", env, learning_rate=1e-3, prioritized_replay=True,gamma=1 , buffer_size=50000,param_noise=False,\n",
    "    # exploration_initial_eps=0.1, exploration_final_eps=0.1,learning_starts=1, verbose=1)\n",
    "\n",
    "\n",
    "    #model = PPO2(\"MlpPolicy\", env,verboThe new research shows that \"we must expect extreme event records to be broken - not just by small marse=0)\n",
    "    #model = PPO2(\"MlpLstmPolicy\", env,nminibatches=1, n_steps=80, #exploration_initial_eps=1, exploration_final_eps=0.1,\n",
    "    #             verbose=0,tensorboard_log=\"./ScottPPOLstm/\") #n_lstm=2, n_batch=80, nminibatches=10,\n",
    "\n",
    "    model = PPO2(ScottLSTMPolicy, env,nminibatches=1, n_steps=StepsPerEpisode,learning_rate=0.001,\n",
    "                 verbose=2)# DEFAULT learning_rate=0.00025  #n_lstm=2, n_batch=80, nminibatches=10, #exploration_initial_eps=1, exploration_final_eps=0.1,\n",
    "\n",
    "    #model.learn(total_timesteps=timesteps,b_log_name=\"first_run\", reset_num_timesteps=False)#50000\n",
    "    model.learn(total_timesteps=timesteps,reset_num_timesteps=False)#50000\n",
    "    \n",
    "    today = date.today()\n",
    "    todaydate = today.strftime(\"%m_%d_%Y\")\n",
    "    RLmodelfilename=\"UR5-RL_savedpolicy_\"+todaydate\n",
    "    model.save(RLmodelfilename)# save trained model\n",
    "    \n",
    "    del model\n",
    "    print(\"training complete\")\n",
    "    \n",
    "\n",
    "finally:\n",
    "    endmsg='end'\n",
    "    data1=endmsg.encode('ascii')    \n",
    "    sock_SnS.sendall(data1)\n",
    "    sock_SnS.sendall(data1)\n",
    "    print('closing SnS socket')\n",
    "    sock_SnS.close()\n",
    "    \n",
    "    HOST2 = '192.168.0.103'\n",
    "    PORT2= PORT_SnS-10 #65481\n",
    "    sock_DC = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "    server_address_DC = (HOST2, PORT2)\n",
    "    sock_DC.close()\n",
    "    print(\"socket DC Closed\")\n",
    "      \n",
    "    #gitkraken\n",
    "    \n",
    "    #check_env(env)\n",
    "    #https://stable-baselines.readthedocs.io/en/master/modules/dqn.html\n",
    "    #model.learn(total_timesteps=25000)\n",
    "    #del model # remove to demonstrate saving and loading\n",
    "    #model = DQN.load(\"deepq_cartpole\")\n",
    "\n",
    "    \n",
    "    #Starting at 11:02AM\n",
    "    #80 episodes in 45 min\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved!\n"
     ]
    }
   ],
   "source": [
    "model.save(\"UR5-RL_savedpolicy-8-5to6-2021\")\n",
    "print(\"saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HOST2 = '192.168.0.103'learning_rate=0.00025\n",
    "PORT2= 65485\n",
    "sock_DC = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "server_address_DC = (HOST2, PORT2)\n",
    "sock_DC.close()\n",
    "print(\"socket DC Closed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[Errno 9] Bad file descriptor",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-5c92c6e2c5c7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mendmsg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'end'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdata1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mendmsg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ascii'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0msock_SnS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msendall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'closing SnS socket'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0msock_SnS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 9] Bad file descriptor"
     ]
    }
   ],
   "source": [
    "endmsg='end'\n",
    "data1=endmsg.encode('ascii')    \n",
    "sock_SnS.sendall(data1)\n",
    "\n",
    "\n",
    "#print('closing SnS socket')\n",
    "#sock_SnS.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "In /home/scott/.local/lib/python3.6/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The text.latex.preview rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /home/scott/.local/lib/python3.6/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The mathtext.fallback_to_cm rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /home/scott/.local/lib/python3.6/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: Support for setting the 'mathtext.fallback_to_cm' rcParam is deprecated since 3.3 and will be removed two minor releases later; use 'mathtext.fallback : 'cm' instead.\n",
      "In /home/scott/.local/lib/python3.6/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The validate_bool_maybe_none function was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /home/scott/.local/lib/python3.6/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The savefig.jpeg_quality rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /home/scott/.local/lib/python3.6/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The keymap.all_axes rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /home/scott/.local/lib/python3.6/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The animation.avconv_path rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /home/scott/.local/lib/python3.6/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The animation.avconv_args rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "/home/scott/.local/lib/python3.6/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/scott/.local/lib/python3.6/site-packages/stable_baselines/common/policies.py:346: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/scott/.local/lib/python3.6/site-packages/stable_baselines/common/policies.py:442: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.flatten instead.\n",
      "WARNING:tensorflow:From /home/scott/.local/lib/python3.6/site-packages/stable_baselines/common/tf_layers.py:121: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/scott/.local/lib/python3.6/site-packages/stable_baselines/common/tf_layers.py:123: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/scott/.local/lib/python3.6/site-packages/stable_baselines/common/distributions.py:326: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/scott/.local/lib/python3.6/site-packages/stable_baselines/common/distributions.py:327: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/scott/.local/lib/python3.6/site-packages/stable_baselines/ppo2/ppo2.py:190: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/scott/.local/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /home/scott/.local/lib/python3.6/site-packages/stable_baselines/ppo2/ppo2.py:206: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/scott/.local/lib/python3.6/site-packages/stable_baselines/ppo2/ppo2.py:242: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore',category=FutureWarning)\n",
    "import tensorflow as tf\n",
    "#import tensorflow.contrib.layers as layers\n",
    "import tensorflow as tf\n",
    "#from tensorflow import keras\n",
    "#from tensorflow.keras import layers\n",
    "#from keras.utils import to_categorical\n",
    "#from keras.models import Sequential\n",
    "#from keras.layers import Dense, Dropout, Embedding, LSTM, GlobalMaxPooling1D, SpatialDropout1D\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "\n",
    "import gym\n",
    "from stable_baselines import DQN,PPO2\n",
    "from stable_baselines import results_plotter\n",
    "from stable_baselines.bench import Monitor\n",
    "from stable_baselines.results_plotter import load_results, ts2xy\n",
    "from stable_baselines.common.noise import AdaptiveParamNoiseSpec\n",
    "from stable_baselines.common.callbacks import BaseCallback\n",
    "from stable_baselines.deepq.policies import MlpPolicy\n",
    "from stable_baselines.common.policies import FeedForwardPolicy, register_policy,LstmPolicy\n",
    "from stable_baselines.common.tf_layers import conv, linear, conv_to_fc, lstm\n",
    "#from stable_baselines.common import get_vec_normalize_env\n",
    "from stable_baselines.common.env_checker import check_env\n",
    "from stable_baselines.common.vec_env import DummyVecEnv\n",
    "#import balance_bot\n",
    "import MainEnv_RL\n",
    "\n",
    "env= gym.make(\"UR5-RL-env\", render=True)\n",
    "env = DummyVecEnv([lambda: env])\n",
    "\n",
    "#env=get_vec_normalize_env(env) \n",
    "#model = DQN.load(\"MainScott_RL\")\n",
    "model = PPO2.load(\"UR5-RL_savedpolicy\")\n",
    "#env=model.get_env()\n",
    "#obs = env.reset()\n",
    "done = [False for _ in range(1)] #env.num_envs\n",
    "state=None\n",
    "\n",
    "while True:\n",
    "    obs = env.reset()\n",
    "    #env._seed()\n",
    "    for i in range(80):\n",
    "        action, _states = model.predict(obs,state=state,mask=done)\n",
    "        #actionlist.append(action)\n",
    "        obs, rewards, dones, info = env.step(action)\n",
    "    print(rewards)\n",
    "        \n",
    "\"\"\"\n",
    "\n",
    "while True:\n",
    "    obs = env.reset()\n",
    "    env._seed()\n",
    "    actionlist=[]\n",
    "    #print(\"reset\")\n",
    "    for i in range(80):\n",
    "        action, _states = model.predict(obs)\n",
    "        actionlist.append(action)\n",
    "        obs, rewards, dones, info = env.step(action)\n",
    "    print(rewards)       \n",
    "\"\"\"       \n",
    "        \n",
    "        \n",
    "    #print(\"actionlist\",actionlist)\n",
    "    #env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"UR5-RL_savedpolicy-9-13-2021\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
