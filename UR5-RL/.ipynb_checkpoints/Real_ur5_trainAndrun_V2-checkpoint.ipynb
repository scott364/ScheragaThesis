{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfor env in gym.envs.registry.env_specs:\\n     if 'MainEnvRL-v0' in env:\\n        print('Remove {} from registry'.format(env))\\n        del gym.registry.env_specs[env]\\n\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#To register MainEnvRL with local copy of Gym\n",
    "\"\"\"\n",
    "from gym.envs.registration import register\n",
    " \n",
    "register(\n",
    "    id='MainEnvRL-v2',\n",
    "    #entry_point='balance_bot.envs:BalancebotEnv',\n",
    "    entry_point='MainEnv_RL.envs:MainEnvRL',\n",
    ")\n",
    "\n",
    "\"\"\"\n",
    "#import gym\n",
    "#for env in gym.envs.registry.env_specs:\n",
    "#    print(env)\n",
    "    \n",
    "    #if 'MainEnvRL-v1' == env:\n",
    "    #     print('Remove {} from registry'.format(env))\n",
    "         #del gym.envs.registry.env_specs[env]\n",
    "            \n",
    "            \n",
    "#import MainEnv_RL\n",
    "\n",
    "#env = gym.make('MainEnvRL-v2')\n",
    "#env.reset()\n",
    "\"\"\"\n",
    "for env in gym.envs.registry.env_specs:\n",
    "     if 'MainEnvRL-v0' in env:\n",
    "        print('Remove {} from registry'.format(env))\n",
    "        del gym.registry.env_specs[env]\n",
    "\"\"\"                \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "connecting to 128.138.224.89 port 65490\n",
      "port= /dev/ttyACM0\n",
      "the blue  robot is being used. Please change the bot identity variable if this is incorrect\n",
      "DC socket connecting to 128.138.224.89 port 65480\n",
      "GRU model loaded\n",
      "TotalEpisodes: 380    StepsPerEpisode: 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/scott/anaconda3/envs/thesis/lib/python3.7/site-packages/stable_baselines/__init__.py:33: UserWarning: stable-baselines is in maintenance mode, please use [Stable-Baselines3 (SB3)](https://github.com/DLR-RM/stable-baselines3) for an up-to-date version. You can find a [migration guide](https://stable-baselines3.readthedocs.io/en/master/guide/migration.html) in SB3 documentation.\n",
      "  \"stable-baselines is in maintenance mode, please use [Stable-Baselines3 (SB3)](https://github.com/DLR-RM/stable-baselines3) for an up-to-date version. You can find a [migration guide](https://stable-baselines3.readthedocs.io/en/master/guide/migration.html) in SB3 documentation.\"\n",
      "/home/scott/anaconda3/envs/thesis/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total timesteps: 11400\n",
      "WARNING:tensorflow:From /home/scott/anaconda3/envs/thesis/lib/python3.7/site-packages/stable_baselines/common/tf_util.py:191: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/scott/anaconda3/envs/thesis/lib/python3.7/site-packages/stable_baselines/common/tf_util.py:200: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/scott/anaconda3/envs/thesis/lib/python3.7/site-packages/stable_baselines/deepq/dqn.py:129: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/scott/anaconda3/envs/thesis/lib/python3.7/site-packages/stable_baselines/deepq/build_graph.py:358: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/scott/anaconda3/envs/thesis/lib/python3.7/site-packages/stable_baselines/deepq/build_graph.py:359: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/scott/anaconda3/envs/thesis/lib/python3.7/site-packages/stable_baselines/deepq/build_graph.py:139: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/scott/anaconda3/envs/thesis/lib/python3.7/site-packages/stable_baselines/deepq/policies.py:109: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.flatten instead.\n",
      "WARNING:tensorflow:From /home/scott/anaconda3/envs/thesis/lib/python3.7/site-packages/tensorflow_core/python/layers/core.py:332: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n",
      "WARNING:tensorflow:From /home/scott/anaconda3/envs/thesis/lib/python3.7/site-packages/stable_baselines/deepq/build_graph.py:147: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/scott/anaconda3/envs/thesis/lib/python3.7/site-packages/stable_baselines/deepq/build_graph.py:149: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /home/scott/anaconda3/envs/thesis/lib/python3.7/site-packages/stable_baselines/deepq/build_graph.py:372: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/scott/anaconda3/envs/thesis/lib/python3.7/site-packages/stable_baselines/deepq/build_graph.py:372: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/scott/anaconda3/envs/thesis/lib/python3.7/site-packages/stable_baselines/deepq/build_graph.py:372: The name tf.get_variable_scope is deprecated. Please use tf.compat.v1.get_variable_scope instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/scott/anaconda3/envs/thesis/lib/python3.7/site-packages/stable_baselines/deepq/build_graph.py:415: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/scott/anaconda3/envs/thesis/lib/python3.7/site-packages/stable_baselines/deepq/build_graph.py:449: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/scott/anaconda3/envs/thesis/lib/python3.7/site-packages/stable_baselines/common/tf_util.py:241: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/scott/anaconda3/envs/thesis/lib/python3.7/site-packages/stable_baselines/common/tf_util.py:242: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/scott/anaconda3/envs/thesis/lib/python3.7/site-packages/stable_baselines/common/tf_util.py:322: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "Ep: 1  tStep: 1  Reward: -2.0  Button Pressed? 0  GRU output: 0 GRU_TruePosQty 0 GRU_TrueNegQty 0 GRU_FalsePosQty 0 GRU_FalseNegQty 0 TotalQty 0 unfilled GRU buffer at size 1\n",
      "Ep: 1  tStep: 2  Reward: -2.0  Button Pressed? 0  GRU output: 0 GRU_TruePosQty 0 GRU_TrueNegQty 0 GRU_FalsePosQty 0 GRU_FalseNegQty 0 TotalQty 0 unfilled GRU buffer at size 2\n",
      "Ep: 1  tStep: 3  Reward: -2.0  Button Pressed? 0  GRU output: 0 GRU_TruePosQty 0 GRU_TrueNegQty 0 GRU_FalsePosQty 0 GRU_FalseNegQty 0 TotalQty 0 unfilled GRU buffer at size 3\n",
      "Ep: 1  tStep: 4  Reward: -1.95  Button Pressed? 0  GRU output: 0 GRU_TruePosQty 0 GRU_TrueNegQty 0 GRU_FalsePosQty 0 GRU_FalseNegQty 0 TotalQty 0 unfilled GRU buffer at size 4\n",
      "Ep: 1  tStep: 5  Reward: -1.95  Button Pressed? 0  GRU output: 0 GRU_TruePosQty 0 GRU_TrueNegQty 0 GRU_FalsePosQty 0 GRU_FalseNegQty 0 TotalQty 0 unfilled GRU buffer at size 5\n",
      "Ep: 1  tStep: 6  Reward: -1.95  Button Pressed? 0  GRU output: 0 GRU_TruePosQty 0 GRU_TrueNegQty 0 GRU_FalsePosQty 0 GRU_FalseNegQty 0 TotalQty 0 unfilled GRU buffer at size 6\n",
      "Ep: 1  tStep: 7  Reward: -1.95  Button Pressed? 0  GRU output: 0 GRU_TruePosQty 0 GRU_TrueNegQty 0 GRU_FalsePosQty 0 GRU_FalseNegQty 0 TotalQty 0 unfilled GRU buffer at size 7\n",
      "Ep: 1  tStep: 8  Reward: -1.95  Button Pressed? 0  GRU output: 0 GRU_TruePosQty 0 GRU_TrueNegQty 0 GRU_FalsePosQty 0 GRU_FalseNegQty 0 TotalQty 0 unfilled GRU buffer at size 8\n",
      "Ep: 1  tStep: 9  Reward: -0.46  Button Pressed? 0  GRU output: -0.23 GRU_TruePosQty 0 GRU_TrueNegQty 1 GRU_FalsePosQty 0 GRU_FalseNegQty 0 TotalQty 1 00000 GRU Output Correct! 00000\n",
      "Ep: 1  tStep: 10  Reward: -0.48  Button Pressed? 0  GRU output: -0.24 GRU_TruePosQty 0 GRU_TrueNegQty 2 GRU_FalsePosQty 0 GRU_FalseNegQty 0 TotalQty 2 00000 GRU Output Correct! 00000\n",
      "Ep: 1  tStep: 11  Reward: -0.57  Button Pressed? 0  GRU output: -0.28 GRU_TruePosQty 0 GRU_TrueNegQty 3 GRU_FalsePosQty 0 GRU_FalseNegQty 0 TotalQty 3 00000 GRU Output Correct! 00000\n",
      "Ep: 1  tStep: 12  Reward: -0.54  Button Pressed? 0  GRU output: -0.27 GRU_TruePosQty 0 GRU_TrueNegQty 4 GRU_FalsePosQty 0 GRU_FalseNegQty 0 TotalQty 4 00000 GRU Output Correct! 00000\n",
      "Ep: 1  tStep: 13  Reward: -0.53  Button Pressed? 0  GRU output: -0.26 GRU_TruePosQty 0 GRU_TrueNegQty 5 GRU_FalsePosQty 0 GRU_FalseNegQty 0 TotalQty 5 00000 GRU Output Correct! 00000\n",
      "Ep: 1  tStep: 14  Reward: -0.63  Button Pressed? 0  GRU output: -0.31 GRU_TruePosQty 0 GRU_TrueNegQty 6 GRU_FalsePosQty 0 GRU_FalseNegQty 0 TotalQty 6 00000 GRU Output Correct! 00000\n",
      "Ep: 1  tStep: 15  Reward: -0.56  Button Pressed? 0  GRU output: -0.28 GRU_TruePosQty 0 GRU_TrueNegQty 7 GRU_FalsePosQty 0 GRU_FalseNegQty 0 TotalQty 7 00000 GRU Output Correct! 00000\n",
      "Ep: 1  tStep: 16  Reward: -1.07  Button Pressed? 0  GRU output: -0.54 GRU_TruePosQty 0 GRU_TrueNegQty 8 GRU_FalsePosQty 0 GRU_FalseNegQty 0 TotalQty 8 00000 GRU Output Correct! 00000\n",
      "Ep: 1  tStep: 17  Reward: -0.71  Button Pressed? 0  GRU output: -0.35 GRU_TruePosQty 0 GRU_TrueNegQty 9 GRU_FalsePosQty 0 GRU_FalseNegQty 0 TotalQty 9 00000 GRU Output Correct! 00000\n",
      "Ep: 1  tStep: 18  Reward: -1.06  Button Pressed? 0  GRU output: -0.53 GRU_TruePosQty 0 GRU_TrueNegQty 10 GRU_FalsePosQty 0 GRU_FalseNegQty 0 TotalQty 10 00000 GRU Output Correct! 00000\n",
      "Ep: 1  tStep: 19  Reward: -1.23  Button Pressed? 0  GRU output: -0.61 GRU_TruePosQty 0 GRU_TrueNegQty 11 GRU_FalsePosQty 0 GRU_FalseNegQty 0 TotalQty 11 00000 GRU Output Correct! 00000\n",
      "Ep: 1  tStep: 20  Reward: -1.16  Button Pressed? 0  GRU output: -0.58 GRU_TruePosQty 0 GRU_TrueNegQty 12 GRU_FalsePosQty 0 GRU_FalseNegQty 0 TotalQty 12 00000 GRU Output Correct! 00000\n",
      "Ep: 1  tStep: 21  Reward: -0.59  Button Pressed? 0  GRU output: -0.29 GRU_TruePosQty 0 GRU_TrueNegQty 13 GRU_FalsePosQty 0 GRU_FalseNegQty 0 TotalQty 13 00000 GRU Output Correct! 00000\n",
      "Ep: 1  tStep: 22  Reward: -1.27  Button Pressed? 0  GRU output: -0.63 GRU_TruePosQty 0 GRU_TrueNegQty 14 GRU_FalsePosQty 0 GRU_FalseNegQty 0 TotalQty 14 00000 GRU Output Correct! 00000\n",
      "Ep: 1  tStep: 23  Reward: -0.84  Button Pressed? 0  GRU output: -0.42 GRU_TruePosQty 0 GRU_TrueNegQty 15 GRU_FalsePosQty 0 GRU_FalseNegQty 0 TotalQty 15 00000 GRU Output Correct! 00000\n",
      "Ep: 1  tStep: 24  Reward: -0.6  Button Pressed? 0  GRU output: -0.3 GRU_TruePosQty 0 GRU_TrueNegQty 16 GRU_FalsePosQty 0 GRU_FalseNegQty 0 TotalQty 16 00000 GRU Output Correct! 00000\n",
      "Ep: 1  tStep: 25  Reward: -1.17  Button Pressed? 0  GRU output: -0.58 GRU_TruePosQty 0 GRU_TrueNegQty 17 GRU_FalsePosQty 0 GRU_FalseNegQty 0 TotalQty 17 00000 GRU Output Correct! 00000\n",
      "Ep: 1  tStep: 26  Reward: -1.1  Button Pressed? 0  GRU output: -0.55 GRU_TruePosQty 0 GRU_TrueNegQty 18 GRU_FalsePosQty 0 GRU_FalseNegQty 0 TotalQty 18 00000 GRU Output Correct! 00000\n",
      "Ep: 1  tStep: 27  Reward: -1.02  Button Pressed? 0  GRU output: -0.51 GRU_TruePosQty 0 GRU_TrueNegQty 19 GRU_FalsePosQty 0 GRU_FalseNegQty 0 TotalQty 19 00000 GRU Output Correct! 00000\n",
      "Ep: 1  tStep: 28  Reward: -1.26  Button Pressed? 0  GRU output: -0.63 GRU_TruePosQty 0 GRU_TrueNegQty 20 GRU_FalsePosQty 0 GRU_FalseNegQty 0 TotalQty 20 00000 GRU Output Correct! 00000\n",
      "Ep: 1  tStep: 29  Reward: -1.11  Button Pressed? 0  GRU output: -0.55 GRU_TruePosQty 0 GRU_TrueNegQty 21 GRU_FalsePosQty 0 GRU_FalseNegQty 0 TotalQty 21 00000 GRU Output Correct! 00000\n",
      "Ep: 1  tStep: 30  Reward: -1.34  Button Pressed? 0  GRU output: -0.67 GRU_TruePosQty 0 GRU_TrueNegQty 22 GRU_FalsePosQty 0 GRU_FalseNegQty 0 TotalQty 22 00000 GRU Output Correct! 00000\n",
      "   Actionlist: [0, 3, 1, 4, 2, 1, 0, 1, 2, 0, 0, 0, 3, 0, 3, 4, 2, 0, 1, 0, 1, 4, 4, 0, 3, 0, 0, 2, 3, 2]\n",
      "Ep: 2  tStep: 1  Reward: -2.0  Button Pressed? 0  GRU output: 0 GRU_TruePosQty 0 GRU_TrueNegQty 22 GRU_FalsePosQty 0 GRU_FalseNegQty 0 TotalQty 22 unfilled GRU buffer at size 1\n",
      "Ep: 2  tStep: 2  Reward: -2.0  Button Pressed? 0  GRU output: 0 GRU_TruePosQty 0 GRU_TrueNegQty 22 GRU_FalsePosQty 0 GRU_FalseNegQty 0 TotalQty 22 unfilled GRU buffer at size 2\n",
      "Ep: 2  tStep: 3  Reward: -2.0  Button Pressed? 0  GRU output: 0 GRU_TruePosQty 0 GRU_TrueNegQty 22 GRU_FalsePosQty 0 GRU_FalseNegQty 0 TotalQty 22 unfilled GRU buffer at size 3\n",
      "Ep: 2  tStep: 4  Reward: -2.0  Button Pressed? 0  GRU output: 0 GRU_TruePosQty 0 GRU_TrueNegQty 22 GRU_FalsePosQty 0 GRU_FalseNegQty 0 TotalQty 22 unfilled GRU buffer at size 4\n",
      "Ep: 2  tStep: 5  Reward: -2.0  Button Pressed? 0  GRU output: 0 GRU_TruePosQty 0 GRU_TrueNegQty 22 GRU_FalsePosQty 0 GRU_FalseNegQty 0 TotalQty 22 unfilled GRU buffer at size 5\n",
      "Ep: 2  tStep: 6  Reward: -2.0  Button Pressed? 0  GRU output: 0 GRU_TruePosQty 0 GRU_TrueNegQty 22 GRU_FalsePosQty 0 GRU_FalseNegQty 0 TotalQty 22 unfilled GRU buffer at size 6\n",
      "Ep: 2  tStep: 7  Reward: -2.0  Button Pressed? 0  GRU output: 0 GRU_TruePosQty 0 GRU_TrueNegQty 22 GRU_FalsePosQty 0 GRU_FalseNegQty 0 TotalQty 22 unfilled GRU buffer at size 7\n",
      "Ep: 2  tStep: 8  Reward: -2.0  Button Pressed? 0  GRU output: 0 GRU_TruePosQty 0 GRU_TrueNegQty 22 GRU_FalsePosQty 0 GRU_FalseNegQty 0 TotalQty 22 unfilled GRU buffer at size 8\n",
      "Ep: 2  tStep: 9  Reward: -0.42  Button Pressed? 0  GRU output: -0.21 GRU_TruePosQty 0 GRU_TrueNegQty 23 GRU_FalsePosQty 0 GRU_FalseNegQty 0 TotalQty 23 00000 GRU Output Correct! 00000\n",
      "Ep: 2  tStep: 10  Reward: -0.34  Button Pressed? 0  GRU output: -0.17 GRU_TruePosQty 0 GRU_TrueNegQty 24 GRU_FalsePosQty 0 GRU_FalseNegQty 0 TotalQty 24 00000 GRU Output Correct! 00000\n",
      "Ep: 2  tStep: 11  Reward: -0.34  Button Pressed? 0  GRU output: -0.17 GRU_TruePosQty 0 GRU_TrueNegQty 25 GRU_FalsePosQty 0 GRU_FalseNegQty 0 TotalQty 25 00000 GRU Output Correct! 00000\n",
      "Ep: 2  tStep: 12  Reward: -0.5  Button Pressed? 0  GRU output: -0.25 GRU_TruePosQty 0 GRU_TrueNegQty 26 GRU_FalsePosQty 0 GRU_FalseNegQty 0 TotalQty 26 00000 GRU Output Correct! 00000\n",
      "Ep: 2  tStep: 13  Reward: -0.56  Button Pressed? 0  GRU output: -0.28 GRU_TruePosQty 0 GRU_TrueNegQty 27 GRU_FalsePosQty 0 GRU_FalseNegQty 0 TotalQty 27 00000 GRU Output Correct! 00000\n",
      "Ep: 2  tStep: 14  Reward: -0.49  Button Pressed? 0  GRU output: -0.24 GRU_TruePosQty 0 GRU_TrueNegQty 28 GRU_FalsePosQty 0 GRU_FalseNegQty 0 TotalQty 28 00000 GRU Output Correct! 00000\n",
      "Ep: 2  tStep: 15  Reward: 0.31  Button Pressed? 0  GRU output: 0.16 GRU_TruePosQty 0 GRU_TrueNegQty 29 GRU_FalsePosQty 0 GRU_FalseNegQty 0 TotalQty 29 00000 GRU Output Correct! 00000\n",
      "Ep: 2  tStep: 16  Reward: 0.3  Button Pressed? 0  GRU output: 0.15 GRU_TruePosQty 0 GRU_TrueNegQty 30 GRU_FalsePosQty 0 GRU_FalseNegQty 0 TotalQty 30 00000 GRU Output Correct! 00000\n",
      "Ep: 2  tStep: 17  Reward: -0.27  Button Pressed? 0  GRU output: -0.14 GRU_TruePosQty 0 GRU_TrueNegQty 31 GRU_FalsePosQty 0 GRU_FalseNegQty 0 TotalQty 31 00000 GRU Output Correct! 00000\n",
      "Ep: 2  tStep: 18  Reward: -0.05  Button Pressed? 0  GRU output: -0.02 GRU_TruePosQty 0 GRU_TrueNegQty 32 GRU_FalsePosQty 0 GRU_FalseNegQty 0 TotalQty 32 00000 GRU Output Correct! 00000\n",
      "Ep: 2  tStep: 19  Reward: -0.5  Button Pressed? 0  GRU output: -0.25 GRU_TruePosQty 0 GRU_TrueNegQty 33 GRU_FalsePosQty 0 GRU_FalseNegQty 0 TotalQty 33 00000 GRU Output Correct! 00000\n",
      "Ep: 2  tStep: 20  Reward: -0.38  Button Pressed? 0  GRU output: -0.19 GRU_TruePosQty 0 GRU_TrueNegQty 34 GRU_FalsePosQty 0 GRU_FalseNegQty 0 TotalQty 34 00000 GRU Output Correct! 00000\n",
      "Ep: 2  tStep: 21  Reward: -0.15  Button Pressed? 0  GRU output: -0.07 GRU_TruePosQty 0 GRU_TrueNegQty 35 GRU_FalsePosQty 0 GRU_FalseNegQty 0 TotalQty 35 00000 GRU Output Correct! 00000\n",
      "Ep: 2  tStep: 22  Reward: -0.54  Button Pressed? 0  GRU output: -0.27 GRU_TruePosQty 0 GRU_TrueNegQty 36 GRU_FalsePosQty 0 GRU_FalseNegQty 0 TotalQty 36 00000 GRU Output Correct! 00000\n",
      "Ep: 2  tStep: 23  Reward: -0.45  Button Pressed? 0  GRU output: -0.23 GRU_TruePosQty 0 GRU_TrueNegQty 37 GRU_FalsePosQty 0 GRU_FalseNegQty 0 TotalQty 37 00000 GRU Output Correct! 00000\n",
      "Ep: 2  tStep: 24  Reward: -0.6  Button Pressed? 0  GRU output: -0.3 GRU_TruePosQty 0 GRU_TrueNegQty 38 GRU_FalsePosQty 0 GRU_FalseNegQty 0 TotalQty 38 00000 GRU Output Correct! 00000\n",
      "Ep: 2  tStep: 25  Reward: -0.44  Button Pressed? 0  GRU output: -0.22 GRU_TruePosQty 0 GRU_TrueNegQty 39 GRU_FalsePosQty 0 GRU_FalseNegQty 0 TotalQty 39 00000 GRU Output Correct! 00000\n",
      "Ep: 2  tStep: 26  Reward: -0.35  Button Pressed? 0  GRU output: -0.18 GRU_TruePosQty 0 GRU_TrueNegQty 40 GRU_FalsePosQty 0 GRU_FalseNegQty 0 TotalQty 40 00000 GRU Output Correct! 00000\n",
      "Ep: 2  tStep: 27  Reward: -0.52  Button Pressed? 0  GRU output: -0.26 GRU_TruePosQty 0 GRU_TrueNegQty 41 GRU_FalsePosQty 0 GRU_FalseNegQty 0 TotalQty 41 00000 GRU Output Correct! 00000\n",
      "Ep: 2  tStep: 28  Reward: -0.58  Button Pressed? 0  GRU output: -0.29 GRU_TruePosQty 0 GRU_TrueNegQty 42 GRU_FalsePosQty 0 GRU_FalseNegQty 0 TotalQty 42 00000 GRU Output Correct! 00000\n",
      "Ep: 2  tStep: 29  Reward: 0.31  Button Pressed? 0  GRU output: 0.16 GRU_TruePosQty 0 GRU_TrueNegQty 43 GRU_FalsePosQty 0 GRU_FalseNegQty 0 TotalQty 43 00000 GRU Output Correct! 00000\n",
      "Ep: 2  tStep: 30  Reward: -0.01  Button Pressed? 0  GRU output: -0.01 GRU_TruePosQty 0 GRU_TrueNegQty 44 GRU_FalsePosQty 0 GRU_FalseNegQty 0 TotalQty 44 00000 GRU Output Correct! 00000\n",
      "   Actionlist: [1, 1, 2, 1, 3, 1, 3, 2, 4, 1, 1, 3, 0, 3, 4, 0, 2, 4, 1, 2, 3, 4, 3, 3, 4, 4, 1, 2, 2, 2]\n",
      "Ep: 3  tStep: 1  Reward: -2.0  Button Pressed? 0  GRU output: 0 GRU_TruePosQty 0 GRU_TrueNegQty 44 GRU_FalsePosQty 0 GRU_FalseNegQty 0 TotalQty 44 unfilled GRU buffer at size 1\n",
      "Ep: 3  tStep: 2  Reward: -2.0  Button Pressed? 0  GRU output: 0 GRU_TruePosQty 0 GRU_TrueNegQty 44 GRU_FalsePosQty 0 GRU_FalseNegQty 0 TotalQty 44 unfilled GRU buffer at size 2\n",
      "Ep: 3  tStep: 3  Reward: -2.0  Button Pressed? 0  GRU output: 0 GRU_TruePosQty 0 GRU_TrueNegQty 44 GRU_FalsePosQty 0 GRU_FalseNegQty 0 TotalQty 44 unfilled GRU buffer at size 3\n",
      "Ep: 3  tStep: 4  Reward: -2.0  Button Pressed? 0  GRU output: 0 GRU_TruePosQty 0 GRU_TrueNegQty 44 GRU_FalsePosQty 0 GRU_FalseNegQty 0 TotalQty 44 unfilled GRU buffer at size 4\n",
      "Ep: 3  tStep: 5  Reward: -2.0  Button Pressed? 0  GRU output: 0 GRU_TruePosQty 0 GRU_TrueNegQty 44 GRU_FalsePosQty 0 GRU_FalseNegQty 0 TotalQty 44 unfilled GRU buffer at size 5\n",
      "Ep: 3  tStep: 6  Reward: -1.96  Button Pressed? 0  GRU output: 0 GRU_TruePosQty 0 GRU_TrueNegQty 44 GRU_FalsePosQty 0 GRU_FalseNegQty 0 TotalQty 44 unfilled GRU buffer at size 6\n",
      "Ep: 3  tStep: 7  Reward: -1.95  Button Pressed? 0  GRU output: 0 GRU_TruePosQty 0 GRU_TrueNegQty 44 GRU_FalsePosQty 0 GRU_FalseNegQty 0 TotalQty 44 unfilled GRU buffer at size 7\n",
      "Ep: 3  tStep: 8  Reward: -1.95  Button Pressed? 0  GRU output: 0 GRU_TruePosQty 0 GRU_TrueNegQty 44 GRU_FalsePosQty 0 GRU_FalseNegQty 0 TotalQty 44 unfilled GRU buffer at size 8\n",
      "Ep: 3  tStep: 9  Reward: -0.33  Button Pressed? 0  GRU output: -0.16 GRU_TruePosQty 0 GRU_TrueNegQty 45 GRU_FalsePosQty 0 GRU_FalseNegQty 0 TotalQty 45 00000 GRU Output Correct! 00000\n",
      "Ep: 3  tStep: 10  Reward: -0.59  Button Pressed? 0  GRU output: -0.29 GRU_TruePosQty 0 GRU_TrueNegQty 46 GRU_FalsePosQty 0 GRU_FalseNegQty 0 TotalQty 46 00000 GRU Output Correct! 00000\n",
      "Ep: 3  tStep: 11  Reward: -0.53  Button Pressed? 0  GRU output: -0.27 GRU_TruePosQty 0 GRU_TrueNegQty 47 GRU_FalsePosQty 0 GRU_FalseNegQty 0 TotalQty 47 00000 GRU Output Correct! 00000\n",
      "Ep: 3  tStep: 12  Reward: -0.5  Button Pressed? 0  GRU output: -0.25 GRU_TruePosQty 0 GRU_TrueNegQty 48 GRU_FalsePosQty 0 GRU_FalseNegQty 0 TotalQty 48 00000 GRU Output Correct! 00000\n",
      "Ep: 3  tStep: 13  Reward: -0.6  Button Pressed? 0  GRU output: -0.3 GRU_TruePosQty 0 GRU_TrueNegQty 49 GRU_FalsePosQty 0 GRU_FalseNegQty 0 TotalQty 49 00000 GRU Output Correct! 00000\n",
      "Ep: 3  tStep: 14  Reward: -0.51  Button Pressed? 0  GRU output: -0.25 GRU_TruePosQty 0 GRU_TrueNegQty 50 GRU_FalsePosQty 0 GRU_FalseNegQty 0 TotalQty 50 00000 GRU Output Correct! 00000\n",
      "Ep: 3  tStep: 15  Reward: -0.17  Button Pressed? 0  GRU output: -0.09 GRU_TruePosQty 0 GRU_TrueNegQty 51 GRU_FalsePosQty 0 GRU_FalseNegQty 0 TotalQty 51 00000 GRU Output Correct! 00000\n",
      "Ep: 3  tStep: 16  Reward: 0.19  Button Pressed? 0  GRU output: 0.09 GRU_TruePosQty 0 GRU_TrueNegQty 52 GRU_FalsePosQty 0 GRU_FalseNegQty 0 TotalQty 52 00000 GRU Output Correct! 00000\n",
      "Ep: 3  tStep: 17  Reward: -0.34  Button Pressed? 0  GRU output: -0.17 GRU_TruePosQty 0 GRU_TrueNegQty 53 GRU_FalsePosQty 0 GRU_FalseNegQty 0 TotalQty 53 00000 GRU Output Correct! 00000\n",
      "Ep: 3  tStep: 18  Reward: -0.63  Button Pressed? 0  GRU output: -0.31 GRU_TruePosQty 0 GRU_TrueNegQty 54 GRU_FalsePosQty 0 GRU_FalseNegQty 0 TotalQty 54 00000 GRU Output Correct! 00000\n",
      "Ep: 3  tStep: 19  Reward: -0.22  Button Pressed? 0  GRU output: -0.11 GRU_TruePosQty 0 GRU_TrueNegQty 55 GRU_FalsePosQty 0 GRU_FalseNegQty 0 TotalQty 55 00000 GRU Output Correct! 00000\n",
      "Ep: 3  tStep: 20  Reward: 0.11  Button Pressed? 0  GRU output: 0.06 GRU_TruePosQty 0 GRU_TrueNegQty 56 GRU_FalsePosQty 0 GRU_FalseNegQty 0 TotalQty 56 00000 GRU Output Correct! 00000\n",
      "Ep: 3  tStep: 21  Reward: 0.03  Button Pressed? 0  GRU output: 0.02 GRU_TruePosQty 0 GRU_TrueNegQty 57 GRU_FalsePosQty 0 GRU_FalseNegQty 0 TotalQty 57 00000 GRU Output Correct! 00000\n",
      "Ep: 3  tStep: 22  Reward: -0.4  Button Pressed? 0  GRU output: -0.2 GRU_TruePosQty 0 GRU_TrueNegQty 58 GRU_FalsePosQty 0 GRU_FalseNegQty 0 TotalQty 58 00000 GRU Output Correct! 00000\n",
      "Ep: 3  tStep: 23  Reward: -0.05  Button Pressed? 0  GRU output: -0.03 GRU_TruePosQty 0 GRU_TrueNegQty 59 GRU_FalsePosQty 0 GRU_FalseNegQty 0 TotalQty 59 00000 GRU Output Correct! 00000\n",
      "Ep: 3  tStep: 24  Reward: -0.1  Button Pressed? 0  GRU output: -0.05 GRU_TruePosQty 0 GRU_TrueNegQty 60 GRU_FalsePosQty 0 GRU_FalseNegQty 0 TotalQty 60 00000 GRU Output Correct! 00000\n",
      "Ep: 3  tStep: 25  Reward: -0.69  Button Pressed? 0  GRU output: -0.34 GRU_TruePosQty 0 GRU_TrueNegQty 61 GRU_FalsePosQty 0 GRU_FalseNegQty 0 TotalQty 61 00000 GRU Output Correct! 00000\n",
      "Ep: 3  tStep: 26  Reward: -0.6  Button Pressed? 0  GRU output: -0.3 GRU_TruePosQty 0 GRU_TrueNegQty 62 GRU_FalsePosQty 0 GRU_FalseNegQty 0 TotalQty 62 00000 GRU Output Correct! 00000\n",
      "Ep: 3  tStep: 27  Reward: -0.71  Button Pressed? 0  GRU output: -0.36 GRU_TruePosQty 0 GRU_TrueNegQty 63 GRU_FalsePosQty 0 GRU_FalseNegQty 0 TotalQty 63 00000 GRU Output Correct! 00000\n",
      "Ep: 3  tStep: 28  Reward: -0.64  Button Pressed? 0  GRU output: -0.32 GRU_TruePosQty 0 GRU_TrueNegQty 64 GRU_FalsePosQty 0 GRU_FalseNegQty 0 TotalQty 64 00000 GRU Output Correct! 00000\n",
      "Ep: 3  tStep: 29  Reward: -0.23  Button Pressed? 0  GRU output: -0.12 GRU_TruePosQty 0 GRU_TrueNegQty 65 GRU_FalsePosQty 0 GRU_FalseNegQty 0 TotalQty 65 00000 GRU Output Correct! 00000\n",
      "Ep: 3  tStep: 30  Reward: -0.5  Button Pressed? 0  GRU output: -0.25 GRU_TruePosQty 0 GRU_TrueNegQty 66 GRU_FalsePosQty 0 GRU_FalseNegQty 0 TotalQty 66 00000 GRU Output Correct! 00000\n",
      "   Actionlist: [3, 2, 3, 3, 2, 4, 1, 1, 4, 3, 3, 2, 0, 1, 4, 3, 0, 2, 1, 1, 3, 1, 4, 2, 3, 4, 1, 1, 2, 0]\n",
      "Ep: 4  tStep: 1  Reward: -1.95  Button Pressed? 0  GRU output: 0 GRU_TruePosQty 0 GRU_TrueNegQty 66 GRU_FalsePosQty 0 GRU_FalseNegQty 0 TotalQty 66 unfilled GRU buffer at size 1\n",
      "Ep: 4  tStep: 2  Reward: -1.95  Button Pressed? 0  GRU output: 0 GRU_TruePosQty 0 GRU_TrueNegQty 66 GRU_FalsePosQty 0 GRU_FalseNegQty 0 TotalQty 66 unfilled GRU buffer at size 2\n",
      "Ep: 4  tStep: 3  Reward: -1.95  Button Pressed? 0  GRU output: 0 GRU_TruePosQty 0 GRU_TrueNegQty 66 GRU_FalsePosQty 0 GRU_FalseNegQty 0 TotalQty 66 unfilled GRU buffer at size 3\n",
      "Ep: 4  tStep: 4  Reward: -1.95  Button Pressed? 0  GRU output: 0 GRU_TruePosQty 0 GRU_TrueNegQty 66 GRU_FalsePosQty 0 GRU_FalseNegQty 0 TotalQty 66 unfilled GRU buffer at size 4\n",
      "Ep: 4  tStep: 5  Reward: -1.9  Button Pressed? 0  GRU output: 0 GRU_TruePosQty 0 GRU_TrueNegQty 66 GRU_FalsePosQty 0 GRU_FalseNegQty 0 TotalQty 66 unfilled GRU buffer at size 5\n",
      "Ep: 4  tStep: 6  Reward: -1.9  Button Pressed? 0  GRU output: 0 GRU_TruePosQty 0 GRU_TrueNegQty 66 GRU_FalsePosQty 0 GRU_FalseNegQty 0 TotalQty 66 unfilled GRU buffer at size 6\n",
      "Ep: 4  tStep: 7  Reward: -1.9  Button Pressed? 0  GRU output: 0 GRU_TruePosQty 0 GRU_TrueNegQty 66 GRU_FalsePosQty 0 GRU_FalseNegQty 0 TotalQty 66 unfilled GRU buffer at size 7\n",
      "Ep: 4  tStep: 8  Reward: -1.9  Button Pressed? 0  GRU output: 0 GRU_TruePosQty 0 GRU_TrueNegQty 66 GRU_FalsePosQty 0 GRU_FalseNegQty 0 TotalQty 66 unfilled GRU buffer at size 8\n",
      "Ep: 4  tStep: 9  Reward: -0.54  Button Pressed? 0  GRU output: -0.27 GRU_TruePosQty 0 GRU_TrueNegQty 67 GRU_FalsePosQty 0 GRU_FalseNegQty 0 TotalQty 67 00000 GRU Output Correct! 00000\n",
      "Ep: 4  tStep: 10  Reward: -0.54  Button Pressed? 0  GRU output: -0.27 GRU_TruePosQty 0 GRU_TrueNegQty 68 GRU_FalsePosQty 0 GRU_FalseNegQty 0 TotalQty 68 00000 GRU Output Correct! 00000\n",
      "Ep: 4  tStep: 11  Reward: -0.51  Button Pressed? 0  GRU output: -0.25 GRU_TruePosQty 0 GRU_TrueNegQty 69 GRU_FalsePosQty 0 GRU_FalseNegQty 0 TotalQty 69 00000 GRU Output Correct! 00000\n",
      "Ep: 4  tStep: 12  Reward: -0.44  Button Pressed? 0  GRU output: -0.22 GRU_TruePosQty 0 GRU_TrueNegQty 70 GRU_FalsePosQty 0 GRU_FalseNegQty 0 TotalQty 70 00000 GRU Output Correct! 00000\n",
      "Ep: 4  tStep: 13  Reward: -0.44  Button Pressed? 0  GRU output: -0.22 GRU_TruePosQty 0 GRU_TrueNegQty 71 GRU_FalsePosQty 0 GRU_FalseNegQty 0 TotalQty 71 00000 GRU Output Correct! 00000\n",
      "Ep: 4  tStep: 14  Reward: -0.38  Button Pressed? 0  GRU output: -0.19 GRU_TruePosQty 0 GRU_TrueNegQty 72 GRU_FalsePosQty 0 GRU_FalseNegQty 0 TotalQty 72 00000 GRU Output Correct! 00000\n",
      "Ep: 4  tStep: 15  Reward: -0.5  Button Pressed? 0  GRU output: -0.25 GRU_TruePosQty 0 GRU_TrueNegQty 73 GRU_FalsePosQty 0 GRU_FalseNegQty 0 TotalQty 73 00000 GRU Output Correct! 00000\n",
      "closing SnS socket\n",
      "socket DC Closed\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_6516/2556228594.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0;31m#model.learn(total_timesteps=timesteps,b_log_name=\"first_run\", reset_num_timesteps=False)#50000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimesteps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreset_num_timesteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#50000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m     \u001b[0mtoday\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoday\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/thesis/lib/python3.7/site-packages/stable_baselines/deepq/dqn.py\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, replay_wrapper)\u001b[0m\n\u001b[1;32m    217\u001b[0m                 \u001b[0menv_action\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m                 \u001b[0mreset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 219\u001b[0;31m                 \u001b[0mnew_obs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrew\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv_action\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    220\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_timesteps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/thesis/lib/python3.7/site-packages/stable_baselines/common/vec_env/base_vec_env.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    148\u001b[0m         \"\"\"\n\u001b[1;32m    149\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/thesis/lib/python3.7/site-packages/stable_baselines/common/base_class.py\u001b[0m in \u001b[0;36mstep_wait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1110\u001b[0;31m         \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdones\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minformation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvenv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munvec_obs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdones\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minformation\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/thesis/lib/python3.7/site-packages/stable_baselines/common/vec_env/dummy_vec_env.py\u001b[0m in \u001b[0;36mstep_wait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0menv_idx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_envs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuf_rews\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0menv_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuf_dones\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0menv_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuf_infos\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0menv_idx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menvs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0menv_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0menv_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuf_dones\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0menv_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m                 \u001b[0;31m# save final observation where user can get it, then reset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ScheragaThesis/UR5-RL/UR5_RL/envs/ur5_env_0.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[0;31m# we can still check if we violate a torque limit etc and if that happens, we can collect an observation, reward and make sure done is now true so we move onto a new action or terminate the current policy rollout.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmotionselector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_observation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_observation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_reward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ScheragaThesis/UR5-RL/UR5_RL/envs/ur5_env_0.py\u001b[0m in \u001b[0;36m_compute_observation\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    410\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mbot\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'blue'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    411\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 412\u001b[0;31m             \u001b[0mdata2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msock_DC\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m88\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#64\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    413\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mdata2\u001b[0m\u001b[0;34m==\u001b[0m \u001b[0;34mb''\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m                 \u001b[0mdata2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msock_DC\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m88\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#64  #48 bytes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#https://github.com/ZhizhenQin/BalancingBot/blob/master/balance-bot/balance_bot/balancebot_task.py\n",
    "\n",
    "\n",
    "#%load_ext tensorboard\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore',category=FutureWarning)\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import gym\n",
    "from stable_baselines import DQN, PPO2\n",
    "from stable_baselines import results_plotter\n",
    "from stable_baselines.bench import Monitor\n",
    "from stable_baselines.results_plotter import load_results, ts2xy\n",
    "from stable_baselines.common.noise import AdaptiveParamNoiseSpec\n",
    "from stable_baselines.common.callbacks import BaseCallback\n",
    "from stable_baselines.deepq.policies import MlpPolicy\n",
    "from stable_baselines.common.policies import FeedForwardPolicy, register_policy,LstmPolicy\n",
    "from stable_baselines.common.tf_layers import conv, linear, conv_to_fc, lstm\n",
    "from stable_baselines.common.vec_env import DummyVecEnv\n",
    "from stable_baselines.common.env_checker import check_env\n",
    "from datetime import date\n",
    "import csv\n",
    "#import balance_bot\n",
    "import socket\n",
    "import numpy as np\n",
    "from time import sleep\n",
    "import UR5_RL\n",
    "\n",
    "class GRUNet(nn.Module):\n",
    "        def __init__(self, input_dim, hidden_dim, output_dim, n_layers, drop_prob=0.2):\n",
    "            super(GRUNet, self).__init__()\n",
    "            self.hidden_dim = hidden_dim\n",
    "            self.n_layers = n_layers\n",
    "\n",
    "            self.gru = nn.GRU(input_dim, hidden_dim, n_layers, batch_first=True, dropout=drop_prob)\n",
    "            self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "            self.relu = nn.ReLU()\n",
    "\n",
    "        def forward(self, x, h):\n",
    "            out, h = self.gru(x, h)\n",
    "            out = self.fc(self.relu(out[:,-1]))\n",
    "            return out, h\n",
    "\n",
    "        def init_hidden(self, batch_size):\n",
    "            weight = next(self.parameters()).data\n",
    "            hidden = weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device)\n",
    "            return hidden\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "#HOST_SnS = '192.168.0.103'\n",
    "HOST_SnS = '128.138.224.89'\n",
    "PORT_SnS= 65490\n",
    "\n",
    "\n",
    "try:\n",
    "    # Create a TCP/IP socket\n",
    "    sock_SnS = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "    #sock_SnS.setblocking(False)\n",
    "    # Connect the socket to the port where the server is listening\n",
    "    server_address_SnS = (HOST_SnS, PORT_SnS)\n",
    "    print('connecting to {} port {}'.format(*server_address_SnS))\n",
    "    sock_SnS.connect(server_address_SnS)\n",
    "\n",
    "\n",
    "    #def callback(lcl, glb):\n",
    "         #stop training if reward exceeds 199\n",
    "    #    is_solved = lcl['t'] > 1000 and sum(lcl['episode_rewards'][-101:-1]) / 100 >= 1\n",
    "    #    return is_solved\n",
    "\n",
    "    #https://stable-baselines.readthedocs.io/en/master/guide/custom_policy.html\n",
    "    #Layers of 20 and 15\n",
    "\n",
    "    #Do this only after restarting the notebook!! \n",
    "    #register_policy('ScottLSTMPolicy', ScottLSTMPolicy)    \n",
    "    #print(\"lstm registered\")\n",
    "\n",
    "    #try:\n",
    "    \n",
    "    #code stopped at ep 36 at 10steps per ep, 80 ep\n",
    "    StepsPerEpisode=30 #was 10\n",
    "    TotalEpisodes=500  #was 80\n",
    "    env= gym.make(\"ur5-rl-v0\",StepsPerEpisode=StepsPerEpisode,TotalEpisodes=TotalEpisodes,\n",
    "                 continuousactionspace=False,actualbutton=True,GRUrewards=True)\n",
    "    env = DummyVecEnv([lambda: env])\n",
    "\n",
    "    timesteps=(StepsPerEpisode*TotalEpisodes)\n",
    "    print(\"Total timesteps:\",timesteps)\n",
    "    \n",
    "    \"\"\"\n",
    "    class ScottLSTMPolicy(LstmPolicy):\n",
    "        def __init__(self, sess, ob_space, ac_space, n_env=1, n_steps=StepsPerEpisode,\n",
    "                     n_batch=StepsPerEpisode, n_lstm=StepsPerEpisode, reuse=False,  **_kwargs):\n",
    "            super().__init__(sess, ob_space, ac_space, n_env, n_steps, n_batch, n_lstm, reuse,\n",
    "                             net_arch=[7,'lstm',dict(vf=[20, 15],pi=[20,15])],\n",
    "                             layer_norm=True, feature_extraction=\"mlp\", **_kwargs)\n",
    "    \"\"\"        \n",
    "    #model = DQN(\"LnMlpPolicy\", env, learning_rate=1e-3, prioritized_replay=True,gamma=1 , buffer_size=50000,param_noise=False,\n",
    "    # exploration_initial_eps=0.1, exploration_final_eps=0.1,learning_starts=1, verbose=1)\n",
    "\n",
    "\n",
    "    #model = PPO2(\"MlpPolicy\", env,verboThe new research shows that \"we must expect extreme event records to be broken - not just by small marse=0)\n",
    "    #model = PPO2(\"MlpLstmPolicy\", env,nminibatches=1, n_steps=80, #exploration_initial_eps=1, exploration_final_eps=0.1,\n",
    "    #             verbose=0,tensorboard_log=\"./ScottPPOLstm/\") #n_lstm=2, n_batch=80, nminibatches=10,\n",
    "\n",
    "    #model = PPO2(ScottLSTMPolicy, env,nminibatches=1, n_steps=StepsPerEpisode,learning_rate=0.1,\n",
    "    #             verbose=2)\n",
    "    \n",
    "    model = DQN(MlpPolicy, env, learning_rate=0.1,exploration_final_eps=0.02, exploration_initial_eps=1.0,  verbose=1)\n",
    "    \n",
    "    # DEFAULT learning_rate=0.00025  #n_lstm=2, n_batch=80, nminibatches=10,\n",
    "    ##exploration_initial_eps=1, exploration_final_eps=0.1,\n",
    "\n",
    "    #model.learn(total_timesteps=timesteps,b_log_name=\"first_run\", reset_num_timesteps=False)#50000\n",
    "    model.learn(total_timesteps=timesteps,reset_num_timesteps=False)#50000\n",
    "    \n",
    "    today = date.today()\n",
    "    todaydate = today.strftime(\"%m_%d_%Y\")\n",
    "    RLmodelfilename=\"UR5-RL_DQN_savedpolicy_cylinder_withbutton_train_noposeobs_GRUrewards_10-4_13-2021GRU_lookahead\"+todaydate\n",
    "    model.save(RLmodelfilename)# save trained model\n",
    "    print(\"model saved\")\n",
    "    del model\n",
    "    print(\"training complete\")\n",
    "    \n",
    "\n",
    "finally:\n",
    "    endmsg='end'\n",
    "    data1=endmsg.encode('ascii')    \n",
    "    sock_SnS.sendall(data1)\n",
    "    sock_SnS.sendall(data1)\n",
    "    print('closing SnS socket')\n",
    "    sock_SnS.close()\n",
    "    \n",
    "    HOST2 = '192.168.0.103'\n",
    "    PORT2= PORT_SnS-10 #65481\n",
    "    sock_DC = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "    server_address_DC = (HOST2, PORT2)\n",
    "    sock_DC.close()\n",
    "    print(\"socket DC Closed\")\n",
    "      \n",
    "    #gitkraken\n",
    "    \n",
    "    #check_env(env)\n",
    "    #https://stable-baselines.readthedocs.io/en/master/modules/dqn.html\n",
    "    #model.learn(total_timesteps=25000)\n",
    "    #del model # remove to demonstrate saving and loading\n",
    "    #model = DQN.load(\"deepq_cartpole\")\n",
    "\n",
    "    \n",
    "    #Starting at 11:02AM\n",
    "    #80 episodes in 45 min\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if arduino error, run this in terminal:\n",
    "#sudo chmod a+rw /dev/ttyACM0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_15278/1992225243.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtodaydate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtoday\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrftime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%m_%d_%Y\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mRLmodelfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"UR5-RL_DQN_savedpolicy_cylinder_withbutton_train_\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mtodaydate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRLmodelfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m# save trained model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"saved\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "from datetime import date\n",
    "today = date.today()\n",
    "todaydate = today.strftime(\"%m_%d_%Y\")\n",
    "RLmodelfilename=\"UR5-RL_DQN_savedpolicy_cylinder_withbutton_train_\"+todaydate\n",
    "model.save(RLmodelfilename)# save trained model\n",
    "print(\"saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved!\n"
     ]
    }
   ],
   "source": [
    "model.save(\"UR5-RL_savedpolicy-10-15-2021\")  #10-4 and 6 were good policies\n",
    "print(\"saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HOST2 = '192.168.0.103'learning_rate=0.00025\n",
    "PORT2= 65485\n",
    "sock_DC = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "server_address_DC = (HOST2, PORT2)\n",
    "sock_DC.close()\n",
    "print(\"socket DC Closed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[Errno 9] Bad file descriptor",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-5c92c6e2c5c7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mendmsg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'end'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdata1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mendmsg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ascii'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0msock_SnS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msendall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'closing SnS socket'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0msock_SnS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 9] Bad file descriptor"
     ]
    }
   ],
   "source": [
    "endmsg='end'\n",
    "data1=endmsg.encode('ascii')    \n",
    "sock_SnS.sendall(data1)\n",
    "\n",
    "\n",
    "#print('closing SnS socket')\n",
    "#sock_SnS.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "connecting to 128.138.224.89 port 65490\n",
      "GRU model loaded\n",
      "TotalEpisodes: 380    StepsPerEpisode: 30\n",
      "Loading a model without an environment, this model cannot be trained until it has a valid environment.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/scott/anaconda3/envs/thesis/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4974/915785649.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;31m#model = DQN.load(\"UR5-RL_savedpolicy-10-6-2021\")  #seems to only go to the side (action 1).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;31m#model = DQN.load(\"UR5-RL_DQN_savedpolicy_10_14_2021\")  #seems to only go down (action 4).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDQN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"UR5-RL_DQN_savedpolicy_cylinder_withbutton_train_noposeobs_10_21_2021\"\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m#seems to only go down (action 4).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/thesis/lib/python3.7/site-packages/stable_baselines/common/base_class.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(cls, load_path, env, custom_objects, **kwargs)\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1054\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_env\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1055\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1056\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1057\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/thesis/lib/python3.7/site-packages/stable_baselines/deepq/dqn.py\u001b[0m in \u001b[0;36msetup_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    139\u001b[0m                     \u001b[0msess\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m                     \u001b[0mfull_tensorboard_log\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull_tensorboard_log\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m                     \u001b[0mdouble_q\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdouble_q\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m                 )\n\u001b[1;32m    143\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproba_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproba_step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/thesis/lib/python3.7/site-packages/stable_baselines/deepq/build_graph.py\u001b[0m in \u001b[0;36mbuild_train\u001b[0;34m(q_func, ob_space, ac_space, optimizer, sess, grad_norm_clipping, gamma, double_q, scope, reuse, param_noise, param_noise_filter_func, full_tensorboard_log)\u001b[0m\n\u001b[1;32m    391\u001b[0m         \u001b[0mact_t_ph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"action\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m         \u001b[0mrew_t_ph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"reward\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 393\u001b[0;31m         \u001b[0mdone_mask_ph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"done\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    394\u001b[0m         \u001b[0mimportance_weights_ph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"weight\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/thesis/lib/python3.7/site-packages/tensorflow_core/python/ops/array_ops.py\u001b[0m in \u001b[0;36mplaceholder\u001b[0;34m(dtype, shape, name)\u001b[0m\n\u001b[1;32m   2617\u001b[0m                        \"eager execution.\")\n\u001b[1;32m   2618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2619\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2620\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/thesis/lib/python3.7/site-packages/tensorflow_core/python/ops/gen_array_ops.py\u001b[0m in \u001b[0;36mplaceholder\u001b[0;34m(dtype, shape, name)\u001b[0m\n\u001b[1;32m   6667\u001b[0m   \u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_execute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"shape\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6668\u001b[0m   _, _, _op = _op_def_lib._apply_op_helper(\n\u001b[0;32m-> 6669\u001b[0;31m         \"Placeholder\", dtype=dtype, shape=shape, name=name)\n\u001b[0m\u001b[1;32m   6670\u001b[0m   \u001b[0m_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6671\u001b[0m   \u001b[0m_inputs_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/thesis/lib/python3.7/site-packages/tensorflow_core/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    792\u001b[0m         op = g.create_op(op_type_name, inputs, dtypes=None, name=scope,\n\u001b[1;32m    793\u001b[0m                          \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattr_protos\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 794\u001b[0;31m                          op_def=op_def)\n\u001b[0m\u001b[1;32m    795\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    796\u001b[0m       \u001b[0;31m# Conditionally invoke tfdbg v2's op callback(s).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/thesis/lib/python3.7/site-packages/tensorflow_core/python/util/deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    478\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mis_in_graph_mode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIS_IN_GRAPH_MODE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0m_PRINT_DEPRECATION_WARNINGS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m         \u001b[0minvalid_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 480\u001b[0;31m         \u001b[0mnamed_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_inspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetcallargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    481\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0marg_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspec\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeprecated_positions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    482\u001b[0m           if (spec.position < len(args) and\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore',category=FutureWarning)\n",
    "import tensorflow as tf\n",
    "#import tensorflow.contrib.layers as layers\n",
    "import tensorflow as tf\n",
    "#from tensorflow import keras\n",
    "#from tensorflow.keras import layers\n",
    "#from keras.utils import to_categorical\n",
    "#from keras.models import Sequential\n",
    "#from keras.layers import Dense, Dropout, Embedding, LSTM, GlobalMaxPooling1D, SpatialDropout1D\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import gym\n",
    "from stable_baselines import DQN,PPO2\n",
    "from stable_baselines import results_plotter\n",
    "from stable_baselines.bench import Monitor\n",
    "from stable_baselines.results_plotter import load_results, ts2xy\n",
    "from stable_baselines.common.noise import AdaptiveParamNoiseSpec\n",
    "from stable_baselines.common.callbacks import BaseCallback\n",
    "from stable_baselines.deepq.policies import MlpPolicy\n",
    "from stable_baselines.common.policies import FeedForwardPolicy, register_policy,LstmPolicy\n",
    "from stable_baselines.common.tf_layers import conv, linear, conv_to_fc, lstm\n",
    "#from stable_baselines.common import get_vec_normalize_env\n",
    "from stable_baselines.common.env_checker import check_env\n",
    "from stable_baselines.common.vec_env import DummyVecEnv\n",
    "#import balance_bot\n",
    "\n",
    "import socket\n",
    "import numpy as np\n",
    "from time import sleep\n",
    "import UR5_RL\n",
    "\n",
    "#env= gym.make(\"UR5-RL-env\", render=True)\n",
    "#HOST_SnS = '192.168.0.103'\n",
    "HOST_SnS = '128.138.224.89'\n",
    "PORT_SnS= 65490\n",
    "\n",
    "class GRUNet(nn.Module):\n",
    "        def __init__(self, input_dim, hidden_dim, output_dim, n_layers, drop_prob=0.2):\n",
    "            super(GRUNet, self).__init__()\n",
    "            self.hidden_dim = hidden_dim\n",
    "            self.n_layers = n_layers\n",
    "\n",
    "            self.gru = nn.GRU(input_dim, hidden_dim, n_layers, batch_first=True, dropout=drop_prob)\n",
    "            self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "            self.relu = nn.ReLU()\n",
    "\n",
    "        def forward(self, x, h):\n",
    "            out, h = self.gru(x, h)\n",
    "            out = self.fc(self.relu(out[:,-1]))\n",
    "            return out, h\n",
    "\n",
    "        def init_hidden(self, batch_size):\n",
    "            weight = next(self.parameters()).data\n",
    "            hidden = weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device)\n",
    "            return hidden\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "#try:\n",
    "\n",
    "# Create a TCP/IP socket\n",
    "sock_SnS = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "#sock_SnS.setblocking(False)\n",
    "# Connect the socket to the port where the server is listening\n",
    "server_address_SnS = (HOST_SnS, PORT_SnS)\n",
    "print('connecting to {} port {}'.format(*server_address_SnS))\n",
    "sock_SnS.connect(server_address_SnS)\n",
    "\n",
    "\n",
    "StepsPerEpisode=30 #was 10\n",
    "TotalEpisodes=380  #was 80\n",
    "env= gym.make(\"ur5-rl-v0\",StepsPerEpisode=StepsPerEpisode,TotalEpisodes=TotalEpisodes,\n",
    "                 continuousactionspace=False,actualbutton=True)\n",
    "\n",
    "\n",
    "env = DummyVecEnv([lambda: env])  #no clue what this line does\n",
    "\n",
    "#model = DQN.load(\"UR5-RL_savedpolicy-10-4-2021\")  #seems to only go down (action 4). \n",
    "#model = DQN.load(\"UR5-RL_savedpolicy-10-6-2021\")  #seems to only go to the side (action 1).\n",
    "#model = DQN.load(\"UR5-RL_DQN_savedpolicy_10_14_2021\")  #seems to only go down (action 4). \n",
    "model = DQN.load(\"UR5-RL_DQN_savedpolicy_cylinder_withbutton_train_noposeobs_10_21_2021\")  #seems to only go down (action 4).\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#model = DQN(MlpPolicy, env, learning_rate=0.1,exploration_final_eps=0.02, exploration_initial_eps=1.0,  verbose=1)\n",
    "\n",
    "\n",
    "\n",
    "#env=model.get_env()\n",
    "#obs = env.reset()\n",
    "done = [False for _ in range(1)] #env.num_envs\n",
    "state=None\n",
    "\n",
    "while True:\n",
    "    obs = env.reset()\n",
    "    #env._seed()\n",
    "    for i in range(500000):\n",
    "        action, _states = model.predict(obs,state=state,mask=done)\n",
    "        #actionlist.append(action)\n",
    "        obs, rewards, dones, info = env.step(action)\n",
    "        print(obs)\n",
    "    #print(rewards)\n",
    "        \n",
    "        \n",
    "        \n",
    "\"\"\"\n",
    "\n",
    "while True:\n",
    "    obs = env.reset()\n",
    "    env._seed()\n",
    "    actionlist=[]\n",
    "    #print(\"reset\")\n",
    "    for i in range(80):\n",
    "        action, _states = model.predict(obs)\n",
    "        actionlist.append(action)\n",
    "        obs, rewards, dones, info = env.step(action)\n",
    "    print(rewards)       \n",
    "\"\"\"       \n",
    "        \n",
    "        \n",
    "#print(\"actionlist\",actionlist)\n",
    "#env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.5522386]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
