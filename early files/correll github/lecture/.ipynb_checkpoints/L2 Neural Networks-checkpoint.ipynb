{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-layer Neural Networks\n",
    "\n",
    "We have seen that the single Perceptron is able to linearly separate a dataset, spitting out \"0\" or \"1\" as a function of the data being below or above the separating hyperplane defined by the weight vector $w$. It is easy to see that some problems cannot be linearly separated. \n",
    "\n",
    "<center>\n",
    "<img src=\"figs/xorproblem.svg\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the example above, the red and the blue data points are not separable by a single line, but require at least two. This problem is known as the \"XOR\" problem, which can be seen by looking at just four data points at $(0,0)$, $(0,1)$, $(1,0)$, and $(1,1)$. Creating a truth table (right) reveals a truth table with the characteristics of logical exclusive or (XOR), that is $x_1$ and $x_2$ have to be different for the output to be true (here \"blue\"), whereas the output is false (here \"red\") when the inputs are the same.\n",
    "\n",
    "We already know that a single Perceptron can create a single separating hyperplane, we will therefore need at least two Perceptrons to solve the XOR problem. Using two Perceptrons in parallel will yield us with tuples of the kind $(0,0)$, $(0,1)$ and so on. We therefore need one more Perceptron to recombine these tuples into a single output. The figure below shows the simple-most multi-layer Perceptron that can be trained for the XOR problem, with one input layer, a so-called hidden layer, and an output layer.\n",
    "\n",
    "<center>\n",
    "<img src=\"figs/basicnetworks.svg\" width=\"50%\">\n",
    "</center>    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that each Perceptron comes with its own weight vector $w$. Given the input being of dimension $m$, the multi-layer perceptron with two Perceptrons in the hidden layer therefore has $2m+2+3$ parameters, which account for the $m$-dimensional input to the two Perceptrons in the hidden layer, the 2 parameters of the output layer Perceptron, and the three bias values. \n",
    "\n",
    "The output is computed by first computing the output from the two Perceptrons in the hidden layer and stacking their outputs into a vector, which serves as input vector to the Output perceptron. This is called <i>forward propagation</i>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formal description of a multi-layer ANN\n",
    "\n",
    "As with the simple Perceptron, we will use node i's bias as the 0-th weight vector, that is \n",
    "\n",
    "$$w^k_{0,j}=b^k_j$$\n",
    "\n",
    "Here, we use the following notation. We will denote the <i>layer</i> with a superscript, and the index of the incoming node and the outgoing node with a subscript tuple. That is $w^k_{i,j}$ is connecting the i-th incoming weight to the $j-th$ node of the $k-th$ layer. (The i-th incoming weight is the j-th node in layer $k-1$.) This, as well as the simple example network from above, are illustrated below: \n",
    "\n",
    "<center>\n",
    "    <img src=\"figs/backpropnotation.svg\" width=\"50%\">\n",
    "</center>\n",
    "\n",
    "Each layer, denoted by the index $k$, has exactly $r^k$ nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inputs and outputs\n",
    "\n",
    "The output $o_i$ of output node $i$ is given by\n",
    "\n",
    "$$o_i=g(a_i^k)$$\n",
    "\n",
    "where $g()$ is a non-linear activation function. Here, $a_i^k$ is the weighted sum computed by node $i$ in layer $k$, also known as the <i>activation</i>:\n",
    "\n",
    "$$a_i^k=\\sum_{j=0}^{r_{k-1}}w_{j,i}^ko_j^{k-1}$$\n",
    "\n",
    "with $o_j^{k-1}$ is the j-th output of the previous layer. This is illustrated in the image below:\n",
    "\n",
    "<center>\n",
    "    <img src=\"figs/backpropnotation2.svg\" width=\"50%\">\n",
    "</center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case of $k$ being the output layer, $o_i^k$ should be equivalent to $y_i^k$. Likewise, in case of $k-1$ being the input layer $o_i^{k-1}=x_i$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation functions\n",
    "\n",
    "Using a on-off Heaviside step function makes training a neural network using backpropagation rather difficult as a function that switches from \"not working at all\" to \"working completely\" provides very little information in which direction to move. It is therefore more desirable to have a smooth activation function. One such function is the <a href=\"https://en.wikipedia.org/wiki/Sigmoid_function\">sigmoid function</a>:\n",
    "\n",
    "$$\\sigma(x)=\\frac{1}{1+e^{-x}}$$\n",
    "\n",
    "Its main characteristics are that it stays between 0 and 1, and crosses the y-axis at 0.5\n",
    "\n",
    "<center>\n",
    "<img src=\"figs/sigmoid.svg\" width=\"50%\">\n",
    "</center>\n",
    "\n",
    "The sigmoid function is very attractive for learning using backpropagation as the direction in which the weights should move to improve the error is very clear in the vicinity of $wx=0$, and computing its derivative is rather simple as we see below. In case of $wx$ being very large, or very small, the neuron either saturates or never activates, also known as the <i>vanishing gradient</i> problem. Another drawback is that computing the sigmoid function is computationally expensive. A similar function is the hyperbolic tangent $\\tanh()$ which remains in the range of -1 to 1 and crosses the y-axis at 0.\n",
    "\n",
    "A popular solution to decrease computation time is the Rectified Linear Unit (ReLU), which is given by\n",
    "\n",
    "$$R(x)=max(0,x)$$\n",
    "\n",
    "and is shown in the figure below\n",
    "<center>\n",
    "<img src=\"figs/ReLU.svg\" width=\"50%\">\n",
    "</center>\n",
    "the dashed line indicates a refinement of the ReLU known as \"leaky ReLU\" with a slope of typical 0.1, and improves learning for negative $wx$ by providing a directional gradient. \n",
    "\n",
    "Note that we only talk about \"Perceptrons\" when the Heaviside step function is used as activation function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a multi-layer neural network\n",
    "\n",
    "Finding a set of weights and bias values, that is 9 parameters for a simple two-dimensional problem, manually is an NP-complete problem (Blum, 1992). We consider a training dataset consisting of input-output pairs $x_i$ and ouput $y_i$ with $i=1..N$, and a feedforward neural network with parameters $w$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error function\n",
    "\n",
    "The goal of training is to minimize an error function such as the mean squared error \n",
    "$$E(x,y,w)=\\frac{1}{2N}\\sum_{i=1}^{N}(\\hat{y_i}-y_i)^2$$\n",
    "between the output $\\hat{y_i}$ that the neural network with parameters $w$ computes and the known value $y_i$ from what is known as the <i>training set</i>. \n",
    "\n",
    "Similar to the Perceptron, we can reduce $E(x,y,w)$ by iteratively descending along its gradient, that is\n",
    "\n",
    "$$ w(t+1)=w(t)-\\alpha \\frac{\\partial E(x,y,w(t))}{\\partial w}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropagation\n",
    "Calculating the partial derivatives for the error function is not straightforward, however, as the neural network implements a computation graph, transforming the input $x$ by a series of multiplications and non-linear activation functions, which in turn require the <a href=\"https://en.wikipedia.org/wiki/Chain_rule\">chain</a> and <a href=\"https://en.wikipedia.org/wiki/Product_rule\">product</a> rule. \n",
    "\n",
    "Applying the chain and product rules can be done in two ways: moving forwards or backwards through the computation graph. Actually doing this by hand for a simple graph shows that going backwards is significantly more efficient, which is extensively described <a href=\"http://colah.github.io/posts/2015-08-Backprop/\">here</a>. Manually deriving the individual partial derivatives also illustrates that many of the computations can actually be recycled. This solution is known as <i>backpropagation</i> (Rumelhart, 1985), a technique that has been independently discovered in multiple fields. The derivation below follows the blog post on <a href=\"https://brilliant.org/wiki/backpropagation/\">brilliant.org</a>.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a first step, we note that the error function is a sum over all input-output pairs:\n",
    "\n",
    "$$ \\frac{\\partial E(x,y,w)}{\\partial w_{i,j}^k}=\\frac{1}{2N}\\sum^N_{d=1}\\frac{\\partial}{\\partial w_{i,j}^k}(\\hat{y_d}-y_d)^2=\\frac{1}{2N}\\sum_{d=1}^N\\frac{\\partial E_d}{\\partial w_{i,j}^k}$$\n",
    "\n",
    "We will therefore focus on only one input-output pair $(x_d,y_d)$ and differentiate against $w_{i,j}^k$. (The index $d$ has been chosen to avoid confusion with the indices $i$ and $j$, and will be omitted for brevity in the remainder)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The chain rule\n",
    "\n",
    "The key for understanding the backpropagation algorithm is to apply the chain rule in a correct way. Specifically, if a variable $z$ depends on the variable $y$, which itself depends on the variable $x$, then\n",
    "$$\\frac{dz}{dx}=\\frac{dz}{dy}\\frac{dy}{dx}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the output layer having index $m$ and a single output ($a^m_1$), the error is computed by the recursive formula\n",
    "$$E(x,y,w_{i,j})=\\frac{1}{2}(\\hat{y}-y)^2=\\frac{1}{2}(g(a_1^m)-y)^2=\n",
    "\\frac{1}{2}\\left(g\\left(\\sum_{l=0}^{r_{m-1}}w_{l,1}^mo_l^{m-1}\\right)-y\\right)^2.$$\n",
    "We observe that the variable $E$ depends on the outputs $o_l^{m-1}$ with $l=0..r_{m-1}$ from the previous layer. Recall that $o_l^{m-1}$ is simply the activation $a_l^{m-1}$ after applying the activation function. Also recall that $w^m_{i,1}$ are weights coming into node $1$. The error with respect to $w_{i,j}$ is therefore dependent on all $a^k_j$ for all previous layers. This is also visualized below:\n",
    "\n",
    "<center>\n",
    "    <img src=\"figs/backpropnotation3.svg\" width=\"100%\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The chain rule therefore states\n",
    "\n",
    "$$ \\frac{\\partial E}{\\partial w_{i,j}^k}=\\frac{\\partial E}{\\partial a_i^k}\\frac{\\partial a_i^k}{\\partial w_{i,j}^k}.$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Error at layer k\n",
    "The first term is part of a vector called the \"error at layer $k$\"  that consists of errors at all nodes $j$ in layer $k$ and is denoted by \n",
    "\n",
    "$$\\delta^k_j=\\frac{\\partial E}{\\partial a_j^k}$$\n",
    "\n",
    "The second term can be computed from the definition of $a_j^k$ above\n",
    "\n",
    "$$\\frac{\\partial a^k_j}{\\partial w_{i,j}^k}=\\frac{\\partial}{\\partial w_{i,j}^k}\\left(\\sum_{l=0}^{r_{k-1}} w_{l,j}^k o^{k-1}_l\\right)=o^{k-1}_i$$\n",
    "\n",
    "which follows from the fact that only the term involving $o^{k-1}_i$ is the one where $l=i$. In case you expect the chain rule to apply further, remember that $o^{k-1}_i$ is actually not dependent on $w_{i,j}^k$, so you are done here.\n",
    "\n",
    "Thus, the partial derivative of the error function $E$ with respect to weight $w_{i,j}^k$ is\n",
    "\n",
    "$$ \\frac{\\partial E}{\\partial w^k_{i,j}}=\\delta^k_jo^{k-1}_i.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the error $E$ with respect to each individual weight $w_{i,j}^k$ in a layer $k$ depends on the output of the layers coming before that. This is intuitive, as information propagates through the network. We will now also show that the error term $\\delta_j^k$ actually depends on the error at layers above $k$, that is stems from the error $\\hat{y}-y$ that we ultimately want to minimize. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Backward propagation of error\n",
    "\n",
    "In order to show how the error term $\\delta^k_i$ relates to the error  at the output layer, we will start working backwards. Let $m$ be the index of the output layer. We are also only considering a network with one output neuron, that is $j=1$. The error at this final layer $m$ is given by\n",
    "\n",
    "$$E=\\frac{1}{2}(\\hat{y}-y)^2=\\frac{1}{2}(g(a_1^m)-y)^2$$\n",
    "\n",
    "Using the chain rule $\\frac{\\partial E}{\\partial w_{i,1}^m}=\\frac{\\partial E}{\\partial a^m_i}\\frac{\\partial a^m_i}{\\partial w^k_{i,1}}$ as before yields \n",
    "\n",
    "$$ \\delta^m_1=\\frac{\\partial E}{\\partial a^m_1}=(g(a^m_1)-y)g'(a^m_1)=(\\hat{y}-y)g'(a^m_1)$$\n",
    "\n",
    "for the error at layer $m$ and\n",
    "\n",
    "$$\\frac{\\partial a^m_1}{\\partial w^k_{i,1}}=o_i^{m-1}.$$\n",
    "\n",
    "Together, these two result into\n",
    "\n",
    "$$\\frac{\\partial E}{\\partial w_{i,1}^m}=(\\hat{y}-y)g'(a^m_1)o_i^{m-1}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We continue to use the chain rule to work backward along the computation graph. Specifically, the activation $a^k_j$ at node $j$ in layer $k$, with $1\\leq k <m$ feeds into all nodes $l=1..r^{k+1}$ of layer $k+1$. Therefore, the error $\\delta^k_j$ calculates to\n",
    "\n",
    "$$\\delta^k_j=\\frac{\\partial E}{\\partial a^k_j}=\\sum_{l=1}^{r^{k+1}}\\frac{\\partial E}{\\partial a_l^{k+1}}\\frac{\\partial a_l^{k+1}}{\\partial a^k_j}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using $\\delta^{k+1}_l=\\frac{\\partial E}{\\partial a_l^{k+1}}$, the above equation simplifies to\n",
    "\n",
    "$$\\delta^k_j=\\sum_{l=1}^{r^{k+1}}\\delta_l^{k+1}\\frac{\\partial a_l^{k+1}}{\\partial a^k_j}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspecting the computation graph or the definition of $a^k_j$, we recall that $a_l^{k+1}$ receives the output $g(a_j^k)$ from every node $j=1..r^k$ in layer $k$ via weight $w_{j,l}^{k+1}$, i.e.\n",
    "\n",
    "$$a_l^{k+1}=\\sum_{j=1}^{r^k}w_{j,l}^{k+1}g(a_j^k)$$\n",
    "\n",
    "allowing us to compute the partial derivative\n",
    "\n",
    "$$\\frac{\\partial a_l^{k+1}}{\\partial a^k_j}=w_{j,l}^{k+1}g'(a_j^k).$$\n",
    "\n",
    "This allows us to provide the error at node $j$ in layer $k$, also known as the <b>backpropagation formula</b>:\n",
    "\n",
    "$$ \\delta^k_j=g'(a^k_j)\\sum_{l=1}^{r^{k+1}}w_{j,l}^{k+1}\\delta^{k+1}_l$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this last part, we are able to define a recursive definition to calculate the desired error gradient with respect to all weights in the neural network:\n",
    "\n",
    "$$\\frac{\\partial E}{\\partial w_{i,j}^k}=\\delta_j^ko_i^{k-1}=g'(a_j^k)o_i^{k-1}\\sum_{l=1}^{r^{k+1}}w_{j,l}^{k+1}\\delta_l^{k+1}.$$\n",
    "\n",
    "This computation can be executed layer by layer, starting from the output layer and working its way backward. This phase is computationally very similar to the forward phase and allows reusing all the activations and outputs that have been previously computed. As an extra goody, the derivative of the sigmoid function $\\sigma'(x)=\\sigma(x)(1-\\sigma(x))$, resulting in\n",
    "\n",
    "$$\\frac{\\partial E}{\\partial w_{i,j}^k}=\\delta_j^ko_i^{k-1}=g(a_j^k)(1-g(a_j^k))o_i^{k-1}\\sum_{l=1}^{r^{k+1}}w_{j,l}^{k+1}\\delta_l^{k+1}.$$\n",
    "\n",
    "and from there\n",
    "\n",
    "$$\\frac{\\partial E}{\\partial w_{i,j}^k}=\\delta_j^ko_i^{k-1}=o_j^k(1-o_j^k)o_i^{k-1}\\sum_{l=1}^{r^{k+1}}w_{j,l}^{k+1}\\delta_l^{k+1},$$\n",
    "\n",
    "omitting the need to store $a_j^k$ in addition to $o_j^k$, reducing the memory requirements of the algorithm by half.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropagation algorithm\n",
    "\n",
    "Training a network now follows these simple steps:\n",
    "\n",
    "1. Randomly initialize the network's weigths\n",
    "2. Compute the error for this network for each item in the training set and store the output from each layer (forward propagation)\n",
    "3. Use the recursive formula for $\\frac{\\partial E}{\\partial w^k_{i,j}}$ to compute the gradient of the error function with respect to each weight using the stored values of the output from forward propagation and calculate the average over the entire training set.\n",
    "4. Repeat steps 2-3 for a fixed number of iterations or when the error becomes reasonably small.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code from <a href=\"https://brilliant.org/wiki/backpropagation/\">Brilliant.org</a>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output After Training: \n",
      "[[2.68748579e-09]\n",
      " [9.99999996e-01]\n",
      " [9.99999996e-01]\n",
      " [7.21968052e-09]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# define the sigmoid function\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# choose a random seed for reproducible results\n",
    "np.random.seed(1)\n",
    "\n",
    "# learning rate\n",
    "alpha = 1\n",
    "\n",
    "# number of nodes in the hidden layer\n",
    "num_hidden = 2\n",
    "\n",
    "# inputs\n",
    "X = np.array([  \n",
    "    [0, 0],\n",
    "    [0, 1],\n",
    "    [1, 0],\n",
    "    [1, 1]\n",
    "])\n",
    "\n",
    "# outputs\n",
    "# x.T is the transpose of x, making this a column vector\n",
    "y = np.array([[0, 1, 1, 0]]).T\n",
    "\n",
    "# initialize weights randomly with mean 0 and range [-1, 1]\n",
    "# the +1 in the 1st dimension of the weight matrices is for the bias weight\n",
    "hidden_weights = 2*np.random.random((X.shape[1] + 1, num_hidden)) - 1\n",
    "output_weights = 2*np.random.random((num_hidden + 1, y.shape[1])) - 1\n",
    "\n",
    "# number of iterations of gradient descent\n",
    "num_iterations = 1000\n",
    "\n",
    "# for each iteration of gradient descent\n",
    "for i in range(num_iterations):\n",
    "\n",
    "    # forward phase\n",
    "    # np.hstack((np.ones(...), X) adds a fixed input of 1 for the bias weight\n",
    "    input_layer_outputs = np.hstack((np.ones((X.shape[0], 1)), X))\n",
    "    hidden_layer_outputs = np.hstack((np.ones((X.shape[0], 1)), sigmoid(np.dot(input_layer_outputs, hidden_weights))))\n",
    "    output_layer_outputs = np.dot(hidden_layer_outputs, output_weights)\n",
    "\n",
    "    # backward phase\n",
    "    # output layer error term\n",
    "    output_error = output_layer_outputs - y\n",
    "    # hidden layer error term\n",
    "    # [:, 1:] removes the bias term from the backpropagation\n",
    "    hidden_error = hidden_layer_outputs[:, 1:] * (1 - hidden_layer_outputs[:, 1:]) * np.dot(output_error, output_weights.T[:, 1:])\n",
    "\n",
    "    # partial derivatives\n",
    "    hidden_pd = input_layer_outputs[:, :, np.newaxis] * hidden_error[: , np.newaxis, :]\n",
    "    output_pd = hidden_layer_outputs[:, :, np.newaxis] * output_error[:, np.newaxis, :]\n",
    "\n",
    "    # average for total gradients\n",
    "    total_hidden_gradient = np.average(hidden_pd, axis=0)\n",
    "    total_output_gradient = np.average(output_pd, axis=0)\n",
    "\n",
    "    # update weights\n",
    "    hidden_weights += - alpha * total_hidden_gradient\n",
    "    output_weights += - alpha * total_output_gradient\n",
    "\n",
    "# print the final outputs of the neural network on the inputs X\n",
    "print(\"Output After Training: \\n{}\".format(output_layer_outputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This process is very slow, requiring thousands of forward and back-propagation steps for each training element. This is also the reason for why neural networks have remained in obscurity for so long. Computers were simply not powerful enough to deal with millions of high-dimensional inputs - a 640x480 image has dimension $m=307,200$ - and large networks with tens of thousand of weights. It is not only computers that have become faster, but the community has also developed additional improvements to the backpropagation algorithm and how to better break the learning problem apart, resulting in a number of breakthroughs in a variety of fields that deep learning has become famous for."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From single outputs to representing higher dimensional data\n",
    "\n",
    "Extending a neural network from one single output to multiple binary classifiers is straightforward, requiring only to increase the dimensionality of the output vector. How to represent numerical values, such as digits from 0-9 or characters from A-Z? A very common approach is known as <b>One-Hot Encoding (OHE)</b>. In OHE, $n$ discrete labels such as numbers or characters will be encoded as a binary vector of length $n$. To encode the $i-th$ element of a set of lables, this vector is zero except at position $i$. For example, to encode the characters 0..9, OHE would result into\n",
    "\n",
    "$$ 0 = (1,0,0,0,0,0,0,0,0,0)$$\n",
    "$$ 1 = (0,1,0,0,0,0,0,0,0,0)$$\n",
    "$$ 2 = (0,0,1,0,0,0,0,0,0,0)$$\n",
    "$$ 3 = (0,0,0,1,0,0,0,0,0,0)$$\n",
    "$$ 4 = (0,0,0,0,1,0,0,0,0,0)$$\n",
    "$$ 5 = (0,0,0,0,0,1,0,0,0,0)$$\n",
    "$$ 6 = (0,0,0,0,0,0,1,0,0,0)$$\n",
    "$$ 7 = (0,0,0,0,0,0,0,1,0,0)$$\n",
    "$$ 8 = (0,0,0,0,0,0,0,0,1,0)$$\n",
    "$$ 9 = (0,0,0,0,0,0,0,0,0,1).$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whereas One-Hot Encoding transforms the training input into a discrete probability distribution, nothing in the neural network will ensure that the data will also comes out like that. A sigmoidal activation function would insure that each value remains between 0 and 1, but a ReLU does not. We therefore need a final layer that ensures each output to be limited to the range 0 to 1 <i>and</i> the sum of all elements to be adding up to one. This is usually achieved using a so-called <b>Softmax</b> layer. The softmax function is given by\n",
    "\n",
    "$${\\sigma (\\mathbf {z} )_{j}={\\frac {e^{z_{j}}}{\\sum _{k=1}^{K}e^{z_{k}}}}} \\quad for \\quad j=1,\\ldots,K$$\n",
    "\n",
    "That is, a vector $z \\in \\mathbb{R}^K$ will be turned into a K-dimensional vector, which j-th element is given by the above formula."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, why not just normalizing with the actual values, i.e. using $z_j$ instead of $e^{z_j}$, or even easier, just using $\\arg \\max_j$ function to set the highest value of $z$ to 1 and leave the rest at zero? The reason is that each layer needs to remain differentiable for backpropagation to work. Yet, a brutal cut-off like the $\\arg \\max$ function would introduce is what we actually really want for the network to optimally match the training input. This is why the exponential function is used. It - literally - exponentially emphasizes larger values over smaller values, making the class with the highest probability stand out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "20.00107702976334"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "y=np.random.random(10)\n",
    "sigma=np.exp(y)/np.sum(np.exp(y))\n",
    "\n",
    "plt.bar(range(10),y)\n",
    "plt.bar(range(10),sigma)\n",
    "plt.legend(['Y','Sigma'])\n",
    "plt.show()\n",
    "\n",
    "np.sum(np.exp(y))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observations and Exercises\n",
    "\n",
    "- The sigmoid function has the computationally attractive property that its derivative does not require to recompute the fraction involving the exponent, but simply $\\sigma'(x)=\\sigma(x)(1-\\sigma(x))$. <i>Can you find where the derivative of the sigmoid function is being used in the code?</i>\n",
    "\n",
    "- Increasing the learning rate leads to better fits with lesser epochs. <i>Experiment with the learning rate. What do you observe when the learning rate gets too high? What happens if it gets too slow? Revisit the simple Perceptron, what is the difference here?</i>\n",
    "\n",
    "- An iteration of the algorithm above presents the entire training set to the learning algorithm. This is also known as an <i>epoch</i>. <i>Compute the total number of forward and back propagations as a function of the number of epochs and the size of the training set.</i>\n",
    "\n",
    "- Adding more data can sometimes help to decrease the number of epochs, but whether this works depends on the kind of data provided as well as the architecture of the neural network. <i>Experiment with increasing the size of the training set. Are you able to improve prediction accuracy with lesser number of epochs for the XOR function?</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Literature\n",
    "\n",
    "Blum, A., & Rivest, R. L. (1989). Training a 3-node neural network is NP-complete. In Advances in neural information processing systems (pp. 494-501).\n",
    "\n",
    "Rumelhart, D. E., Hinton, G. E., & Williams, R. J. (1985). Learning internal representations by error propagation (No. ICS-8506). California Univ San Diego La Jolla Inst for Cognitive Science.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
