{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Deep RL network to play pygame \"Catcher\"\n",
    "\n",
    "The code below implements the learning algorithm based on the class game_wrapper defined in game_wrapper.py. Run it stand-alone, to see how the game works. In a nutshell, the game has been tuned to move the fruit by one step for every move the paddle makes. Actions are left, still, right, and the rewards at every time step are -1 (lost), 0 (nothing), or +1 (hit paddle).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wrapped_game'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-fc74eb958ea3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mwrapped_game\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'wrapped_game'"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# from https://github.com/PacktPublishing/Deep-Learning-with-Keras/tree/master/Chapter08\n",
    "from __future__ import division, print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import sklearn\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Activation, Dense, Flatten\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.optimizers import Adam\n",
    "#from scipy.misc import imresize\n",
    "from skimage.transform import resize as imresize\n",
    "import collections\n",
    "\n",
    "import os\n",
    "\n",
    "import wrapped_game\n",
    "\n",
    "\n",
    "# initialize parameters\n",
    "DATA_DIR = \"data\"\n",
    "NUM_ACTIONS = 3 # number of valid actions (left, stay, right)\n",
    "GAMMA = 0.999 # decay rate of past observations\n",
    "INITIAL_EPSILON = 1 # starting value of epsilon\n",
    "FINAL_EPSILON = 0.1 # final value of epsilon\n",
    "MEMORY_SIZE = 50000 # number of previous transitions to remember\n",
    "NUM_EPOCHS_OBSERVE = 100\n",
    "NUM_EPOCHS_TRAIN = 1000\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = NUM_EPOCHS_OBSERVE + NUM_EPOCHS_TRAIN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "The state of the game is given by the last four images. The function below returns s_t, resizing the images to 80x80, normalizing them, and ensuring four images are returned. This is important as the first three steps of the game do not provide a history of four images. For those, the function simply replicates the same image four times.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_images(images):\n",
    "    if images.shape[0] < 4:\n",
    "        # single image\n",
    "        x_t = images[0]\n",
    "        x_t = imresize(x_t, (80, 80))\n",
    "        x_t = x_t.astype(\"float\")\n",
    "        x_t /= 255.0\n",
    "        s_t = np.stack((x_t, x_t, x_t, x_t), axis=2)\n",
    "    else:\n",
    "        # 4 images\n",
    "        xt_list = []\n",
    "        for i in range(images.shape[0]):\n",
    "            x_t = imresize(images[i], (80, 80))\n",
    "            x_t = x_t.astype(\"float\")\n",
    "            x_t /= 255.0\n",
    "            xt_list.append(x_t)\n",
    "        s_t = np.stack((xt_list[0], xt_list[1], xt_list[2], xt_list[3]), \n",
    "                       axis=2)\n",
    "    s_t = np.expand_dims(s_t, axis=0)\n",
    "    return s_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "This is the core of the experience replay RL algorithm. A batch consists of batch_size state and Q-value pairs (one Q-value per action). These pairs are randomly sampled from all previous experiences.\n",
    "\n",
    "The state X is a tensor of dimension (batch_size, 80, 80, 4) as every state considers the last four iamges. The training values Y are a tensor of dimension (batch_size, 3), a Q-value for each action.\n",
    "\n",
    "These Q-values are computed by doing a prediction step using the machine learning model. Note, that two predictions are made: One to compute the Q-values for the current state s_t, the other for the next state s_tp1 that the action a_t has led to. Finally, the algorithm computes the discounted reward based on the actual reward r_t and the discounted reward of the next state. In case the game is done (game_over), there is no future step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_next_batch(experience, model, num_actions, gamma, batch_size):\n",
    "    batch_indices = np.random.randint(low=0, high=len(experience),\n",
    "                                      size=batch_size)\n",
    "    batch = [experience[i] for i in batch_indices]\n",
    "    X = np.zeros((batch_size, 80, 80, 4))\n",
    "    Y = np.zeros((batch_size, num_actions))\n",
    "    for i in range(len(batch)):\n",
    "        s_t, a_t, r_t, s_tp1, game_over = batch[i]\n",
    "        X[i] = s_t\n",
    "        Y[i] = model.predict(s_t)[0]\n",
    "        Q_sa = np.max(model.predict(s_tp1)[0])\n",
    "        if game_over:\n",
    "            Y[i, a_t] = Y[i, a_t]*0.1+r_t*0.9\n",
    "        else:\n",
    "            Y[i, a_t] = Y[i, a_t]*0.1+(r_t + gamma * Q_sa)*0.9\n",
    "    return X, Y\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The neural network encoding the Q-table is straightforward. A (80,80,4) tensor (the last four images) serves as an input, resulting in three outputs with ReLu activation, resulting into a regression network. No max-pooling layer is used in order to maintain the actual position of the fruit and paddle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the model\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=8, strides=4, \n",
    "                 kernel_initializer=\"normal\", \n",
    "                 padding=\"same\",\n",
    "                 input_shape=(80, 80, 4)))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(Conv2D(64, kernel_size=4, strides=2, \n",
    "                 kernel_initializer=\"normal\", \n",
    "                 padding=\"same\"))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(Conv2D(64, kernel_size=3, strides=1,\n",
    "                 kernel_initializer=\"normal\",\n",
    "                 padding=\"same\"))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512, kernel_initializer=\"normal\"))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(Dense(3, kernel_initializer=\"normal\"))\n",
    "\n",
    "model.compile(optimizer=Adam(lr=1e-6), loss=\"mse\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Previous games are stored in a Python collection deque of length MEMORY_SIZE. Learning is divided into two steps: an observation phase for the first NUM_EPOCHS_OBSERVE and a training phase.\n",
    "\n",
    "The key function to run an iteration of the game is game.step(action). It returns up to four of the last frames in the vector x_t, the reward r_0 and whether the game is over. preprocess_images(x_t) then takes care of padding the images to be at least four as well as normalizing them.\n",
    "\n",
    "The algorithm calls this function over and over again, selecting a random action for the first NUM_EPOCHS_OBSERVE games, and choosing the best one with probability $1-\\epsilon$ otherwise.\n",
    "\n",
    "In order to find the best action, the neural network performs a prediction step, and the action that is chosen is the one with the highest q-value.\n",
    "\n",
    "The experience is stored by pushing back the previous state (s_tm1), whatever action has been taken, the reward that resulted, the new state s_t and whether the game is over.\n",
    "\n",
    "experience.append((s_tm1, a_t, r_t, s_t, game_over))\n",
    "\n",
    "This experience is then used to incrementally train the neural network model. Finally, the learning algorithm decreases epsilon by a tiny amount.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train network\n",
    "game = wrapped_game.MyWrappedGame()\n",
    "experience = collections.deque(maxlen=MEMORY_SIZE)\n",
    "\n",
    "fout = open(os.path.join(DATA_DIR, \"rl-network-results.tsv\"), \"w\")\n",
    "num_games, num_wins = 0, 0\n",
    "epsilon = INITIAL_EPSILON\n",
    "for e in range(NUM_EPOCHS):\n",
    "    loss = 0.0\n",
    "    game.reset()\n",
    "    \n",
    "    # get first state\n",
    "    a_0 = 1  # (0 = left, 1 = stay, 2 = right)\n",
    "    x_t, r_0, game_over = game.step(a_0) \n",
    "    s_t = preprocess_images(x_t)\n",
    "\n",
    "    while not game_over:\n",
    "        s_tm1 = s_t\n",
    "        # next action\n",
    "        if e <= NUM_EPOCHS_OBSERVE:\n",
    "            a_t = np.random.randint(low=0, high=NUM_ACTIONS, size=1)[0]\n",
    "        else:\n",
    "            if np.random.rand() <= epsilon:\n",
    "                a_t = np.random.randint(low=0, high=NUM_ACTIONS, size=1)[0]\n",
    "            else:\n",
    "                q = model.predict(s_t)[0]\n",
    "                a_t = np.argmax(q)\n",
    "                \n",
    "        # apply action, get reward\n",
    "        x_t, r_t, game_over = game.step(a_t)\n",
    "        s_t = preprocess_images(x_t)\n",
    "        # if reward, increment num_wins\n",
    "        if r_t == 1:\n",
    "            num_wins += 1\n",
    "        # store experience\n",
    "        experience.append((s_tm1, a_t, r_t, s_t, game_over))\n",
    "        \n",
    "        if e > NUM_EPOCHS_OBSERVE:\n",
    "            # finished observing, now start training\n",
    "            # get next batch\n",
    "            X, Y = get_next_batch(experience, model, NUM_ACTIONS, \n",
    "                                  GAMMA, BATCH_SIZE)\n",
    "            loss += model.train_on_batch(X, Y)\n",
    "        \n",
    "    # reduce epsilon gradually\n",
    "    if epsilon > FINAL_EPSILON:\n",
    "        epsilon -= (INITIAL_EPSILON - FINAL_EPSILON) / NUM_EPOCHS\n",
    "        \n",
    "    print(\"Epoch {:04d}/{:d} | Loss {:.5f} | Win Count: {:d}\"\n",
    "          .format(e + 1, NUM_EPOCHS, loss, num_wins))\n",
    "    fout.write(\"{:04d}\\t{:.5f}\\t{:d}\\n\"\n",
    "          .format(e + 1, loss, num_wins))\n",
    "\n",
    "    if e % 100 == 0:\n",
    "        model.save(os.path.join(DATA_DIR, \"rl-network.h5\"), overwrite=True)\n",
    "        \n",
    "fout.close()\n",
    "model.save(os.path.join(DATA_DIR, \"rl-network-2100.h5\"), overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.load(\"../data/game-screenshots.npy\")\n",
    "print(X.shape)\n",
    "\n",
    "sidx = 140\n",
    "for soff in range(4):\n",
    "    plt.subplot(sidx + soff + 1)\n",
    "    plt.imshow(X[soff].T, cmap=\"gray\")\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
