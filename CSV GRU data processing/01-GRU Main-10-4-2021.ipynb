{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we'll be using a GRU model for a time series prediction task and we will compare the performance of the GRU model against an LSTM model as well. The dataset that we will be using is the Hourly Energy Consumption dataset which can be found on [Kaggle](https://www.kaggle.com/robikscube/hourly-energy-consumption). The dataset contains power consumption data across different regions around the United States recorded on an hourly basis.\n",
    "\n",
    "You can run the code implementation in this article on FloydHub using their GPUs on the cloud by clicking the following link and using the main.ipynb notebook.\n",
    "\n",
    "[![Run on FloydHub](https://static.floydhub.com/button/button-small.svg)](https://floydhub.com/run?template=https://github.com/gabrielloye/https://github.com/gabrielloye/GRU_Prediction)\n",
    "\n",
    "This will speed up the training process significantly. Alternatively, the link to the GitHub repository can be found [here]().\n",
    "\n",
    "The goal of this implementation is to create a model that can accurately predict the energy usage in the next hour given historical usage data. We will be using both the GRU and LSTM model to train on a set of historical data and evaluate both models on an unseen test set. To do so, weâ€™ll start with feature selection, data-preprocessing, followed by defining, training and eventually evaluating the models.\n",
    "\n",
    "We will be using the PyTorch library to implement both types of models along with other common Python libraries used in data analytics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.python-engineer.com/posts/pytorch-rnn-lstm-gru/\n",
    "\n",
    "#https://blog.floydhub.com/gru-with-pytorch/\n",
    "\n",
    "import os\n",
    "import time\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import random\n",
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from tqdm import tqdm_notebook\n",
    "from sklearn.preprocessing import MinMaxScaler,QuantileTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Local ###\n",
    "#from data_processing import *\n",
    "\n",
    "\n",
    "\n",
    "# Define data root directory\n",
    "\n",
    "#data_dir = \"./data/\"\n",
    "#print(os.listdir(data_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a total of **12** *.csv* files containing hourly energy trend data (*'est_hourly.paruqet'* and *'pjm_hourly_est.csv'* are not used). In our next step, we will be reading these files and pre-processing these data in this order:\n",
    "- Getting the time data of each individual time step and generalizing them\n",
    "    - Hour of the day *i.e. 0-23*\n",
    "    - Day of the week *i.e. 1-7*\n",
    "    - Month *i.e. 1-12*\n",
    "    - Day of the year *i.e. 1-365*\n",
    "    \n",
    "    \n",
    "- Scale the data to values between 0 and 1\n",
    "    - Algorithms tend to perform better or converge faster when features are on a relatively similar scale and/or close to normally distributed\n",
    "    - Scaling preserves the shape of the original distribution and doesn't reduce the importance of outliers.\n",
    "    \n",
    "    \n",
    "- Group the data into sequences to be used as inputs to the model and store their corresponding labels\n",
    "    - The **sequence length** or **lookback period** is the number of data points in history that the model will use to make the prediction\n",
    "    - The label will be the next data point in time after the last one in the input sequence\n",
    "    \n",
    "\n",
    "- The inputs and labels will then be split into training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "794.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "4764/6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4764, 10)\n",
      "total runs: 794\n"
     ]
    }
   ],
   "source": [
    "choppeddata=pd.read_csv('choppeddata_10_04_2021.csv')#.head()\n",
    "print(choppeddata.shape)\n",
    "runqty=int(choppeddata.shape[0]/6)\n",
    "print(\"total runs:\",runqty)\n",
    "choppedheaders=[]\n",
    "lookback=10 #save only the last 11 timesteps\n",
    "for i in range(lookback):  \n",
    "    label=str(i)\n",
    "    choppedheaders.append(\"header\"+label)\n",
    "\n",
    "#put chopped data in np.arrays\n",
    "State=np.zeros((runqty,5,lookback)) #96 runs,with 5 sets of data (x,y,z,roll,pitch) each, and each run is 11 timesteps long\n",
    "Labels=np.zeros((runqty,lookback)) #96 runs, each run is 11 timesteps long\n",
    "runcounter=0\n",
    "\n",
    "for i in range(0,choppeddata.shape[0],6):\n",
    "            State[runcounter][0][:]=(choppeddata[choppedheaders[:]].iloc[i]).tolist()\n",
    "            State[runcounter][1][:]=(choppeddata[choppedheaders[:]].iloc[i+1]).tolist()\n",
    "            State[runcounter][2][:]=(choppeddata[choppedheaders[:]].iloc[i+2]).tolist()\n",
    "            State[runcounter][3][:]=(choppeddata[choppedheaders[:]].iloc[i+3]).tolist()\n",
    "            State[runcounter][4][:]=(choppeddata[choppedheaders[:]].iloc[i+4]).tolist()\n",
    "            Labels[runcounter][:]=(choppeddata[choppedheaders[:]].iloc[i+5]).tolist()  #labels   \n",
    "            runcounter+=1\n",
    "#print(State[0])\n",
    "#print(Labels)\n",
    "#print(Labels[:,9]) #just getting finals labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  1  2]\n",
      " [ 3  4  5]\n",
      " [ 6  7  8]\n",
      " [ 9 10 11]]\n",
      "(4, 3)\n",
      "[ 2  5  8 11]\n"
     ]
    }
   ],
   "source": [
    "j = np.array([[ 0,  1,  2],\n",
    "              [ 3,  4,  5],\n",
    "              [ 6,  7,  8],\n",
    "              [ 9, 10, 11]])\n",
    "print(j)\n",
    "print(j.shape)\n",
    "print(j[:,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(794, 5, 10)\n",
      "Train\n",
      "[[ 0.1919868   0.20295908  0.1708541   0.22038047  0.25170856  0.12045436\n",
      "   0.17892917  0.12992046  0.21222982  0.28112493]\n",
      " [-0.99498577 -1.05777836 -1.12205553 -1.09031563 -1.11248541 -1.17982159\n",
      "  -1.11762962 -1.24154463 -1.13633394 -0.87064657]\n",
      " [ 0.01764448  0.02640724 -0.00905184  0.0170208  -0.07141802  0.03502758\n",
      "   0.0271122   0.11102508 -0.0237645  -1.42699265]\n",
      " [ 0.76305747  0.81754833  0.83689249  0.8886317   0.87156457  0.93642718\n",
      "   0.89742798  0.99148405  0.90944266  0.77985954]\n",
      " [ 0.03847182  0.03385914  0.00550103  0.03519293  0.05680903 -0.03655296\n",
      "   0.00945988 -0.05662262 -0.00407364  0.05035823]]\n",
      "[1.]\n",
      "Train set Y size 595\n",
      "Test\n",
      "[[ 0.30313077 -0.08898389 -0.25620024 -0.41666164 -0.5776041  -0.7606174\n",
      "  -1.02097349 -1.24527903 -1.17246895 -1.14011889]\n",
      " [ 0.9641715   0.92918768  0.96482868  1.00091305  0.99974194  1.01031189\n",
      "   0.98427372  0.99107952  0.97402077  1.01232777]\n",
      " [ 0.17576999  0.26262693  0.30154294  0.26582735  0.19916645  0.24801437\n",
      "   0.27951944  0.25736245  0.26295279 -0.41261505]\n",
      " [-0.95187587 -0.95508939 -0.9678942  -1.01624048 -1.03586912 -1.04103744\n",
      "  -1.04799664 -1.06210899 -1.06351984 -1.08522618]\n",
      " [ 0.5642761   0.10997262 -0.10376535 -0.27373037 -0.50099504 -0.71945179\n",
      "  -1.04823148 -1.34092391 -1.33350432 -1.30410957]]\n",
      "[1.]\n",
      "Test set Y size 199\n"
     ]
    }
   ],
   "source": [
    "#X= range(0,575,6)\n",
    "#y= range(0,575,6)\n",
    "\n",
    "X=State\n",
    "y=Labels[:,lookback-1]\n",
    "print(X.shape)\n",
    "\n",
    "y=y.reshape(runqty,1)\n",
    "\n",
    "random_seed=int(time.time())\n",
    "#print(int(time.time()))\n",
    "train_x, test_x, train_y,test_y = train_test_split(X, y, test_size=.25, #0.33, \n",
    "                                                   random_state=random_seed)\n",
    "print(\"Train\")\n",
    "print(train_x[0])\n",
    "print(train_y[0])\n",
    "print(\"Train set Y size\", train_y.size)\n",
    "print(\"Test\")\n",
    "print(test_x[0])\n",
    "print(test_y[0])\n",
    "print(\"Test set Y size\", test_y.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(72, 5, 11)\n"
     ]
    }
   ],
   "source": [
    "print(train_x.shape)  #example was (980185, 90, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a total of 980,185 sequences of training data\n",
    "\n",
    "To improve the speed of our training, we can process the data in batches so that the model does not need to update its weights as frequently. The Torch *Dataset* and *DataLoader* classes are useful for splitting our data into batches and shuffling them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "#a batch size of 64, i.e. each element in the dataloader iterable will return a batch of 64 features and labels.\n",
    "\n",
    "train_data = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size, drop_last=True)\n",
    "\n",
    "test_data   = TensorDataset( torch.from_numpy( test_x ), torch.from_numpy( test_y ) )\n",
    "test_loader = DataLoader( test_data, shuffle = True, batch_size = batch_size, drop_last = True )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataloader.DataLoader object at 0x7f40bdaf3350>\n"
     ]
    }
   ],
   "source": [
    "print(train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also check if we have any GPUs to speed up our training time by many folds. If youâ€™re using FloydHub with GPU to run this code, the training time will be significantly reduced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# torch.cuda.is_available() checks and returns a Boolean True if a GPU is available, else it'll return False\n",
    "is_cuda = torch.cuda.is_available()\n",
    "\n",
    "# If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.\n",
    "if is_cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    \n",
    "print(device)\n",
    "\n",
    "\n",
    "def get_torch_device( v=0 ):\n",
    "    # torch.cuda.is_available() checks and returns a Boolean True if a GPU is available, else it'll return False\n",
    "    is_cuda = torch.cuda.is_available()\n",
    "    # If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.\n",
    "    if is_cuda:\n",
    "        device = torch.device(\"cuda\")\n",
    "        if v:  print( \"CUDA Available!\" )\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        if v:  print( \"NO CUDA\" )\n",
    "    return device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUNet(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, n_layers, drop_prob=0.2):\n",
    "        super(GRUNet, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.gru = nn.GRU(input_dim, hidden_dim, n_layers, batch_first=True, dropout=drop_prob)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x, h):\n",
    "        out, h = self.gru(x, h)\n",
    "        out = self.fc(self.relu(out[:,-1]))\n",
    "        return out, h\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        hidden = weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device)\n",
    "        return hidden\n",
    "def train(train_loader, learn_rate, hidden_dim=128, EPOCHS=500, model_type=\"GRU\"):\n",
    "    losslist=[]\n",
    "    # Setting common hyperparameters\n",
    "    input_dim = next(iter(train_loader))[0].shape[2]  #  = 11\n",
    "    #print(input_dim)\n",
    "    #print(\"input_dim\",input_dim)\n",
    "    output_dim = 1\n",
    "    n_layers = 2\n",
    "    # Instantiating the models\n",
    "    if model_type == \"GRU\":\n",
    "        model = GRUNet(input_dim, hidden_dim, output_dim, n_layers)\n",
    "    else:\n",
    "        model = LSTMNet(input_dim, hidden_dim, output_dim, n_layers)\n",
    "    model.to(device)\n",
    "    \n",
    "    # Defining loss function and optimizer\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learn_rate)\n",
    "    \n",
    "    model.train()\n",
    "    print(\"Starting Training of {} model\".format(model_type))\n",
    "    epoch_times = []\n",
    "    # Start training loop\n",
    "    for epoch in range(1,EPOCHS+1):\n",
    "        start_time = time.clock()\n",
    "        h = model.init_hidden(batch_size)\n",
    "        avg_loss = 0.\n",
    "        counter = 0\n",
    "        for x, label in train_loader:\n",
    "            #print(\"x\",x)\n",
    "            #print(\"label\",label)\n",
    "            counter += 1\n",
    "            if model_type == \"GRU\":\n",
    "                h = h.data\n",
    "            else:\n",
    "                h = tuple([e.data for e in h])\n",
    "            model.zero_grad()\n",
    "            \n",
    "            out, h = model(x.to(device).float(), h)\n",
    "            #print(\"out\",out)\n",
    "            loss = criterion(out, label.to(device).float())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            avg_loss += loss.item()\n",
    "            \n",
    "            if counter%20000 == 0:\n",
    "                print(\"Epoch {}......Step: {}/{}....... Average Loss for Epoch: {}\".format(epoch, counter, len(train_loader), avg_loss/counter))\n",
    "        current_time = time.clock()\n",
    "        if epoch%20 == 0:\n",
    "            print(\"Epoch {}/{} Done, Total Loss: {}\".format(epoch, EPOCHS, avg_loss/len(train_loader)))\n",
    "        \n",
    "            print(\"Total Time Elapsed: {} seconds\".format(str(current_time-start_time)))\n",
    "        losslist.append(avg_loss/len(train_loader))\n",
    "        epoch_times.append(current_time-start_time)\n",
    "    print(\"Total Training Time: {} seconds\".format(str(sum(epoch_times))))\n",
    "    plt.plot(losslist)\n",
    "    plt.title(\"Loss\")\n",
    "    plt.show()\n",
    "    return model\n",
    "\n",
    "def evaluate(model, test_x, test_y):\n",
    "    model.eval()\n",
    "    outputs = []\n",
    "    targets = []\n",
    "    start_time = time.clock()\n",
    "    #for i in test_x.keys():\n",
    "    for i in range( len( test_x ) ):    \n",
    "        inp = torch.from_numpy(np.array(test_x[i])) # should be 5x1\n",
    "        labs = torch.from_numpy(np.array(test_y[i])) #should be 1x1\n",
    "        h = model.init_hidden(inp.shape[0])\n",
    "        #print(\"inp\",inp)\n",
    "        #print(\"labs\",labs)\n",
    "        #print(\"h\",h)\n",
    "        out, h = model(inp.to(device).float(), h)\n",
    "        #outputs.append(label_scalers[i].inverse_transform(out.cpu().detach().numpy()).reshape(-1))\n",
    "        #targets.append(label_scalers[i].inverse_transform(labs.numpy()).reshape(-1))\n",
    "        outputs.append( out.cpu().detach().numpy().reshape(-1) )\n",
    "        targets.append( labs.numpy().reshape(-1) )\n",
    "        \n",
    "    print(\"Evaluation Time: {}\".format(str(time.clock()-start_time)))\n",
    "    sMAPE = 0\n",
    "    for i in range(len(outputs)):\n",
    "        sMAPE += np.mean(abs(outputs[i]-targets[i])/(targets[i]+outputs[i])/2)/len(outputs)\n",
    "    print(\"sMAPE: {}%\".format(sMAPE*100))\n",
    "    return outputs, targets, sMAPE\n",
    "                               \n",
    "def evaluate2(model, test_x, test_y):\n",
    "    model.eval()\n",
    "    outputs = []\n",
    "    targets = []\n",
    "    start_time = time.clock()\n",
    "    #for i in test_x.keys():\n",
    "    #for i in range( len( test_x ) ):    \n",
    "    inp = torch.from_numpy(np.array(test_x)) # should be 5x1\n",
    "    labs = torch.from_numpy(np.array(test_y)) #should be 1x1\n",
    "    h = model.init_hidden(inp.shape[0])\n",
    "    #print(\"inp\",inp)\n",
    "    #print(\"labs\",labs)\n",
    "    #print(\"h\",h)\n",
    "    out, h = model(inp.to(device).float(), h)\n",
    "    #outputs.append(label_scalers[i].inverse_transform(out.cpu().detach().numpy()).reshape(-1))\n",
    "    #targets.append(label_scalers[i].inverse_transform(labs.numpy()).reshape(-1))\n",
    "    outputs.append( out.cpu().detach().numpy().reshape(-1) )\n",
    "    targets.append( labs.numpy().reshape(-1) )\n",
    "        \n",
    "    print(\"Evaluation Time: {}\".format(str(time.clock()-start_time)))\n",
    "    sMAPE = 0\n",
    "    for i in range(len(outputs)):\n",
    "        sMAPE += np.mean(abs(outputs[i]-targets[i])/(targets[i]+outputs[i])/2)/len(outputs)\n",
    "    print(\"sMAPE: {}%\".format(sMAPE*100))\n",
    "    return outputs, targets, sMAPE                               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training of GRU model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/scott/anaconda3/envs/thesis/lib/python3.7/site-packages/ipykernel_launcher.py:44: DeprecationWarning: time.clock has been deprecated in Python 3.3 and will be removed from Python 3.8: use time.perf_counter or time.process_time instead\n",
      "/home/scott/anaconda3/envs/thesis/lib/python3.7/site-packages/ipykernel_launcher.py:67: DeprecationWarning: time.clock has been deprecated in Python 3.3 and will be removed from Python 3.8: use time.perf_counter or time.process_time instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/500 Done, Total Loss: 0.052156878056357034\n",
      "Total Time Elapsed: 4.5010560000000055 seconds\n",
      "Epoch 40/500 Done, Total Loss: 0.045948860884449015\n",
      "Total Time Elapsed: 8.533313000000021 seconds\n",
      "Epoch 60/500 Done, Total Loss: 0.034757563738133504\n",
      "Total Time Elapsed: 6.065253999999982 seconds\n",
      "Epoch 80/500 Done, Total Loss: 0.02867205695499276\n",
      "Total Time Elapsed: 3.6188829999999825 seconds\n",
      "Epoch 100/500 Done, Total Loss: 0.02252964684196962\n",
      "Total Time Elapsed: 6.674994999999967 seconds\n",
      "Epoch 120/500 Done, Total Loss: 0.01740498730115989\n",
      "Total Time Elapsed: 5.386986999999976 seconds\n",
      "Epoch 140/500 Done, Total Loss: 0.012372965126937866\n",
      "Total Time Elapsed: 3.719524999999976 seconds\n",
      "Epoch 160/500 Done, Total Loss: 0.015492703600446819\n",
      "Total Time Elapsed: 3.8867630000000872 seconds\n",
      "Epoch 180/500 Done, Total Loss: 0.01007566292006428\n",
      "Total Time Elapsed: 3.5821529999999484 seconds\n",
      "Epoch 200/500 Done, Total Loss: 0.007304394226200718\n",
      "Total Time Elapsed: 3.6032559999999876 seconds\n",
      "Epoch 220/500 Done, Total Loss: 0.011160681737090124\n",
      "Total Time Elapsed: 3.62504100000001 seconds\n",
      "Epoch 240/500 Done, Total Loss: 0.013605853060112536\n",
      "Total Time Elapsed: 3.9870370000001003 seconds\n",
      "Epoch 260/500 Done, Total Loss: 0.005883929553327697\n",
      "Total Time Elapsed: 3.915821000000051 seconds\n",
      "Epoch 280/500 Done, Total Loss: 0.018190903806523263\n",
      "Total Time Elapsed: 3.834372999999914 seconds\n",
      "Epoch 300/500 Done, Total Loss: 0.004975292562375142\n",
      "Total Time Elapsed: 3.4131069999998545 seconds\n",
      "Epoch 320/500 Done, Total Loss: 0.004229946091269281\n",
      "Total Time Elapsed: 3.656592000000046 seconds\n",
      "Epoch 340/500 Done, Total Loss: 0.004269744224960025\n",
      "Total Time Elapsed: 3.7629210000000057 seconds\n",
      "Epoch 360/500 Done, Total Loss: 0.0048158662986581375\n",
      "Total Time Elapsed: 3.325715000000173 seconds\n",
      "Epoch 380/500 Done, Total Loss: 0.005638410225671848\n",
      "Total Time Elapsed: 3.7477570000000924 seconds\n",
      "Epoch 400/500 Done, Total Loss: 0.005028208194916432\n",
      "Total Time Elapsed: 3.5251169999999092 seconds\n",
      "Epoch 420/500 Done, Total Loss: 0.005232158241130025\n",
      "Total Time Elapsed: 3.521327999999812 seconds\n",
      "Epoch 440/500 Done, Total Loss: 0.0024761810848734035\n",
      "Total Time Elapsed: 4.240105999999969 seconds\n",
      "Epoch 460/500 Done, Total Loss: 0.00390159488882558\n",
      "Total Time Elapsed: 4.230998 seconds\n",
      "Epoch 480/500 Done, Total Loss: 0.002027947889104866\n",
      "Total Time Elapsed: 4.13092400000005 seconds\n",
      "Epoch 500/500 Done, Total Loss: 0.009060308792413707\n",
      "Total Time Elapsed: 3.68797799999993 seconds\n",
      "Total Training Time: 1991.1211600000004 seconds\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "lr = 0.001#0.001\n",
    "gru_model = train(train_loader, lr, model_type=\"GRU\")\n",
    "\n",
    "#The target size means the label size!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: unspecified launch failure",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_81968/3312255129.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mlabs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#h = model.init_hidden(inp.shape[0])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_hidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;31m#print(\"inp\",inp)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_81968/2830187747.py\u001b[0m in \u001b[0;36minit_hidden\u001b[0;34m(self, batch_size)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minit_hidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mweight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_layers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearn_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEPOCHS\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"GRU\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: unspecified launch failure"
     ]
    }
   ],
   "source": [
    "model=gru_model\n",
    "i=1\n",
    "inp = torch.from_numpy(np.array(test_x))\n",
    "labs = torch.from_numpy(np.array(test_y))\n",
    "#h = model.init_hidden(inp.shape[0])\n",
    "h = model.init_hidden(inp.shape[0])\n",
    "#print(\"inp\",inp)\n",
    "\n",
    "#print(\"INP SHAPE\",inp.shape)\n",
    "#print(\"INP SHAPE[0]\",inp.shape[0])\n",
    "#print(\"labs\",labs)\n",
    "#print(\"h\",h)\n",
    "#print(\"h.shape\",h.shape)\n",
    "#print(inp.to(device).float())\n",
    "#print(inp.to(device).float().shape)\n",
    "print(inp.to(device).float().shape)\n",
    "\n",
    "out, h = model(inp.to(device).float(), h)\n",
    "#print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Time: 0.05078300000013769\n",
      "sMAPE: 4.989344605758979%\n",
      "[array([ 1.00534678e+00,  1.00077581e+00,  1.00392318e+00,  9.85081673e-01,\n",
      "        9.83425736e-01,  9.99026775e-01,  9.88190174e-01,  9.98457432e-01,\n",
      "        1.09058249e+00,  1.01287079e+00,  9.86622214e-01,  1.00459409e+00,\n",
      "        1.66290402e-01,  9.85718131e-01,  9.85605121e-01,  9.97464776e-01,\n",
      "        1.00186193e+00,  9.86945033e-01,  9.85866666e-01,  1.00437474e+00,\n",
      "        9.66140866e-01,  1.00762713e+00,  1.02153862e+00,  9.86978531e-01,\n",
      "        9.86677170e-01,  9.96387601e-01,  1.01089346e+00,  9.86069083e-01,\n",
      "        9.91702914e-01,  9.87051845e-01,  1.53060466e-01,  1.00645769e+00,\n",
      "        1.00749195e+00,  9.83780384e-01,  1.09024927e-01,  8.21739435e-02,\n",
      "        9.88014579e-01,  9.84740853e-01,  9.88767982e-01,  6.85875475e-01,\n",
      "        1.00291991e+00,  9.93662834e-01,  9.85796213e-01,  9.88445759e-01,\n",
      "        9.85255837e-01,  1.01048934e+00,  9.94968414e-01,  9.92957950e-01,\n",
      "        1.03807855e+00,  1.79242611e-01,  1.02681494e+00,  9.93275046e-01,\n",
      "        9.87203598e-01,  1.00211740e+00,  1.00167704e+00,  9.90513921e-01,\n",
      "        1.44453943e-02,  9.91711617e-01,  1.00057185e+00,  9.86167073e-01,\n",
      "        9.86701369e-01,  9.92922068e-01,  6.12124801e-03,  1.03343844e+00,\n",
      "        9.65536594e-01,  9.84199405e-01,  9.86682177e-01,  1.02583754e+00,\n",
      "        9.93235111e-01, -9.99540091e-03,  9.99040127e-01,  9.95969057e-01,\n",
      "        9.98317599e-01,  9.88907933e-01,  9.84995484e-01,  9.99417186e-01,\n",
      "        1.01824629e+00, -7.52339065e-02,  1.01237488e+00,  1.00056350e+00,\n",
      "        9.99151826e-01,  9.85546589e-01,  9.85364437e-01,  1.01365221e+00,\n",
      "        1.01627421e+00,  1.00859797e+00,  5.44983000e-02,  9.85194325e-01,\n",
      "        1.02750993e+00, -8.88084173e-02,  1.09046960e+00,  1.01367092e+00,\n",
      "        9.88282561e-01,  1.93564147e-02,  9.96979237e-01,  2.58801103e-01,\n",
      "        9.96873140e-01,  9.86950517e-01,  9.83647823e-01,  9.96743441e-01,\n",
      "        1.01076162e+00,  1.01184249e+00,  9.85077381e-01,  5.59507757e-02,\n",
      "        1.00455177e+00,  1.60815746e-01,  9.94588614e-01,  8.15429330e-01,\n",
      "        1.00193453e+00,  2.51920342e-01,  1.00350344e+00,  9.88557220e-01,\n",
      "        1.01090264e+00,  9.87007737e-01,  9.86863852e-01,  9.86792207e-01,\n",
      "        1.37280837e-01,  9.94212627e-01,  1.36420250e-01,  9.82214808e-01,\n",
      "        1.03837895e+00,  1.02564061e+00,  1.08455908e+00,  9.88267541e-01,\n",
      "        9.79808569e-01,  9.84640002e-01,  1.00837696e+00,  9.90475535e-01,\n",
      "        3.88582200e-02,  1.00477397e+00,  9.86611009e-01,  1.26602843e-01,\n",
      "        9.96778369e-01,  9.86969352e-01,  6.83317423e-01,  6.96805120e-03,\n",
      "        9.83158708e-01,  9.87174153e-01,  9.92891192e-01,  9.92774725e-01,\n",
      "        2.91374922e-02,  9.87532973e-01,  9.90619302e-01, -1.59969628e-02,\n",
      "       -6.55534565e-02,  9.93322730e-01,  1.00378501e+00,  9.87070680e-01,\n",
      "        1.00214338e+00,  1.00053453e+00,  1.07551503e+00,  9.94073629e-01,\n",
      "        1.05278647e+00,  9.86825466e-01,  9.91003752e-01,  9.87138510e-01,\n",
      "       -2.39465833e-02,  9.93965983e-01,  9.86382604e-01,  1.00822496e+00,\n",
      "        9.92389679e-01,  6.20846748e-02,  9.87324834e-01,  9.93783236e-01,\n",
      "        2.01528847e-01,  5.26120782e-01,  7.59860158e-01,  9.84842539e-01,\n",
      "        1.31386206e-01,  9.86316085e-01,  1.01301980e+00,  9.93681788e-01,\n",
      "        9.92365003e-01,  9.84957695e-01,  1.00406957e+00,  9.86082673e-01,\n",
      "        1.00633645e+00,  9.86407757e-01,  9.98879552e-01,  9.99959707e-01,\n",
      "        9.95115757e-01,  9.87434387e-01,  9.89289761e-01,  9.13360476e-01,\n",
      "        9.83731270e-01,  9.91735458e-01,  2.71594077e-02,  9.96713877e-01,\n",
      "        9.98390913e-01,  1.03134036e+00,  9.24883842e-01,  3.37195098e-02,\n",
      "        9.85649467e-01,  9.79395509e-01,  9.95430708e-01,  9.88992691e-01,\n",
      "        1.00209081e+00,  9.86988902e-01,  9.85821605e-01,  9.98427987e-01,\n",
      "        4.35537994e-02,  7.92473316e-01,  9.86177087e-01,  9.83775496e-01,\n",
      "        9.86906767e-01,  4.38493192e-02,  1.50536105e-01,  9.91940498e-01,\n",
      "        9.87151146e-01,  5.57326555e-01, -4.89812791e-02,  1.00385201e+00,\n",
      "        1.00294125e+00,  1.02165151e+00,  9.86391664e-01,  9.94822383e-01,\n",
      "        9.88675714e-01,  9.84551430e-01,  9.91665483e-01,  9.92503643e-01,\n",
      "        9.86863613e-01,  6.11720085e-02,  5.69358617e-02,  9.89660501e-01,\n",
      "        9.88498807e-01,  9.54341888e-01,  9.76950884e-01,  5.60671687e-02,\n",
      "        9.98540044e-01, -2.96947360e-02,  1.00633180e+00, -4.37868536e-02,\n",
      "        1.31716922e-01,  1.03755586e-01,  1.00535762e+00,  9.86953139e-01,\n",
      "        7.03096390e-03,  9.86929536e-01,  4.25943136e-02,  9.87067461e-01,\n",
      "        1.60281271e-01,  9.87069726e-01,  9.87255573e-01, -7.93112814e-03,\n",
      "        1.03541577e+00,  9.98376131e-01,  1.00841129e+00,  9.83473659e-01,\n",
      "        9.51806679e-02,  9.93468881e-01,  9.85959411e-01,  9.87286687e-01,\n",
      "        9.89100456e-01, -9.18363035e-03,  9.90423322e-01,  1.02237928e+00,\n",
      "        9.84063506e-01,  9.88984942e-01,  9.96777892e-01,  9.85715389e-01,\n",
      "        1.07070342e-01,  6.83518171e-01,  1.00155950e+00,  9.94900942e-01,\n",
      "        1.00592053e+00,  9.94215012e-01,  1.00055122e+00,  8.05721357e-02,\n",
      "        9.45889473e-01,  9.99352098e-01,  2.01682538e-01,  9.95156527e-01,\n",
      "        9.94562984e-01,  1.01570177e+00,  1.44208074e-02,  9.97412205e-01,\n",
      "        9.85400319e-01,  9.87141490e-01,  9.87107873e-01,  9.85549331e-01,\n",
      "        1.07621717e+00,  9.97008204e-01,  1.02349198e+00,  9.85282660e-01,\n",
      "        9.98809695e-01,  9.84339237e-01,  1.00761545e+00,  9.84116912e-01,\n",
      "        9.99211907e-01,  1.01276636e+00,  9.84981298e-01,  9.85818982e-01,\n",
      "        9.94413853e-01,  1.01056457e+00,  9.92694497e-01,  9.88318801e-01,\n",
      "        9.87587452e-01,  1.03610530e-01,  9.93809819e-01,  1.00183439e+00,\n",
      "        1.00570869e+00,  1.06149590e+00,  9.87457633e-01,  9.11956429e-01,\n",
      "        8.05392385e-01,  1.00482273e+00,  9.92239594e-01,  9.90756869e-01,\n",
      "        9.94770527e-01,  1.00894380e+00,  9.95497942e-01,  9.90709186e-01,\n",
      "        9.86557484e-01,  9.96593356e-01,  9.89966869e-01,  9.85007167e-01,\n",
      "        1.00583720e+00,  1.00167871e+00,  9.86631989e-01,  1.00174427e+00,\n",
      "        9.90075827e-01,  1.00949442e+00,  2.29272991e-02,  9.92553711e-01,\n",
      "        9.95958328e-01,  9.41020370e-01,  9.87523556e-01,  9.98839289e-02,\n",
      "        9.85673428e-01,  9.90288615e-01,  9.91639972e-01,  9.87295628e-01,\n",
      "        9.98003840e-01,  9.66513276e-01,  9.87245560e-01,  1.00029778e+00,\n",
      "        9.98626709e-01,  9.97051477e-01,  9.89181399e-01,  9.87041354e-01,\n",
      "        9.77742791e-01,  9.85725284e-01,  9.41514492e-01,  9.94640112e-01,\n",
      "        1.00437987e+00,  9.87303853e-01,  9.90045309e-01,  9.75171328e-01,\n",
      "        9.77346778e-01,  1.03405917e+00,  9.87130165e-01,  1.00328350e+00,\n",
      "        1.00090182e+00,  9.40151930e-01,  9.94172692e-01,  1.00071561e+00,\n",
      "        9.86339211e-01,  8.64226073e-02,  9.91965413e-01,  9.84325290e-01,\n",
      "        9.98862386e-01,  2.45307803e-01,  1.03336573e-01,  9.88907576e-01,\n",
      "        9.88979697e-01,  7.95677662e-01,  9.89281893e-01,  9.86946583e-01,\n",
      "        9.94374037e-01,  9.98539805e-01,  9.95205402e-01,  8.62111330e-01,\n",
      "        9.32214499e-01,  5.16560674e-02,  9.85194087e-01,  9.84019876e-01,\n",
      "        1.01285326e+00,  9.94543910e-01,  9.64692235e-01,  9.88977671e-01,\n",
      "        9.89313722e-01,  9.86199975e-01,  7.75497556e-01,  9.98429656e-01,\n",
      "        9.85862970e-01,  9.87046480e-01,  9.87001657e-01,  9.87003088e-01,\n",
      "        1.00326908e+00,  9.22480345e-01,  9.89072442e-01,  3.58234495e-02,\n",
      "        9.89110112e-01,  1.00489032e+00,  9.86141205e-01,  1.00247884e+00,\n",
      "        9.94953156e-01,  5.17509341e-01,  9.90182519e-01,  9.96748805e-01,\n",
      "        9.90882397e-01,  9.91026163e-01,  9.81008768e-01,  1.00450206e+00,\n",
      "        1.02315414e+00,  9.90895867e-01,  1.01712775e+00,  9.92982864e-01,\n",
      "        9.84941244e-01,  1.02121532e+00,  9.87773776e-01,  5.51009774e-01,\n",
      "        1.00836897e+00,  9.86885071e-01,  9.87162113e-01,  9.99221087e-01,\n",
      "        9.86530781e-01,  9.90626454e-01,  7.38136292e-01,  9.93536949e-01,\n",
      "        1.00249875e+00,  9.87105012e-01,  9.87201333e-01,  1.01960480e+00,\n",
      "        9.95446801e-01,  9.90637064e-01,  9.97161984e-01,  9.88511324e-01,\n",
      "        9.93634224e-01,  9.86050129e-01,  1.00127304e+00,  9.97511983e-01,\n",
      "        9.61573243e-01,  9.84276772e-01,  9.86429691e-01,  9.95837569e-01,\n",
      "        9.89199758e-01,  9.86132026e-01,  9.87896919e-01,  1.00794435e+00,\n",
      "        1.30498528e-01,  9.88682032e-01,  1.02649140e+00,  1.01673305e+00,\n",
      "        9.93192077e-01,  1.00582826e+00,  8.83957744e-01,  9.90460277e-01,\n",
      "        1.00289917e+00,  9.88613248e-01,  9.86725807e-01,  9.92216825e-01,\n",
      "        1.07383394e+00,  9.94086027e-01,  9.96790171e-01,  9.92745519e-01,\n",
      "        9.85936642e-01,  9.93486762e-01,  9.87005711e-01,  1.06125295e+00,\n",
      "        1.07042038e+00,  1.56363770e-01,  9.90231991e-01,  9.98686075e-01,\n",
      "        9.85714197e-01,  1.00541365e+00,  9.96193886e-02,  9.87175226e-01,\n",
      "        1.00686705e+00,  9.99294758e-01,  8.65418077e-01,  6.87620044e-01,\n",
      "        9.85190392e-01,  9.86474633e-01,  5.83713353e-02,  1.08370304e-01,\n",
      "        9.86297488e-01,  1.00700557e+00,  8.56321007e-02,  9.84077573e-01,\n",
      "        9.92518663e-01,  1.02999282e+00,  9.85379457e-01,  9.85826969e-01,\n",
      "        9.83861327e-01,  9.97616053e-01,  9.89363074e-01,  9.86054540e-01,\n",
      "        1.02585089e+00,  9.94736433e-01,  7.50929117e-04,  9.90236282e-01,\n",
      "        9.85931158e-01,  9.84507799e-01,  1.00043178e+00,  7.42673278e-01,\n",
      "       -4.21188921e-02,  1.00194955e+00,  9.89292860e-01,  1.59235656e-01,\n",
      "        1.02266002e+00,  9.86728430e-01,  9.92096305e-01,  1.02389741e+00,\n",
      "        9.39260721e-01,  1.08769143e+00,  9.86450791e-01,  9.18260336e-01,\n",
      "        1.00203228e+00,  9.86060977e-01,  9.92002368e-01,  9.87216473e-01,\n",
      "        1.00068307e+00,  9.92297411e-01,  1.01552069e+00,  9.85159516e-01,\n",
      "        9.85245585e-01,  9.98001575e-01,  9.86911654e-01,  9.86835241e-01,\n",
      "        1.01585031e+00,  9.85669255e-01,  8.41387630e-01,  9.87423182e-01,\n",
      "        2.92203695e-01,  1.02380216e+00,  9.91755724e-01,  6.03488326e-01,\n",
      "        9.85710740e-01,  5.80438226e-02,  9.89445329e-01, -7.09572434e-03,\n",
      "        9.94073749e-01,  9.89742994e-01,  9.85152483e-01,  9.84800935e-01,\n",
      "        9.90494013e-01,  9.86643434e-01,  9.85767126e-01,  1.72354713e-01,\n",
      "        9.83159304e-01,  5.72560132e-02,  1.62347764e-01,  8.82124066e-01,\n",
      "        1.00200415e+00,  9.70409870e-01,  5.58394641e-02,  1.00023878e+00,\n",
      "        9.85566378e-01,  9.88009214e-01,  9.89906073e-01,  9.95727777e-01,\n",
      "        9.59728718e-01,  9.83618379e-01,  9.85083938e-01,  1.00200415e+00,\n",
      "        9.84681010e-01,  5.51748276e-02,  1.00308573e+00,  9.97473359e-01,\n",
      "        9.98865843e-01,  9.87112164e-01,  9.86860037e-01,  9.87413526e-01,\n",
      "        9.85823512e-01,  1.01803041e+00,  1.54710963e-01,  9.84707236e-01,\n",
      "        1.00212121e+00,  1.03181958e+00,  9.96116042e-01,  1.04212987e+00,\n",
      "       -3.90444696e-03,  1.00386238e+00,  9.95655298e-01,  9.93344903e-01,\n",
      "        9.85108495e-01,  9.89086747e-01,  9.95443821e-01,  1.00440872e+00,\n",
      "        8.78003001e-01,  1.00476158e+00,  1.01681328e+00, -8.03205669e-02,\n",
      "        1.02417636e+00,  9.98651385e-01,  6.62080944e-02,  1.00503135e+00,\n",
      "        9.88786578e-01,  9.85926032e-01,  9.98369455e-01,  9.85391498e-01,\n",
      "        9.90782619e-01,  9.86176372e-01,  1.12548470e-03], dtype=float32)]\n",
      "0.9990268\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/scott/anaconda3/envs/thesis/lib/python3.7/site-packages/ipykernel_launcher.py:110: DeprecationWarning: time.clock has been deprecated in Python 3.3 and will be removed from Python 3.8: use time.perf_counter or time.process_time instead\n",
      "/home/scott/anaconda3/envs/thesis/lib/python3.7/site-packages/ipykernel_launcher.py:125: DeprecationWarning: time.clock has been deprecated in Python 3.3 and will be removed from Python 3.8: use time.perf_counter or time.process_time instead\n"
     ]
    }
   ],
   "source": [
    "#gru_outputs, targets, gru_sMAPE = evaluate2(gru_model, test_x, test_y)\n",
    "gru_outputs, targets, gru_sMAPE = evaluate2(gru_model, train_x, train_y)\n",
    "#print(test_y)\n",
    "print(gru_outputs)\n",
    "print(gru_outputs[0][5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Time: 0.0514180000000124\n",
      "sMAPE: 4.989344605758979%\n",
      "199\n",
      "595\n",
      "0.98342574\n",
      "successess vs training data= 574 /595\n",
      "Evaluation Time: 0.0405639999999039\n",
      "sMAPE: 8.76511135213007%\n",
      "1.0 0.97854614 tensor(0.9785)\n",
      "1.0 1.00629 tensor(1.0063)\n",
      "1.0 0.29501936 tensor(0.2950)\n",
      "1.0 0.9852978 tensor(0.9853)\n",
      "1.0 0.9852041 tensor(0.9852)\n",
      "1.0 0.999086 tensor(0.9991)\n",
      "1.0 0.98720646 tensor(0.9872)\n",
      "1.0 0.9850999 tensor(0.9851)\n",
      "1.0 0.9877224 tensor(0.9877)\n",
      "1.0 1.0038041 tensor(1.0038)\n",
      "1.0 1.0010406 tensor(1.0010)\n",
      "1.0 -0.14068243 tensor(0.)\n",
      "0.0 0.99949396 tensor(0.9995)\n",
      "1.0 0.24434444 tensor(0.2443)\n",
      "1.0 0.985412 tensor(0.9854)\n",
      "1.0 0.986547 tensor(0.9865)\n",
      "1.0 1.0020455 tensor(1.0020)\n",
      "1.0 0.9874655 tensor(0.9875)\n",
      "1.0 0.17920569 tensor(0.1792)\n",
      "1.0 0.9839809 tensor(0.9840)\n",
      "1.0 1.0054525 tensor(1.0055)\n",
      "1.0 1.0003178 tensor(1.0003)\n",
      "1.0 0.21049133 tensor(0.2105)\n",
      "1.0 0.06532976 tensor(0.0653)\n",
      "1.0 -0.06742266 tensor(0.)\n",
      "1.0 1.0006107 tensor(1.0006)\n",
      "1.0 0.15727425 tensor(0.1573)\n",
      "1.0 0.99073696 tensor(0.9907)\n",
      "1.0 0.7989253 tensor(0.7989)\n",
      "1.0 0.9874222 tensor(0.9874)\n",
      "0.0 0.98825884 tensor(0.9883)\n",
      "1.0 0.92250705 tensor(0.9225)\n",
      "1.0 0.42492554 tensor(0.4249)\n",
      "1.0 0.78296435 tensor(0.7830)\n",
      "0.0 0.9923568 tensor(0.9924)\n",
      "0.0 0.98728776 tensor(0.9873)\n",
      "1.0 0.98667467 tensor(0.9867)\n",
      "1.0 0.987406 tensor(0.9874)\n",
      "1.0 0.9857329 tensor(0.9857)\n",
      "1.0 0.9857998 tensor(0.9858)\n",
      "1.0 0.9979383 tensor(0.9979)\n",
      "1.0 0.24955265 tensor(0.2496)\n",
      "1.0 0.9947972 tensor(0.9948)\n",
      "1.0 0.99066854 tensor(0.9907)\n",
      "1.0 1.008009 tensor(1.0080)\n",
      "1.0 1.0027602 tensor(1.0028)\n",
      "1.0 0.9938816 tensor(0.9939)\n",
      "1.0 0.9981116 tensor(0.9981)\n",
      "1.0 0.9867518 tensor(0.9868)\n",
      "0.0 0.044868007 tensor(0.0449)\n",
      "1.0 0.27791953 tensor(0.2779)\n",
      "1.0 0.99239016 tensor(0.9924)\n",
      "1.0 0.09642081 tensor(0.0964)\n",
      "1.0 0.8550737 tensor(0.8551)\n",
      "1.0 0.83042943 tensor(0.8304)\n",
      "1.0 0.9869753 tensor(0.9870)\n",
      "0.0 1.0303254 tensor(1.0303)\n",
      "1.0 0.9876925 tensor(0.9877)\n",
      "1.0 0.9844825 tensor(0.9845)\n",
      "1.0 0.996307 tensor(0.9963)\n",
      "1.0 1.0031732 tensor(1.0032)\n",
      "1.0 0.9974581 tensor(0.9975)\n",
      "0.0 0.9563143 tensor(0.9563)\n",
      "1.0 1.0115448 tensor(1.0115)\n",
      "1.0 1.0060333 tensor(1.0060)\n",
      "1.0 0.99165535 tensor(0.9917)\n",
      "1.0 0.9958755 tensor(0.9959)\n",
      "1.0 0.9863397 tensor(0.9863)\n",
      "1.0 0.9870858 tensor(0.9871)\n",
      "0.0 0.14161605 tensor(0.1416)\n",
      "1.0 0.99557185 tensor(0.9956)\n",
      "1.0 0.41330928 tensor(0.4133)\n",
      "1.0 0.9912814 tensor(0.9913)\n",
      "1.0 0.99069977 tensor(0.9907)\n",
      "1.0 0.98779726 tensor(0.9878)\n",
      "1.0 0.9968009 tensor(0.9968)\n",
      "1.0 0.117317915 tensor(0.1173)\n",
      "0.0 1.0000407 tensor(1.0000)\n",
      "1.0 1.015721 tensor(1.0157)\n",
      "1.0 0.32706314 tensor(0.3271)\n",
      "1.0 0.26953983 tensor(0.2695)\n",
      "1.0 0.19608839 tensor(0.1961)\n",
      "1.0 0.99766815 tensor(0.9977)\n",
      "1.0 1.0030501 tensor(1.0031)\n",
      "1.0 1.0251828 tensor(1.0252)\n",
      "1.0 1.0192767 tensor(1.0193)\n",
      "0.0 0.9990152 tensor(0.9990)\n",
      "1.0 0.990432 tensor(0.9904)\n",
      "1.0 0.98533404 tensor(0.9853)\n",
      "0.0 1.0071081 tensor(1.0071)\n",
      "1.0 0.9858756 tensor(0.9859)\n",
      "1.0 1.0007567 tensor(1.0008)\n",
      "1.0 1.0043389 tensor(1.0043)\n",
      "0.0 0.9878521 tensor(0.9879)\n",
      "1.0 0.9966425 tensor(0.9966)\n",
      "0.0 1.0482508 tensor(1.0483)\n",
      "1.0 0.00027364492 tensor(0.0003)\n",
      "1.0 1.0036947 tensor(1.0037)\n",
      "1.0 0.993871 tensor(0.9939)\n",
      "1.0 0.99426746 tensor(0.9943)\n",
      "1.0 0.9858732 tensor(0.9859)\n",
      "1.0 0.98737323 tensor(0.9874)\n",
      "1.0 0.99787116 tensor(0.9979)\n",
      "0.0 0.97731984 tensor(0.9773)\n",
      "1.0 0.99734163 tensor(0.9973)\n",
      "0.0 0.6226975 tensor(0.6227)\n",
      "1.0 0.9495909 tensor(0.9496)\n",
      "1.0 0.99183464 tensor(0.9918)\n",
      "1.0 1.0008979 tensor(1.0009)\n",
      "0.0 1.0353533 tensor(1.0354)\n",
      "1.0 0.98652315 tensor(0.9865)\n",
      "1.0 0.9742359 tensor(0.9742)\n",
      "1.0 0.3041128 tensor(0.3041)\n",
      "1.0 0.9999503 tensor(1.0000)\n",
      "1.0 0.16320476 tensor(0.1632)\n",
      "1.0 0.9985498 tensor(0.9985)\n",
      "0.0 1.1200382 tensor(1.1200)\n",
      "1.0 0.99808013 tensor(0.9981)\n",
      "0.0 1.0346442 tensor(1.0346)\n",
      "1.0 0.984974 tensor(0.9850)\n",
      "1.0 0.9862311 tensor(0.9862)\n",
      "1.0 1.0515409 tensor(1.0515)\n",
      "1.0 0.986357 tensor(0.9864)\n",
      "1.0 -0.0054676086 tensor(0.)\n",
      "1.0 0.21562295 tensor(0.2156)\n",
      "1.0 0.9797907 tensor(0.9798)\n",
      "1.0 0.9851024 tensor(0.9851)\n",
      "1.0 0.987522 tensor(0.9875)\n",
      "0.0 1.0340466 tensor(1.0340)\n",
      "1.0 -0.008084029 tensor(0.)\n",
      "1.0 1.0712447 tensor(1.0712)\n",
      "0.0 1.0166003 tensor(1.0166)\n",
      "1.0 0.9907534 tensor(0.9908)\n",
      "1.0 1.0275221 tensor(1.0275)\n",
      "1.0 0.99121726 tensor(0.9912)\n",
      "0.0 0.9954238 tensor(0.9954)\n",
      "1.0 0.99395716 tensor(0.9940)\n",
      "1.0 0.9766208 tensor(0.9766)\n",
      "1.0 0.99067986 tensor(0.9907)\n",
      "1.0 0.18418354 tensor(0.1842)\n",
      "0.0 0.6928443 tensor(0.6928)\n",
      "1.0 1.0519804 tensor(1.0520)\n",
      "1.0 0.78079 tensor(0.7808)\n",
      "0.0 1.0427055 tensor(1.0427)\n",
      "0.0 0.9864857 tensor(0.9865)\n",
      "1.0 1.0004499 tensor(1.0004)\n",
      "1.0 0.98657787 tensor(0.9866)\n",
      "1.0 0.9889914 tensor(0.9890)\n",
      "1.0 1.0991414 tensor(1.0991)\n",
      "1.0 0.99770606 tensor(0.9977)\n",
      "1.0 0.9879221 tensor(0.9879)\n",
      "1.0 0.9888381 tensor(0.9888)\n",
      "1.0 0.7690884 tensor(0.7691)\n",
      "1.0 0.98494756 tensor(0.9849)\n",
      "1.0 0.9845277 tensor(0.9845)\n",
      "1.0 0.9503156 tensor(0.9503)\n",
      "0.0 0.99366796 tensor(0.9937)\n",
      "1.0 0.9851805 tensor(0.9852)\n",
      "1.0 0.9859338 tensor(0.9859)\n",
      "1.0 0.9939817 tensor(0.9940)\n",
      "1.0 0.7999816 tensor(0.8000)\n",
      "0.0 0.98569334 tensor(0.9857)\n",
      "1.0 1.0055585 tensor(1.0056)\n",
      "1.0 0.1882399 tensor(0.1882)\n",
      "0.0 1.0148467 tensor(1.0148)\n",
      "1.0 1.0042052 tensor(1.0042)\n",
      "1.0 1.0450152 tensor(1.0450)\n",
      "1.0 0.988032 tensor(0.9880)\n",
      "0.0 0.9602386 tensor(0.9602)\n",
      "1.0 0.98695576 tensor(0.9870)\n",
      "1.0 0.36327368 tensor(0.3633)\n",
      "1.0 0.98764443 tensor(0.9876)\n",
      "1.0 0.45103967 tensor(0.4510)\n",
      "1.0 0.9862474 tensor(0.9862)\n",
      "1.0 0.99886024 tensor(0.9989)\n",
      "1.0 -0.052882403 tensor(0.)\n",
      "1.0 0.9897673 tensor(0.9898)\n",
      "1.0 0.6897002 tensor(0.6897)\n",
      "1.0 1.0070995 tensor(1.0071)\n",
      "1.0 0.99087596 tensor(0.9909)\n",
      "1.0 0.9864565 tensor(0.9865)\n",
      "1.0 0.9969746 tensor(0.9970)\n",
      "1.0 0.99448276 tensor(0.9945)\n",
      "1.0 0.9219433 tensor(0.9219)\n",
      "1.0 1.0086161 tensor(1.0086)\n",
      "1.0 0.991158 tensor(0.9912)\n",
      "0.0 0.48970416 tensor(0.4897)\n",
      "1.0 0.9870899 tensor(0.9871)\n",
      "1.0 0.9852506 tensor(0.9853)\n",
      "1.0 0.9973632 tensor(0.9974)\n",
      "1.0 1.0091336 tensor(1.0091)\n",
      "0.0 0.9870901 tensor(0.9871)\n",
      "1.0 0.9929309 tensor(0.9929)\n",
      "1.0 0.9948189 tensor(0.9948)\n",
      "1.0 0.023435414 tensor(0.0234)\n",
      "1.0 0.985047 tensor(0.9850)\n",
      "1.0 0.7055968 tensor(0.7056)\n",
      "1.0 0.7003323 tensor(0.7003)\n",
      "1.0 0.9978342 tensor(0.9978)\n",
      "successess vs test data= 167 /199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/scott/anaconda3/envs/thesis/lib/python3.7/site-packages/ipykernel_launcher.py:110: DeprecationWarning: time.clock has been deprecated in Python 3.3 and will be removed from Python 3.8: use time.perf_counter or time.process_time instead\n",
      "/home/scott/anaconda3/envs/thesis/lib/python3.7/site-packages/ipykernel_launcher.py:125: DeprecationWarning: time.clock has been deprecated in Python 3.3 and will be removed from Python 3.8: use time.perf_counter or time.process_time instead\n"
     ]
    }
   ],
   "source": [
    "#print(test_y.reshape(-1))\n",
    "\n",
    "m = nn.ReLU()\n",
    "#m = nn.Sigmoid()\n",
    "#output = m(input)\n",
    "\n",
    "gru_outputs, targets, gru_sMAPE = evaluate2(gru_model, train_x, train_y)\n",
    "#print(test_y)\n",
    "#print(gru_outputs)\n",
    "#print(gru_outputs[0][5])\n",
    "\n",
    "\n",
    "testy=test_y.reshape(-1)\n",
    "trainy=train_y.reshape(-1)\n",
    "\n",
    "print(testy.size)\n",
    "print(trainy.size)\n",
    "print(gru_outputs[0][4])\n",
    "successcounter=0\n",
    "for i in range(595):\n",
    "    #print(testy[i],gru_outputs[0][i],m(torch.tensor(gru_outputs[0][i])))\n",
    "    #print(train[i],gru_outputs[0][i],m(torch.tensor(gru_outputs[0][i])))\n",
    "    #print(trainy[i],gru_outputs[0][i], m(torch.tensor(gru_outputs[0][i])))\n",
    "    \n",
    "    \n",
    "    if abs(trainy[i]-gru_outputs[0][i])<0.2 :\n",
    "        successcounter+=1\n",
    "    #print(testy[i])\n",
    "    #print\n",
    "    #output = m(input)\n",
    "print(\"successess vs training data=\" ,successcounter,\"/595\")\n",
    "\n",
    "\n",
    "successcounter=0\n",
    "\n",
    "gru_outputs, targets, gru_sMAPE = evaluate2(gru_model, test_x, test_y)\n",
    "\n",
    "for i in range(199):\n",
    "   \n",
    "    \n",
    "    print(trainy[i],gru_outputs[0][i], m(torch.tensor(gru_outputs[0][i])))\n",
    "    \n",
    "    \n",
    "    if abs(testy[i]-gru_outputs[0][i])<0.2 :\n",
    "        successcounter+=1\n",
    "    #print(testy[i])\n",
    "    #print\n",
    "    #output = m(input)\n",
    "print(\"successess vs test data=\" ,successcounter,\"/199\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
