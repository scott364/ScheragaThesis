{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we'll be using a GRU model for a time series prediction task and we will compare the performance of the GRU model against an LSTM model as well. The dataset that we will be using is the Hourly Energy Consumption dataset which can be found on [Kaggle](https://www.kaggle.com/robikscube/hourly-energy-consumption). The dataset contains power consumption data across different regions around the United States recorded on an hourly basis.\n",
    "\n",
    "You can run the code implementation in this article on FloydHub using their GPUs on the cloud by clicking the following link and using the main.ipynb notebook.\n",
    "\n",
    "[![Run on FloydHub](https://static.floydhub.com/button/button-small.svg)](https://floydhub.com/run?template=https://github.com/gabrielloye/https://github.com/gabrielloye/GRU_Prediction)\n",
    "\n",
    "This will speed up the training process significantly. Alternatively, the link to the GitHub repository can be found [here]().\n",
    "\n",
    "The goal of this implementation is to create a model that can accurately predict the energy usage in the next hour given historical usage data. We will be using both the GRU and LSTM model to train on a set of historical data and evaluate both models on an unseen test set. To do so, we’ll start with feature selection, data-preprocessing, followed by defining, training and eventually evaluating the models.\n",
    "\n",
    "We will be using the PyTorch library to implement both types of models along with other common Python libraries used in data analytics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.python-engineer.com/posts/pytorch-rnn-lstm-gru/\n",
    "\n",
    "#https://blog.floydhub.com/gru-with-pytorch/\n",
    "\n",
    "import os\n",
    "import time\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import random\n",
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from tqdm import tqdm_notebook\n",
    "from sklearn.preprocessing import MinMaxScaler,QuantileTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Local ###\n",
    "#from data_processing import *\n",
    "\n",
    "\n",
    "\n",
    "# Define data root directory\n",
    "\n",
    "#data_dir = \"./data/\"\n",
    "#print(os.listdir(data_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a total of **12** *.csv* files containing hourly energy trend data (*'est_hourly.paruqet'* and *'pjm_hourly_est.csv'* are not used). In our next step, we will be reading these files and pre-processing these data in this order:\n",
    "- Getting the time data of each individual time step and generalizing them\n",
    "    - Hour of the day *i.e. 0-23*\n",
    "    - Day of the week *i.e. 1-7*\n",
    "    - Month *i.e. 1-12*\n",
    "    - Day of the year *i.e. 1-365*\n",
    "    \n",
    "    \n",
    "- Scale the data to values between 0 and 1\n",
    "    - Algorithms tend to perform better or converge faster when features are on a relatively similar scale and/or close to normally distributed\n",
    "    - Scaling preserves the shape of the original distribution and doesn't reduce the importance of outliers.\n",
    "    \n",
    "    \n",
    "- Group the data into sequences to be used as inputs to the model and store their corresponding labels\n",
    "    - The **sequence length** or **lookback period** is the number of data points in history that the model will use to make the prediction\n",
    "    - The label will be the next data point in time after the last one in the input sequence\n",
    "    \n",
    "\n",
    "- The inputs and labels will then be split into training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# torch.cuda.is_available() checks and returns a Boolean True if a GPU is available, else it'll return False\n",
    "is_cuda = torch.cuda.is_available()\n",
    "\n",
    "# If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.\n",
    "if is_cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    \n",
    "print(device)\n",
    "\n",
    "\n",
    "def get_torch_device( v=0 ):\n",
    "    # torch.cuda.is_available() checks and returns a Boolean True if a GPU is available, else it'll return False\n",
    "    is_cuda = torch.cuda.is_available()\n",
    "    # If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.\n",
    "    if is_cuda:\n",
    "        device = torch.device(\"cuda\")\n",
    "        if v:  print( \"CUDA Available!\" )\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        if v:  print( \"NO CUDA\" )\n",
    "    return device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1440, 10)\n",
      "(3960, 10)\n",
      "(5400, 10)\n"
     ]
    }
   ],
   "source": [
    "#choppeddata=pd.read_csv('choppeddata_10_06_2021.csv')#.head()\n",
    "choppeddata1=pd.read_csv('choppeddata_10_04_2021_equalsuccessfail.csv')#.head()\n",
    "choppeddata2=pd.read_csv('choppeddata_10_06_2021_equalsuccessfail.csv')#.head()\n",
    "print(choppeddata1.shape)\n",
    "print(choppeddata2.shape)\n",
    "frames = [choppeddata1, choppeddata2]\n",
    "choppeddata = pd.concat(frames)\n",
    "print(choppeddata.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5400, 10)\n",
      "total runs: 900\n"
     ]
    }
   ],
   "source": [
    "print(choppeddata.shape)\n",
    "runqty=int(choppeddata.shape[0]/6)\n",
    "print(\"total runs:\",runqty)\n",
    "choppedheaders=[]\n",
    "lookback=10 #save only the last 11 timesteps\n",
    "for i in range(lookback):  \n",
    "    label=str(i)\n",
    "    choppedheaders.append(\"header\"+label)\n",
    "\n",
    "#put chopped data in np.arrays\n",
    "State=np.zeros((runqty,5,lookback)) #96 runs,with 5 sets of data (x,y,z,roll,pitch) each, and each run is 11 timesteps long\n",
    "Labels=np.zeros((runqty,lookback)) #96 runs, each run is 11 timesteps long\n",
    "runcounter=0\n",
    "\n",
    "for i in range(0,choppeddata.shape[0],6):\n",
    "            State[runcounter][0][:]=(choppeddata[choppedheaders[:]].iloc[i]).tolist()\n",
    "            State[runcounter][1][:]=(choppeddata[choppedheaders[:]].iloc[i+1]).tolist()\n",
    "            State[runcounter][2][:]=(choppeddata[choppedheaders[:]].iloc[i+2]).tolist()\n",
    "            State[runcounter][3][:]=(choppeddata[choppedheaders[:]].iloc[i+3]).tolist()\n",
    "            State[runcounter][4][:]=(choppeddata[choppedheaders[:]].iloc[i+4]).tolist()\n",
    "            Labels[runcounter][:]=(choppeddata[choppedheaders[:]].iloc[i+5]).tolist()  #labels   \n",
    "            runcounter+=1\n",
    "#print(State[0])\n",
    "#print(Labels)\n",
    "#print(Labels[:,9]) #just getting finals labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape (900, 5, 10)\n",
      "Test set X size 11250\n",
      "Train set Y size 675\n",
      "Test set X size 11250\n",
      "Test set Y size 225\n"
     ]
    }
   ],
   "source": [
    "#X= range(0,575,6)\n",
    "#y= range(0,575,6)\n",
    "\n",
    "X=State\n",
    "y=Labels[:,lookback-1]\n",
    "print(\"x.shape\",X.shape)\n",
    "\n",
    "y=y.reshape(runqty,1)\n",
    "\n",
    "random_seed=int(time.time())\n",
    "#print(int(time.time()))\n",
    "train_x, test_x, train_y,test_y = train_test_split(X, y, test_size=.25, #0.33, \n",
    "                                                   random_state=random_seed)\n",
    "#print(\"Train\")\n",
    "#print(train_x[0])\n",
    "#print(train_y[0])\n",
    "print(\"Test set X size\", test_x.size)\n",
    "print(\"Train set Y size\", train_y.size)\n",
    "#print(test_x[0])\n",
    "#print(test_y[0])\n",
    "print(\"Test set X size\", test_x.size)\n",
    "print(\"Test set Y size\", test_y.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a total of 980,185 sequences of training data\n",
    "\n",
    "To improve the speed of our training, we can process the data in batches so that the model does not need to update its weights as frequently. The Torch *Dataset* and *DataLoader* classes are useful for splitting our data into batches and shuffling them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataloader.DataLoader object at 0x7f0738479590>\n"
     ]
    }
   ],
   "source": [
    "batch_size = 4\n",
    "#a batch size of 64, i.e. each element in the dataloader iterable will return a batch of 64 features and labels.\n",
    "\n",
    "train_data = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size, drop_last=True)\n",
    "\n",
    "test_data   = TensorDataset( torch.from_numpy( test_x ), torch.from_numpy( test_y ) )\n",
    "test_loader = DataLoader( test_data, shuffle = True, batch_size = batch_size, drop_last = True )\n",
    "print(train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also check if we have any GPUs to speed up our training time by many folds. If you’re using FloydHub with GPU to run this code, the training time will be significantly reduced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUNet(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, n_layers, drop_prob=0.2):\n",
    "        super(GRUNet, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.gru = nn.GRU(input_dim, hidden_dim, n_layers, batch_first=True, dropout=drop_prob)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x, h):\n",
    "        out, h = self.gru(x, h)\n",
    "        out = self.fc(self.relu(out[:,-1]))\n",
    "        return out, h\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        hidden = weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device)\n",
    "        return hidden\n",
    "def train(train_loader, learn_rate, hidden_dim=128, EPOCHS=400, model_type=\"GRU\"):\n",
    "\n",
    "    losslist=[]\n",
    "    # Setting common hyperparameters\n",
    "    input_dim = next(iter(train_loader))[0].shape[2]  #  = 11\n",
    "    #print(input_dim)\n",
    "    #print(\"input_dim\",input_dim)\n",
    "    output_dim = 1\n",
    "    n_layers = 2\n",
    "    # Instantiating the models\n",
    "    if model_type == \"GRU\":\n",
    "        model = GRUNet(input_dim, hidden_dim, output_dim, n_layers)\n",
    "    else:\n",
    "        model = LSTMNet(input_dim, hidden_dim, output_dim, n_layers)\n",
    "    model.to(device)\n",
    "    \n",
    "    # Defining loss function and optimizer\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learn_rate)\n",
    "    \n",
    "    model.train()\n",
    "    print(\"Starting Training of {} model\".format(model_type))\n",
    "    epoch_times = []\n",
    "    # Start training loop\n",
    "    for epoch in range(1,EPOCHS+1):\n",
    "        start_time = time.clock()\n",
    "        h = model.init_hidden(batch_size)\n",
    "        avg_loss = 0.\n",
    "        counter = 0\n",
    "        for x, label in train_loader:\n",
    "            #print(\"x\",x)\n",
    "            #print(\"label\",label)\n",
    "            counter += 1\n",
    "            if model_type == \"GRU\":\n",
    "                h = h.data\n",
    "            else:\n",
    "                h = tuple([e.data for e in h])\n",
    "            model.zero_grad()\n",
    "            \n",
    "            out, h = model(x.to(device).float(), h)\n",
    "            #print(\"out\",out)\n",
    "            loss = criterion(out, label.to(device).float())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            avg_loss += loss.item()\n",
    "            \n",
    "            if counter%20000 == 0:\n",
    "                print(\"Epoch {}......Step: {}/{}....... Average Loss for Epoch: {}\".format(epoch, counter, len(train_loader), avg_loss/counter))\n",
    "        current_time = time.clock()\n",
    "        if epoch%40 == 0:\n",
    "            print(\"Epoch {}/{} Done, Total Loss: {}   Time Elapsed: {} seconds\".format(epoch, EPOCHS, avg_loss/len(train_loader),str(current_time-start_time)))\n",
    "        \n",
    "            #print(\"Total\".format())\n",
    "        losslist.append(avg_loss/len(train_loader))\n",
    "        epoch_times.append(current_time-start_time)\n",
    "    print(\"Total Training Time: {} seconds\".format(str(sum(epoch_times))))\n",
    "    plt.plot(losslist)\n",
    "    plt.title(\"Loss\")\n",
    "    plt.show()\n",
    "    return model\n",
    "\n",
    "def evaluate(model, test_x, test_y):\n",
    "    model.eval()\n",
    "    outputs = []\n",
    "    targets = []\n",
    "    start_time = time.clock()\n",
    "    #for i in test_x.keys():\n",
    "    for i in range( len( test_x ) ):    \n",
    "        inp = torch.from_numpy(np.array(test_x[i])) # should be 5x1\n",
    "        labs = torch.from_numpy(np.array(test_y[i])) #should be 1x1\n",
    "        h = model.init_hidden(inp.shape[0])\n",
    "        #print(\"inp\",inp)\n",
    "        #print(\"labs\",labs)\n",
    "        #print(\"h\",h)\n",
    "        out, h = model(inp.to(device).float(), h)\n",
    "        #outputs.append(label_scalers[i].inverse_transform(out.cpu().detach().numpy()).reshape(-1))\n",
    "        #targets.append(label_scalers[i].inverse_transform(labs.numpy()).reshape(-1))\n",
    "        outputs.append( out.cpu().detach().numpy().reshape(-1) )\n",
    "        targets.append( labs.numpy().reshape(-1) )\n",
    "        \n",
    "    print(\"Evaluation Time: {}\".format(str(time.clock()-start_time)))\n",
    "    sMAPE = 0\n",
    "    for i in range(len(outputs)):\n",
    "        sMAPE += np.mean(abs(outputs[i]-targets[i])/(targets[i]+outputs[i])/2)/len(outputs)\n",
    "    print(\"sMAPE: {}%\".format(sMAPE*100))\n",
    "    return outputs, targets, sMAPE\n",
    "                               \n",
    "def evaluate2(model, test_x, test_y):\n",
    "    model.eval()\n",
    "    outputs = []\n",
    "    targets = []  #labels\n",
    "    start_time = time.clock()\n",
    "    #for i in test_x.keys():\n",
    "    #for i in range( len( test_x ) ):    \n",
    "    inp = torch.from_numpy(np.array(test_x)) # should be 5x1\n",
    "    labs = torch.from_numpy(np.array(test_y)) #should be 1x1\n",
    "    h = model.init_hidden(inp.shape[0])\n",
    "    #print(\"inp\",inp)\n",
    "    #print(\"labs\",labs)\n",
    "    #print(\"h\",h)\n",
    "    out, h = model(inp.to(device).float(), h)\n",
    "    #outputs.append(label_scalers[i].inverse_transform(out.cpu().detach().numpy()).reshape(-1))\n",
    "    #targets.append(label_scalers[i].inverse_transform(labs.numpy()).reshape(-1))\n",
    "    outputs.append( out.cpu().detach().numpy().reshape(-1) )\n",
    "    targets.append( labs.numpy().reshape(-1) )\n",
    "        \n",
    "    print(\"Evaluation Time: {}\".format(str(time.clock()-start_time)))\n",
    "    sMAPE = 0\n",
    "    for i in range(len(outputs)):\n",
    "        sMAPE += np.mean(abs(outputs[i]-targets[i])/(targets[i]+outputs[i])/2)/len(outputs)\n",
    "    print(\"sMAPE: {}%\".format(sMAPE*100))\n",
    "    return outputs, targets, sMAPE                               \n",
    "\n",
    "def evaluatefull(model, train_x, train_y, test_x, test_y,maxdifference=0.2, verbose=False):\n",
    "\n",
    "    m = nn.ReLU()\n",
    "    #m = nn.Sigmoid()\n",
    "    #output = m(input)\n",
    "    print(\"Vs Training Set\")\n",
    "    gru_outputs, targets, gru_sMAPE = evaluate2(gru_model, train_x, train_y)\n",
    "    #print(test_y)\n",
    "    #print(gru_outputs)\n",
    "    #print(gru_outputs[0][5])\n",
    "\n",
    "\n",
    "    testy=test_y.reshape(-1)\n",
    "    trainy=train_y.reshape(-1)\n",
    "\n",
    "\n",
    "    #print(\"Train size:\",trainy.size)\n",
    "    print(gru_outputs[0][4])\n",
    "    train_successcounter=0\n",
    "    for i in range(int(trainy.size)):\n",
    "        #print(testy[i],gru_outputs[0][i],m(torch.tensor(gru_outputs[0][i])))\n",
    "        #print(train[i],gru_outputs[0][i],m(torch.tensor(gru_outputs[0][i])))\n",
    "\n",
    "\n",
    "        #print(trainy[i],gru_outputs[0][i], m(torch.tensor(gru_outputs[0][i])))\n",
    "\n",
    "\n",
    "        if abs(trainy[i]-gru_outputs[0][i])<maxdifference :\n",
    "            train_successcounter+=1\n",
    "        #print(testy[i])\n",
    "        #print\n",
    "        #output = m(input)\n",
    "\n",
    "\n",
    "\n",
    "    test_successcounter=0\n",
    "    print(\"Vs Test Set\")\n",
    "    gru_outputs, targets, gru_sMAPE = evaluate2(gru_model, test_x, test_y)\n",
    "    #print(\"test size: \",testy.size)\n",
    "\n",
    "    for i in range(int(testy.size)):\n",
    "\n",
    "\n",
    "        #, m(torch.tensor(gru_outputs[0][i])))\n",
    "\n",
    "\n",
    "        if abs(testy[i]-gru_outputs[0][i])<maxdifference :\n",
    "            test_successcounter+=1\n",
    "            if verbose==True:\n",
    "                print(testy[i],gru_outputs[0][i], \"OK\" )\n",
    "        else:\n",
    "            if verbose==True:\n",
    "                print(testy[i],gru_outputs[0][i], \"X\" )\n",
    "            #print(testy[i])\n",
    "        #print\n",
    "        #output = m(input)\n",
    "    print(\"\")\n",
    "    print(\" vs training data=\" ,train_successcounter,\"/\",trainy.size, \" vs test data=\" ,\n",
    "          test_successcounter,\"/\",testy.size,int(100*test_successcounter/testy.size),\"%\", \"at max difference\",maxdifference )\n",
    "    return ( train_successcounter ,test_successcounter)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape (900, 5, 10)\n",
      "Starting Training of GRU model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/scott/anaconda3/envs/thesis/lib/python3.7/site-packages/ipykernel_launcher.py:45: DeprecationWarning: time.clock has been deprecated in Python 3.3 and will be removed from Python 3.8: use time.perf_counter or time.process_time instead\n",
      "/home/scott/anaconda3/envs/thesis/lib/python3.7/site-packages/ipykernel_launcher.py:68: DeprecationWarning: time.clock has been deprecated in Python 3.3 and will be removed from Python 3.8: use time.perf_counter or time.process_time instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40/3000 Done, Total Loss: 0.2474797921521323   Time Elapsed: 0.04506500000002234 seconds\n",
      "Epoch 80/3000 Done, Total Loss: 0.23171513988858178   Time Elapsed: 0.045098999999936495 seconds\n",
      "Epoch 120/3000 Done, Total Loss: 0.20816763313043685   Time Elapsed: 0.04659000000015112 seconds\n",
      "Epoch 160/3000 Done, Total Loss: 0.19000478727476938   Time Elapsed: 0.04443500000002132 seconds\n",
      "Epoch 200/3000 Done, Total Loss: 0.1578974269685291   Time Elapsed: 0.04446800000005169 seconds\n",
      "Epoch 240/3000 Done, Total Loss: 0.13247635783184142   Time Elapsed: 0.04548900000008871 seconds\n",
      "Epoch 280/3000 Done, Total Loss: 0.12310370412610826   Time Elapsed: 0.04611899999986235 seconds\n",
      "Epoch 320/3000 Done, Total Loss: 0.11385272248160272   Time Elapsed: 0.044951999999966574 seconds\n",
      "Epoch 360/3000 Done, Total Loss: 0.11579080351761409   Time Elapsed: 0.04571600000008402 seconds\n",
      "Epoch 400/3000 Done, Total Loss: 0.09885252834785552   Time Elapsed: 0.04534899999998743 seconds\n",
      "Epoch 440/3000 Done, Total Loss: 0.09010580643301919   Time Elapsed: 0.044988999999986845 seconds\n",
      "Epoch 480/3000 Done, Total Loss: 0.0914161755215554   Time Elapsed: 0.04421100000013212 seconds\n",
      "Epoch 520/3000 Done, Total Loss: 0.07794926226848647   Time Elapsed: 0.04680400000006557 seconds\n",
      "Epoch 560/3000 Done, Total Loss: 0.07566605880856514   Time Elapsed: 0.04446899999993548 seconds\n",
      "Epoch 600/3000 Done, Total Loss: 0.0738664244612058   Time Elapsed: 0.04491799999982504 seconds\n",
      "Epoch 640/3000 Done, Total Loss: 0.0718814390046256   Time Elapsed: 0.04817800000000716 seconds\n",
      "Epoch 680/3000 Done, Total Loss: 0.06905211242181915   Time Elapsed: 0.04712799999992967 seconds\n",
      "Epoch 720/3000 Done, Total Loss: 0.06761889740647305   Time Elapsed: 0.04521599999998216 seconds\n",
      "Epoch 760/3000 Done, Total Loss: 0.06389099412730762   Time Elapsed: 0.045663999999987936 seconds\n",
      "Epoch 800/3000 Done, Total Loss: 0.06429714869175639   Time Elapsed: 0.04591400000003887 seconds\n",
      "Epoch 840/3000 Done, Total Loss: 0.05953437258445081   Time Elapsed: 0.04660999999987325 seconds\n",
      "Epoch 880/3000 Done, Total Loss: 0.06789185975988705   Time Elapsed: 0.044208000000026004 seconds\n",
      "Epoch 920/3000 Done, Total Loss: 0.05994539814335959   Time Elapsed: 0.04452499999979409 seconds\n",
      "Epoch 960/3000 Done, Total Loss: 0.058042005236659734   Time Elapsed: 0.047603999999864754 seconds\n",
      "Epoch 1000/3000 Done, Total Loss: 0.05693677063321784   Time Elapsed: 0.04461799999990035 seconds\n",
      "Epoch 1040/3000 Done, Total Loss: 0.059574381492677186   Time Elapsed: 0.043754999999919164 seconds\n",
      "Epoch 1080/3000 Done, Total Loss: 0.05461094967488732   Time Elapsed: 0.0463799999999992 seconds\n",
      "Epoch 1120/3000 Done, Total Loss: 0.057113983003156524   Time Elapsed: 0.04605900000001384 seconds\n",
      "Epoch 1160/3000 Done, Total Loss: 0.05811813034649406   Time Elapsed: 0.04620799999997871 seconds\n",
      "Epoch 1200/3000 Done, Total Loss: 0.052663444563569056   Time Elapsed: 0.0569590000000062 seconds\n",
      "Epoch 1240/3000 Done, Total Loss: 0.05087193554001195   Time Elapsed: 0.04871200000002318 seconds\n",
      "Epoch 1280/3000 Done, Total Loss: 0.054409166531903405   Time Elapsed: 0.0454650000001493 seconds\n",
      "Epoch 1320/3000 Done, Total Loss: 0.05175580909209592   Time Elapsed: 0.047953999999890584 seconds\n",
      "Epoch 1360/3000 Done, Total Loss: 0.05697060363101108   Time Elapsed: 0.051375000000007276 seconds\n",
      "Epoch 1400/3000 Done, Total Loss: 0.04937739084873881   Time Elapsed: 0.04889200000002347 seconds\n",
      "Epoch 1440/3000 Done, Total Loss: 0.046332256957179026   Time Elapsed: 0.0457770000000437 seconds\n",
      "Epoch 1480/3000 Done, Total Loss: 0.04894163016052473   Time Elapsed: 0.05149299999993673 seconds\n",
      "Epoch 1520/3000 Done, Total Loss: 0.04741478871021952   Time Elapsed: 0.0489509999999882 seconds\n",
      "Epoch 1560/3000 Done, Total Loss: 0.04904958162279356   Time Elapsed: 0.06629700000007688 seconds\n",
      "Epoch 1600/3000 Done, Total Loss: 0.05013545788824558   Time Elapsed: 0.045548000000053435 seconds\n",
      "Epoch 1640/3000 Done, Total Loss: 0.047341310020004   Time Elapsed: 0.046459000000140804 seconds\n",
      "Epoch 1680/3000 Done, Total Loss: 0.045651379662255444   Time Elapsed: 0.046654999999873326 seconds\n",
      "Epoch 1720/3000 Done, Total Loss: 0.04670186633510249   Time Elapsed: 0.0453899999999976 seconds\n",
      "Epoch 1760/3000 Done, Total Loss: 0.04507897732158502   Time Elapsed: 0.046983000000182074 seconds\n",
      "Epoch 1800/3000 Done, Total Loss: 0.046306894205155824   Time Elapsed: 0.04757500000005166 seconds\n",
      "Epoch 1840/3000 Done, Total Loss: 0.04256418990414767   Time Elapsed: 0.04799400000001697 seconds\n",
      "Epoch 1880/3000 Done, Total Loss: 0.042137195489236286   Time Elapsed: 0.045521000000007916 seconds\n",
      "Epoch 1920/3000 Done, Total Loss: 0.04196034425071308   Time Elapsed: 0.04578300000002855 seconds\n",
      "Epoch 1960/3000 Done, Total Loss: 0.03993998773928199   Time Elapsed: 0.05209700000000339 seconds\n",
      "Epoch 2000/3000 Done, Total Loss: 0.04285686844516368   Time Elapsed: 0.0473700000000008 seconds\n",
      "Epoch 2040/3000 Done, Total Loss: 0.03981699351043928   Time Elapsed: 0.04655600000000959 seconds\n",
      "Epoch 2080/3000 Done, Total Loss: 0.04041362513921091   Time Elapsed: 0.04561300000000301 seconds\n",
      "Epoch 2120/3000 Done, Total Loss: 0.039031797576518285   Time Elapsed: 0.04601600000000872 seconds\n",
      "Epoch 2160/3000 Done, Total Loss: 0.038513769972182456   Time Elapsed: 0.05282400000010057 seconds\n",
      "Epoch 2200/3000 Done, Total Loss: 0.041143964133447126   Time Elapsed: 0.045732999999927415 seconds\n",
      "Epoch 2240/3000 Done, Total Loss: 0.04351629205935058   Time Elapsed: 0.04882199999997283 seconds\n",
      "Epoch 2280/3000 Done, Total Loss: 0.03513234607609255   Time Elapsed: 0.04631700000004457 seconds\n",
      "Epoch 2320/3000 Done, Total Loss: 0.03537949766697628   Time Elapsed: 0.047638000000006286 seconds\n",
      "Epoch 2360/3000 Done, Total Loss: 0.05250420663062306   Time Elapsed: 0.04675600000018676 seconds\n",
      "Epoch 2400/3000 Done, Total Loss: 0.03627175030608972   Time Elapsed: 0.044777999999951135 seconds\n",
      "Epoch 2440/3000 Done, Total Loss: 0.039967951409163927   Time Elapsed: 0.045896000000084314 seconds\n",
      "Epoch 2480/3000 Done, Total Loss: 0.03070259369200184   Time Elapsed: 0.0491300000001047 seconds\n",
      "Epoch 2520/3000 Done, Total Loss: 0.03605886786023066   Time Elapsed: 0.044460999999955675 seconds\n",
      "Epoch 2560/3000 Done, Total Loss: 0.032678975723683834   Time Elapsed: 0.04651799999987816 seconds\n",
      "Epoch 2600/3000 Done, Total Loss: 0.03699842725126516   Time Elapsed: 0.04829199999994671 seconds\n",
      "Epoch 2640/3000 Done, Total Loss: 0.0360975835383648   Time Elapsed: 0.044499999999970896 seconds\n",
      "Epoch 2680/3000 Done, Total Loss: 0.03461351205727884   Time Elapsed: 0.050349999999980355 seconds\n",
      "Epoch 2720/3000 Done, Total Loss: 0.02789678264941488   Time Elapsed: 0.048052999999981694 seconds\n",
      "Epoch 2760/3000 Done, Total Loss: 0.03098316898658162   Time Elapsed: 0.046439999999847714 seconds\n",
      "Epoch 2800/3000 Done, Total Loss: 0.031628597261650224   Time Elapsed: 0.0483520000000226 seconds\n",
      "Epoch 2840/3000 Done, Total Loss: 0.027378682934102557   Time Elapsed: 0.049962000000050466 seconds\n",
      "Epoch 2880/3000 Done, Total Loss: 0.032433002743692624   Time Elapsed: 0.04834600000003775 seconds\n",
      "Epoch 2920/3000 Done, Total Loss: 0.026212837391843397   Time Elapsed: 0.04584100000010949 seconds\n",
      "Epoch 2960/3000 Done, Total Loss: 0.02664258202449197   Time Elapsed: 0.044415000000071814 seconds\n",
      "Epoch 3000/3000 Done, Total Loss: 0.03156151791058835   Time Elapsed: 0.045243999999911466 seconds\n",
      "Total Training Time: 141.0344550000027 seconds\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vs Training Set\n",
      "Evaluation Time: 0.004442000000153712\n",
      "sMAPE: 14.226501666634316%\n",
      "-0.07716861\n",
      "Vs Test Set\n",
      "Evaluation Time: 0.0008539999998902204\n",
      "sMAPE: 16.058104830049913%\n",
      "\n",
      " vs training data= 576 / 675  vs test data= 166 / 225 73 % at max difference 0.2\n",
      "Vs Training Set\n",
      "Evaluation Time: 0.0016550000000279397\n",
      "sMAPE: 14.226501666634316%\n",
      "-0.07716861\n",
      "Vs Test Set\n",
      "Evaluation Time: 0.0009600000000773434\n",
      "sMAPE: 16.058104830049913%\n",
      "\n",
      " vs training data= 618 / 675  vs test data= 181 / 225 80 % at max difference 0.3\n",
      "Vs Training Set\n",
      "Evaluation Time: 0.0019070000000738219\n",
      "sMAPE: 14.226501666634316%\n",
      "-0.07716861\n",
      "Vs Test Set\n",
      "Evaluation Time: 0.0006799999998747808\n",
      "sMAPE: 16.058104830049913%\n",
      "\n",
      " vs training data= 638 / 675  vs test data= 192 / 225 85 % at max difference 0.4\n",
      "Vs Training Set\n",
      "Evaluation Time: 0.001576999999997497\n",
      "sMAPE: 14.226501666634316%\n",
      "-0.07716861\n",
      "Vs Test Set\n",
      "Evaluation Time: 0.0009350000000267755\n",
      "sMAPE: 16.058104830049913%\n",
      "\n",
      " vs training data= 656 / 675  vs test data= 197 / 225 87 % at max difference 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/scott/anaconda3/envs/thesis/lib/python3.7/site-packages/ipykernel_launcher.py:111: DeprecationWarning: time.clock has been deprecated in Python 3.3 and will be removed from Python 3.8: use time.perf_counter or time.process_time instead\n",
      "/home/scott/anaconda3/envs/thesis/lib/python3.7/site-packages/ipykernel_launcher.py:126: DeprecationWarning: time.clock has been deprecated in Python 3.3 and will be removed from Python 3.8: use time.perf_counter or time.process_time instead\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n fixed normalization-400 epochs, batch size of 4\\n vs training data= 496 / 675  vs test data= 142 / 225 63 % at max difference 0.2\\n vs training data= 563 / 675  vs test data= 163 / 225 72 % at max difference 0.3\\n vs training data= 608 / 675  vs test data= 173 / 225 76 % at max difference 0.4\\n vs training data= 633 / 675  vs test data= 186 / 225 82 % at max difference 0.5\\n \\n  fixed normalization-600 epochs, batch size of 4\\n vs training data= 644 / 675  vs test data= 171 / 225 76 % at max difference 0.2\\n vs training data= 662 / 675  vs test data= 178 / 225 79 % at max difference 0.3\\n vs training data= 670 / 675  vs test data= 182 / 225 80 % at max difference 0.4\\n vs training data= 672 / 675  vs test data= 187 / 225 83 % at max difference 0.5\\n \\n \\nfixed normalization-600 epochs  lr=.0005, batch size of 4\\nvs training data= 631 / 675  vs test data= 157 / 225 69 % at max difference 0.2\\nvs training data= 653 / 675  vs test data= 172 / 225 76 % at max difference 0.3\\nvs training data= 667 / 675  vs test data= 181 / 225 80 % at max difference 0.4\\n83 % at max difference 0.5\\n\\nfixed normalization-800 epochs  lr=.0005, batch size of 4\\n vs training data= 661 / 675  vs test data= 173 / 225 76 % at max difference 0.2\\n vs training data= 671 / 675  vs test data= 188 / 225 83 % at max difference 0.3\\n vs training data= 673 / 675  vs test data= 195 / 225 86 % at max difference 0.4\\n vs training data= 674 / 675  vs test data= 203 / 225 90 % at max difference 0.5\\n'"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = 0.0001\n",
    "batch_size = 32\n",
    "#a batch size of 64, i.e. each element in the dataloader iterable will return a batch of 64 features and labels.\n",
    "\n",
    "X=State\n",
    "y=Labels[:,lookback-1]\n",
    "print(\"x.shape\",X.shape)\n",
    "y=y.reshape(runqty,1)\n",
    "\n",
    "\n",
    "random_seed=int(time.time())\n",
    "train_x, test_x, train_y,test_y = train_test_split(X, y, test_size=.25, #0.33, \n",
    "                                                   random_state=random_seed)\n",
    "\n",
    "train_data = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size, drop_last=True)\n",
    "test_data   = TensorDataset( torch.from_numpy( test_x ), torch.from_numpy( test_y ) )\n",
    "test_loader = DataLoader( test_data, shuffle = True, batch_size = batch_size, drop_last = True )\n",
    "\n",
    "\n",
    "gru_model = train(train_loader, lr , hidden_dim=128, EPOCHS=3000, model_type=\"GRU\")\n",
    "train2 ,test2=evaluatefull(gru_model, train_x, train_y, test_x, test_y,maxdifference=.2)\n",
    "train3 ,test3=evaluatefull(gru_model, train_x, train_y, test_x, test_y,maxdifference=.3)\n",
    "train4 ,test4=evaluatefull(gru_model, train_x, train_y, test_x, test_y,maxdifference=.4)\n",
    "train5 ,test5=evaluatefull(gru_model, train_x, train_y, test_x, test_y,maxdifference=.5)\n",
    "\n",
    "\"\"\"\n",
    " fixed normalization-400 epochs, batch size of 4\n",
    " vs training data= 496 / 675  vs test data= 142 / 225 63 % at max difference 0.2\n",
    " vs training data= 563 / 675  vs test data= 163 / 225 72 % at max difference 0.3\n",
    " vs training data= 608 / 675  vs test data= 173 / 225 76 % at max difference 0.4\n",
    " vs training data= 633 / 675  vs test data= 186 / 225 82 % at max difference 0.5\n",
    " \n",
    "  fixed normalization-600 epochs, batch size of 4\n",
    " vs training data= 644 / 675  vs test data= 171 / 225 76 % at max difference 0.2\n",
    " vs training data= 662 / 675  vs test data= 178 / 225 79 % at max difference 0.3\n",
    " vs training data= 670 / 675  vs test data= 182 / 225 80 % at max difference 0.4\n",
    " vs training data= 672 / 675  vs test data= 187 / 225 83 % at max difference 0.5\n",
    " \n",
    " \n",
    "fixed normalization-600 epochs  lr=.0005, batch size of 4\n",
    "vs training data= 631 / 675  vs test data= 157 / 225 69 % at max difference 0.2\n",
    "vs training data= 653 / 675  vs test data= 172 / 225 76 % at max difference 0.3\n",
    "vs training data= 667 / 675  vs test data= 181 / 225 80 % at max difference 0.4\n",
    "83 % at max difference 0.5\n",
    "\n",
    "fixed normalization-800 epochs  lr=.0005, batch size of 4  ---USED FOR MODEL!!\n",
    " vs training data= 661 / 675  vs test data= 173 / 225 76 % at max difference 0.2\n",
    " vs training data= 671 / 675  vs test data= 188 / 225 83 % at max difference 0.3\n",
    " vs training data= 673 / 675  vs test data= 195 / 225 86 % at max difference 0.4\n",
    " vs training data= 674 / 675  vs test data= 203 / 225 90 % at max difference 0.5\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_episode(model, data,  maxdifference=0.2, verbose=False):\n",
    "\n",
    "   \n",
    "    model.eval()\n",
    "    inp = torch.from_numpy(np.array(data)) # should be 5x1\n",
    "    h = model.init_hidden(inp.shape[0])\n",
    "    #print(\"inp\",inp)\n",
    "    #print(\"labs\",labs)\n",
    "    #print(\"h\",h)\n",
    "    out, h = model(inp.to(device).float(), h)\n",
    "    #print(\"model output\",out)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10_09_2021\n",
      "model saved\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from datetime import date\n",
    "\n",
    "today = date.today()    \n",
    "todaydate = today.strftime(\"%m_%d_%Y\")\n",
    "print(todaydate)\n",
    "torch.save(gru_model,\"currentmodel_\"+todaydate+\".pt\")\n",
    "print(\"model saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Test model on new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded\n"
     ]
    }
   ],
   "source": [
    "#gru_model2 = GRUNet(*args, **kwargs)\n",
    "    \n",
    "gru_model3=torch.load('currentmodel_10_09_2021.pt')\n",
    "gru_model3.eval()\n",
    "print(\"loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(225, 5, 10)\n",
      "(5, 10)\n",
      "(5, 3)\n",
      "[[0.62650489 0.6240539  0.62487616 0.61841971 0.60647277 0.61208515\n",
      "  0.60381624 0.58739215]\n",
      " [0.67176594 0.69677379 0.66764657 0.69311813 0.68261415 0.69368848\n",
      "  0.67709365 0.71144774]\n",
      " [0.96183085 0.95093648 0.96197414 0.95185947 0.9806985  0.96233089\n",
      "  0.96227891 0.95703438]\n",
      " [0.3598586  0.34195887 0.35935777 0.3407809  0.3497445  0.33461793\n",
      "  0.34390142 0.32549016]\n",
      " [0.66796758 0.66580207 0.66435866 0.66106838 0.65530888 0.65823256\n",
      "  0.65153814 0.64373128]]\n"
     ]
    }
   ],
   "source": [
    "print(test_x.shape)\n",
    "print(test_x[207].shape)\n",
    "print(test_x[207, 0:5, 0:3].shape)\n",
    "print(test_x[207, 0:5, 0:8])\n",
    "#print(test_x[207][0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index= 111\n",
      "(1, 5, 10)\n",
      "model output tensor([[0.4690]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "actual [1.]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "randomindex=random.randint(0,225)\n",
    "print(\"index=\",randomindex)\n",
    "\n",
    "exampledata=np.expand_dims(test_x[207, 0:5, 0:10], axis=0)\n",
    "\n",
    "print(exampledata.shape)\n",
    "evaluate_episode(gru_model3, exampledata)\n",
    "\n",
    "print(\"actual\",test_y[randomindex])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index= 135\n",
      "final partial data\n",
      "[[[0.         0.26092697 0.2792133  0.25953264 0.27325137 0.22796102\n",
      "   0.23587776 0.24253487 0.24977378 0.25088221]\n",
      "  [0.         0.45486988 0.47709246 0.46515867 0.47735784 0.4838517\n",
      "   0.51791774 0.54748478 0.5798511  0.61216246]\n",
      "  [0.         0.932468   0.93630157 0.94852524 0.94791474 0.94302117\n",
      "   0.94593028 0.95207634 0.94545011 0.95279266]\n",
      "  [0.         0.54036542 0.52477759 0.53353251 0.52547211 0.51902835\n",
      "   0.49752792 0.47108685 0.44457922 0.40975893]\n",
      "  [0.         0.34477817 0.35262265 0.33816335 0.34499276 0.32279125\n",
      "   0.32330711 0.32704192 0.32809967 0.33067307]]]\n",
      "\n",
      "full data\n",
      "[[[0.26092697 0.2792133  0.25953264 0.27325137 0.22796102 0.23587776\n",
      "   0.24253487 0.24977378 0.25088221 0.24520898]\n",
      "  [0.45486988 0.47709246 0.46515867 0.47735784 0.4838517  0.51791774\n",
      "   0.54748478 0.5798511  0.61216246 0.62405565]\n",
      "  [0.932468   0.93630157 0.94852524 0.94791474 0.94302117 0.94593028\n",
      "   0.95207634 0.94545011 0.95279266 0.94208762]\n",
      "  [0.54036542 0.52477759 0.53353251 0.52547211 0.51902835 0.49752792\n",
      "   0.47108685 0.44457922 0.40975893 0.39095052]\n",
      "  [0.34477817 0.35262265 0.33816335 0.34499276 0.32279125 0.32330711\n",
      "   0.32704192 0.32809967 0.33067307 0.32670226]]]\n",
      "predicitions: [-0.0940731018781662, -0.02917778491973877, 0.12347681820392609, 0.07153928279876709, 0.03629066050052643, 0.0813649445772171, 0.23030498623847961, 0.40360528230667114, 0.22185015678405762]\n",
      "\n",
      "\n",
      "prediction from 10 timesteps 0.013949915766716003 actual [0.]\n"
     ]
    }
   ],
   "source": [
    "outputlist=[]\n",
    "\n",
    "randomindex=random.randint(0,225)\n",
    "print(\"index=\",randomindex)\n",
    "exampledata=np.expand_dims(test_x[randomindex, 0:5, 0:10], axis=0)\n",
    "\n",
    "\n",
    "#print(temparray.shape)\n",
    "#temparray=np.expand_dims(temparray, axis=1)\n",
    "\n",
    "#print(temparray.shape)\n",
    "#print(temparray)\n",
    "\n",
    "#temparray2=test_x[randomindex, 0:5, 0]\n",
    "#temparray2=np.expand_dims(temparray2, axis=1)\n",
    "\n",
    "for i in range(9):\n",
    "    if i!=10:\n",
    "        temparray=np.zeros((5,1)) #test_x[randomindex, 0:5, 0]\n",
    "    \n",
    "    for j in range(8-i):\n",
    "        #temparray2=test_x[randomindex, 0:5, 0]\n",
    "        #temparray2=np.expand_dims(temparray2, axis=1)\n",
    "        temparray=np.append(temparray,np.zeros((5,1)),axis=1)\n",
    "        #temparray=np.append(temparray,temparray2,axis=1)\n",
    "        #temparray=np.append(np.zeros((5,1)),temparray,axis=1)   \n",
    "    \n",
    "    for j in range(i+1):\n",
    "\n",
    "        temparray2=test_x[randomindex, 0:5, j]\n",
    "        temparray2=np.expand_dims(temparray2, axis=1)\n",
    "        temparray=np.append(temparray,temparray2,axis=1)\n",
    "        #temparray=np.append(np.zeros((5,1)),temparray,axis=1)\n",
    "\n",
    "    \n",
    "    #print(temparray)\n",
    "    temparray=np.expand_dims(temparray, axis=0)\n",
    "    outputpartial=evaluate_episode(gru_model3, temparray)\n",
    "    \n",
    "    \n",
    "    outputlist.append(float(outputpartial))\n",
    "\n",
    "print(\"final partial data\")\n",
    "print(temparray)   \n",
    "print(\"\")\n",
    "print(\"full data\")\n",
    "print(exampledata)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#print(\"prediction from\",x,\" timesteps\",float(outputpartial),\"actual\",test_y[randomindex])\n",
    "print(\"predicitions:\",outputlist)\n",
    "\n",
    "\n",
    "#print(\"full data\")\n",
    "#print(exampledata)\n",
    "print(\"\")\n",
    "#print(\"evaluating all 10 timesteps\")\n",
    "\n",
    "outputfull=evaluate_episode(gru_model3, exampledata)\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "#print(\"evaluating 1st timestep repeated 10 times\")\n",
    "\n",
    "\n",
    "#print(\"prediction from 1 timestep\",float(outputpartial),\"actual\",test_y[randomindex])\n",
    "print(\"prediction from 10 timesteps\",float(outputfull),\"actual\",test_y[randomindex])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 5, 10)\n",
      "(1, 5, 10)\n"
     ]
    }
   ],
   "source": [
    "print(exampledata.shape)\n",
    "print(temparray.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model output tensor([[1.1806]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "actual [0.]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
