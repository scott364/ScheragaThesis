{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we'll be using a GRU model for a time series prediction task and we will compare the performance of the GRU model against an LSTM model as well. The dataset that we will be using is the Hourly Energy Consumption dataset which can be found on [Kaggle](https://www.kaggle.com/robikscube/hourly-energy-consumption). The dataset contains power consumption data across different regions around the United States recorded on an hourly basis.\n",
    "\n",
    "You can run the code implementation in this article on FloydHub using their GPUs on the cloud by clicking the following link and using the main.ipynb notebook.\n",
    "\n",
    "[![Run on FloydHub](https://static.floydhub.com/button/button-small.svg)](https://floydhub.com/run?template=https://github.com/gabrielloye/https://github.com/gabrielloye/GRU_Prediction)\n",
    "\n",
    "This will speed up the training process significantly. Alternatively, the link to the GitHub repository can be found [here]().\n",
    "\n",
    "The goal of this implementation is to create a model that can accurately predict the energy usage in the next hour given historical usage data. We will be using both the GRU and LSTM model to train on a set of historical data and evaluate both models on an unseen test set. To do so, weâ€™ll start with feature selection, data-preprocessing, followed by defining, training and eventually evaluating the models.\n",
    "\n",
    "We will be using the PyTorch library to implement both types of models along with other common Python libraries used in data analytics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "In /home/scott/.local/lib/python3.6/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The text.latex.preview rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /home/scott/.local/lib/python3.6/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The mathtext.fallback_to_cm rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /home/scott/.local/lib/python3.6/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: Support for setting the 'mathtext.fallback_to_cm' rcParam is deprecated since 3.3 and will be removed two minor releases later; use 'mathtext.fallback : 'cm' instead.\n",
      "In /home/scott/.local/lib/python3.6/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The validate_bool_maybe_none function was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /home/scott/.local/lib/python3.6/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The savefig.jpeg_quality rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /home/scott/.local/lib/python3.6/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The keymap.all_axes rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /home/scott/.local/lib/python3.6/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The animation.avconv_path rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /home/scott/.local/lib/python3.6/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The animation.avconv_args rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n"
     ]
    }
   ],
   "source": [
    "#https://www.python-engineer.com/posts/pytorch-rnn-lstm-gru/\n",
    "\n",
    "#https://blog.floydhub.com/gru-with-pytorch/\n",
    "\n",
    "import os\n",
    "import time\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import random\n",
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from tqdm import tqdm_notebook\n",
    "from sklearn.preprocessing import MinMaxScaler,QuantileTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Local ###\n",
    "#from data_processing import *\n",
    "\n",
    "\n",
    "\n",
    "# Define data root directory\n",
    "\n",
    "#data_dir = \"./data/\"\n",
    "#print(os.listdir(data_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a total of **12** *.csv* files containing hourly energy trend data (*'est_hourly.paruqet'* and *'pjm_hourly_est.csv'* are not used). In our next step, we will be reading these files and pre-processing these data in this order:\n",
    "- Getting the time data of each individual time step and generalizing them\n",
    "    - Hour of the day *i.e. 0-23*\n",
    "    - Day of the week *i.e. 1-7*\n",
    "    - Month *i.e. 1-12*\n",
    "    - Day of the year *i.e. 1-365*\n",
    "    \n",
    "    \n",
    "- Scale the data to values between 0 and 1\n",
    "    - Algorithms tend to perform better or converge faster when features are on a relatively similar scale and/or close to normally distributed\n",
    "    - Scaling preserves the shape of the original distribution and doesn't reduce the importance of outliers.\n",
    "    \n",
    "    \n",
    "- Group the data into sequences to be used as inputs to the model and store their corresponding labels\n",
    "    - The **sequence length** or **lookback period** is the number of data points in history that the model will use to make the prediction\n",
    "    - The label will be the next data point in time after the last one in the input sequence\n",
    "    \n",
    "\n",
    "- The inputs and labels will then be split into training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1440, 10)\n",
      "(3960, 10)\n",
      "(5400, 10)\n"
     ]
    }
   ],
   "source": [
    "#choppeddata=pd.read_csv('choppeddata_10_06_2021.csv')#.head()\n",
    "choppeddata1=pd.read_csv('choppeddata_10_04_2021_equalsuccessfail.csv')#.head()\n",
    "choppeddata2=pd.read_csv('choppeddata_10_06_2021_equalsuccessfail.csv')#.head()\n",
    "print(choppeddata1.shape)\n",
    "print(choppeddata2.shape)\n",
    "frames = [choppeddata1, choppeddata2]\n",
    "choppeddata = pd.concat(frames)\n",
    "print(choppeddata.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5400, 10)\n",
      "total runs: 900\n"
     ]
    }
   ],
   "source": [
    "print(choppeddata.shape)\n",
    "runqty=int(choppeddata.shape[0]/6)\n",
    "print(\"total runs:\",runqty)\n",
    "choppedheaders=[]\n",
    "lookback=10 #save only the last 11 timesteps\n",
    "for i in range(lookback):  \n",
    "    label=str(i)\n",
    "    choppedheaders.append(\"header\"+label)\n",
    "\n",
    "#put chopped data in np.arrays\n",
    "State=np.zeros((runqty,5,lookback)) #96 runs,with 5 sets of data (x,y,z,roll,pitch) each, and each run is 11 timesteps long\n",
    "Labels=np.zeros((runqty,lookback)) #96 runs, each run is 11 timesteps long\n",
    "runcounter=0\n",
    "\n",
    "for i in range(0,choppeddata.shape[0],6):\n",
    "            State[runcounter][0][:]=(choppeddata[choppedheaders[:]].iloc[i]).tolist()\n",
    "            State[runcounter][1][:]=(choppeddata[choppedheaders[:]].iloc[i+1]).tolist()\n",
    "            State[runcounter][2][:]=(choppeddata[choppedheaders[:]].iloc[i+2]).tolist()\n",
    "            State[runcounter][3][:]=(choppeddata[choppedheaders[:]].iloc[i+3]).tolist()\n",
    "            State[runcounter][4][:]=(choppeddata[choppedheaders[:]].iloc[i+4]).tolist()\n",
    "            Labels[runcounter][:]=(choppeddata[choppedheaders[:]].iloc[i+5]).tolist()  #labels   \n",
    "            runcounter+=1\n",
    "#print(State[0])\n",
    "#print(Labels)\n",
    "#print(Labels[:,9]) #just getting finals labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(900, 5, 10)\n",
      "Train\n",
      "[[-0.19238566 -0.02383468 -0.07998902 -0.02979822 -0.04594777 -0.05552694\n",
      "  -0.00953972 -0.06174315 -0.01376076 -0.07650416]\n",
      " [ 0.07594457 -0.49520655 -0.70752082 -0.76173129 -0.83318186 -0.76271486\n",
      "  -0.76788306 -0.82402039 -0.88008633 -0.91614084]\n",
      " [ 0.21220583 -0.31679585  0.09686737  0.05429729  0.11288494  0.08522151\n",
      "   0.18954683 -0.00827614  0.18382697 -0.56520485]\n",
      " [-0.01103151  0.37420186  0.54593194  0.58206958  0.63715643  0.60640562\n",
      "   0.67022455  0.66541171  0.72991329  0.70952725]\n",
      " [ 0.07124055  0.14689781  0.10506888  0.15166578  0.130053    0.1298158\n",
      "   0.14028129  0.12078944  0.14751588  0.07916366]]\n",
      "[1.]\n",
      "Train set Y size 675\n",
      "Test\n",
      "[[-0.05044708  0.08366547  0.11166233  0.06932041 -0.00482253 -0.01134842\n",
      "  -0.01176118  0.00110171  0.03080695  0.03282975]\n",
      " [ 0.64864936  0.65490055  0.71353354  0.75746112  0.7687305   0.76905785\n",
      "   0.86623154  0.89811239  0.86052513  0.7746829 ]\n",
      " [ 0.00476006  0.14399977  0.19675119  0.20224665  0.29294198  0.38857246\n",
      "   0.21847955  0.31503162  0.22700461 -0.2209304 ]\n",
      " [-0.44953737 -0.55507404 -0.57936031 -0.59127176 -0.60967189 -0.60783333\n",
      "  -0.67101556 -0.69860238 -0.72800803 -0.6890893 ]\n",
      " [ 0.0990776   0.2457677   0.27215838  0.24126945  0.1821969   0.19505425\n",
      "   0.16850132  0.19108477  0.21263117  0.19441143]]\n",
      "[1.]\n",
      "Test set Y size 225\n"
     ]
    }
   ],
   "source": [
    "#X= range(0,575,6)\n",
    "#y= range(0,575,6)\n",
    "\n",
    "X=State\n",
    "y=Labels[:,lookback-1]\n",
    "print(X.shape)\n",
    "\n",
    "y=y.reshape(runqty,1)\n",
    "\n",
    "random_seed=int(time.time())\n",
    "#print(int(time.time()))\n",
    "train_x, test_x, train_y,test_y = train_test_split(X, y, test_size=.25, #0.33, \n",
    "                                                   random_state=random_seed)\n",
    "print(\"Train\")\n",
    "print(train_x[0])\n",
    "print(train_y[0])\n",
    "print(\"Train set Y size\", train_y.size)\n",
    "print(\"Test\")\n",
    "print(test_x[0])\n",
    "print(test_y[0])\n",
    "print(\"Test set Y size\", test_y.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a total of 980,185 sequences of training data\n",
    "\n",
    "To improve the speed of our training, we can process the data in batches so that the model does not need to update its weights as frequently. The Torch *Dataset* and *DataLoader* classes are useful for splitting our data into batches and shuffling them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "#a batch size of 64, i.e. each element in the dataloader iterable will return a batch of 64 features and labels.\n",
    "\n",
    "train_data = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size, drop_last=True)\n",
    "\n",
    "test_data   = TensorDataset( torch.from_numpy( test_x ), torch.from_numpy( test_y ) )\n",
    "test_loader = DataLoader( test_data, shuffle = True, batch_size = batch_size, drop_last = True )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataloader.DataLoader object at 0x7fcca2b99e80>\n"
     ]
    }
   ],
   "source": [
    "print(train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also check if we have any GPUs to speed up our training time by many folds. If youâ€™re using FloydHub with GPU to run this code, the training time will be significantly reduced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# torch.cuda.is_available() checks and returns a Boolean True if a GPU is available, else it'll return False\n",
    "is_cuda = torch.cuda.is_available()\n",
    "\n",
    "# If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.\n",
    "if is_cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    \n",
    "print(device)\n",
    "\n",
    "\n",
    "def get_torch_device( v=0 ):\n",
    "    # torch.cuda.is_available() checks and returns a Boolean True if a GPU is available, else it'll return False\n",
    "    is_cuda = torch.cuda.is_available()\n",
    "    # If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.\n",
    "    if is_cuda:\n",
    "        device = torch.device(\"cuda\")\n",
    "        if v:  print( \"CUDA Available!\" )\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        if v:  print( \"NO CUDA\" )\n",
    "    return device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUNet(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, n_layers, drop_prob=0.2):\n",
    "        super(GRUNet, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.gru = nn.GRU(input_dim, hidden_dim, n_layers, batch_first=True, dropout=drop_prob)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x, h):\n",
    "        out, h = self.gru(x, h)\n",
    "        out = self.fc(self.relu(out[:,-1]))\n",
    "        return out, h\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        hidden = weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device)\n",
    "        return hidden\n",
    "def train(train_loader, learn_rate, hidden_dim=128, EPOCHS=400, model_type=\"GRU\"):\n",
    "    #got  109 / 180 on training set, 29 / 60 on test set from 128 hidden dim, 50 epoch, batch size of 4, lr =0.001\n",
    "    #Got training data= 146 / 180, success vs test data= 38 / 60 with same as above but 100 epoch\n",
    "    #Got training data= 172 / 180, success vs test data= 46 / 60 with same as above but 200 epoch\n",
    "    #Got training data= 165 / 180, success vs test data= 52 / 60 with same as above but 200 epoch\n",
    "    \n",
    "    losslist=[]\n",
    "    # Setting common hyperparameters\n",
    "    input_dim = next(iter(train_loader))[0].shape[2]  #  = 11\n",
    "    #print(input_dim)\n",
    "    #print(\"input_dim\",input_dim)\n",
    "    output_dim = 1\n",
    "    n_layers = 2\n",
    "    # Instantiating the models\n",
    "    if model_type == \"GRU\":\n",
    "        model = GRUNet(input_dim, hidden_dim, output_dim, n_layers)\n",
    "    else:\n",
    "        model = LSTMNet(input_dim, hidden_dim, output_dim, n_layers)\n",
    "    model.to(device)\n",
    "    \n",
    "    # Defining loss function and optimizer\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learn_rate)\n",
    "    \n",
    "    model.train()\n",
    "    print(\"Starting Training of {} model\".format(model_type))\n",
    "    epoch_times = []\n",
    "    # Start training loop\n",
    "    for epoch in range(1,EPOCHS+1):\n",
    "        start_time = time.clock()\n",
    "        h = model.init_hidden(batch_size)\n",
    "        avg_loss = 0.\n",
    "        counter = 0\n",
    "        for x, label in train_loader:\n",
    "            #print(\"x\",x)\n",
    "            #print(\"label\",label)\n",
    "            counter += 1\n",
    "            if model_type == \"GRU\":\n",
    "                h = h.data\n",
    "            else:\n",
    "                h = tuple([e.data for e in h])\n",
    "            model.zero_grad()\n",
    "            \n",
    "            out, h = model(x.to(device).float(), h)\n",
    "            #print(\"out\",out)\n",
    "            loss = criterion(out, label.to(device).float())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            avg_loss += loss.item()\n",
    "            \n",
    "            if counter%20000 == 0:\n",
    "                print(\"Epoch {}......Step: {}/{}....... Average Loss for Epoch: {}\".format(epoch, counter, len(train_loader), avg_loss/counter))\n",
    "        current_time = time.clock()\n",
    "        if epoch%40 == 0:\n",
    "            print(\"Epoch {}/{} Done, Total Loss: {}   Time Elapsed: {} seconds\".format(epoch, EPOCHS, avg_loss/len(train_loader),str(current_time-start_time)))\n",
    "        \n",
    "            #print(\"Total\".format())\n",
    "        losslist.append(avg_loss/len(train_loader))\n",
    "        epoch_times.append(current_time-start_time)\n",
    "    print(\"Total Training Time: {} seconds\".format(str(sum(epoch_times))))\n",
    "    plt.plot(losslist)\n",
    "    plt.title(\"Loss\")\n",
    "    plt.show()\n",
    "    return model\n",
    "\n",
    "def evaluate(model, test_x, test_y):\n",
    "    model.eval()\n",
    "    outputs = []\n",
    "    targets = []\n",
    "    start_time = time.clock()\n",
    "    #for i in test_x.keys():\n",
    "    for i in range( len( test_x ) ):    \n",
    "        inp = torch.from_numpy(np.array(test_x[i])) # should be 5x1\n",
    "        labs = torch.from_numpy(np.array(test_y[i])) #should be 1x1\n",
    "        h = model.init_hidden(inp.shape[0])\n",
    "        #print(\"inp\",inp)\n",
    "        #print(\"labs\",labs)\n",
    "        #print(\"h\",h)\n",
    "        out, h = model(inp.to(device).float(), h)\n",
    "        #outputs.append(label_scalers[i].inverse_transform(out.cpu().detach().numpy()).reshape(-1))\n",
    "        #targets.append(label_scalers[i].inverse_transform(labs.numpy()).reshape(-1))\n",
    "        outputs.append( out.cpu().detach().numpy().reshape(-1) )\n",
    "        targets.append( labs.numpy().reshape(-1) )\n",
    "        \n",
    "    print(\"Evaluation Time: {}\".format(str(time.clock()-start_time)))\n",
    "    sMAPE = 0\n",
    "    for i in range(len(outputs)):\n",
    "        sMAPE += np.mean(abs(outputs[i]-targets[i])/(targets[i]+outputs[i])/2)/len(outputs)\n",
    "    print(\"sMAPE: {}%\".format(sMAPE*100))\n",
    "    return outputs, targets, sMAPE\n",
    "                               \n",
    "def evaluate2(model, test_x, test_y):\n",
    "    model.eval()\n",
    "    outputs = []\n",
    "    targets = []\n",
    "    start_time = time.clock()\n",
    "    #for i in test_x.keys():\n",
    "    #for i in range( len( test_x ) ):    \n",
    "    inp = torch.from_numpy(np.array(test_x)) # should be 5x1\n",
    "    labs = torch.from_numpy(np.array(test_y)) #should be 1x1\n",
    "    h = model.init_hidden(inp.shape[0])\n",
    "    #print(\"inp\",inp)\n",
    "    #print(\"labs\",labs)\n",
    "    #print(\"h\",h)\n",
    "    out, h = model(inp.to(device).float(), h)\n",
    "    #outputs.append(label_scalers[i].inverse_transform(out.cpu().detach().numpy()).reshape(-1))\n",
    "    #targets.append(label_scalers[i].inverse_transform(labs.numpy()).reshape(-1))\n",
    "    outputs.append( out.cpu().detach().numpy().reshape(-1) )\n",
    "    targets.append( labs.numpy().reshape(-1) )\n",
    "        \n",
    "    print(\"Evaluation Time: {}\".format(str(time.clock()-start_time)))\n",
    "    sMAPE = 0\n",
    "    for i in range(len(outputs)):\n",
    "        sMAPE += np.mean(abs(outputs[i]-targets[i])/(targets[i]+outputs[i])/2)/len(outputs)\n",
    "    print(\"sMAPE: {}%\".format(sMAPE*100))\n",
    "    return outputs, targets, sMAPE                               \n",
    "\n",
    "def evaluatefull(model, train_x, train_y, test_x, test_y,maxdifference=0.2, verbose=False):\n",
    "\n",
    "    m = nn.ReLU()\n",
    "    #m = nn.Sigmoid()\n",
    "    #output = m(input)\n",
    "    print(\"Vs Training Set\")\n",
    "    gru_outputs, targets, gru_sMAPE = evaluate2(gru_model, train_x, train_y)\n",
    "    #print(test_y)\n",
    "    #print(gru_outputs)\n",
    "    #print(gru_outputs[0][5])\n",
    "\n",
    "\n",
    "    testy=test_y.reshape(-1)\n",
    "    trainy=train_y.reshape(-1)\n",
    "\n",
    "\n",
    "    #print(\"Train size:\",trainy.size)\n",
    "    print(gru_outputs[0][4])\n",
    "    train_successcounter=0\n",
    "    for i in range(int(trainy.size)):\n",
    "        #print(testy[i],gru_outputs[0][i],m(torch.tensor(gru_outputs[0][i])))\n",
    "        #print(train[i],gru_outputs[0][i],m(torch.tensor(gru_outputs[0][i])))\n",
    "\n",
    "\n",
    "        #print(trainy[i],gru_outputs[0][i], m(torch.tensor(gru_outputs[0][i])))\n",
    "\n",
    "\n",
    "        if abs(trainy[i]-gru_outputs[0][i])<maxdifference :\n",
    "            train_successcounter+=1\n",
    "        #print(testy[i])\n",
    "        #print\n",
    "        #output = m(input)\n",
    "\n",
    "\n",
    "\n",
    "    test_successcounter=0\n",
    "    print(\"Vs Test Set\")\n",
    "    gru_outputs, targets, gru_sMAPE = evaluate2(gru_model, test_x, test_y)\n",
    "    #print(\"test size: \",testy.size)\n",
    "\n",
    "    for i in range(int(testy.size)):\n",
    "\n",
    "\n",
    "        #, m(torch.tensor(gru_outputs[0][i])))\n",
    "\n",
    "\n",
    "        if abs(testy[i]-gru_outputs[0][i])<maxdifference :\n",
    "            test_successcounter+=1\n",
    "            if verbose==True:\n",
    "                print(testy[i],gru_outputs[0][i], \"OK\" )\n",
    "        else:\n",
    "            if verbose==True:\n",
    "                print(testy[i],gru_outputs[0][i], \"X\" )\n",
    "            #print(testy[i])\n",
    "        #print\n",
    "        #output = m(input)\n",
    "    print(\"\")\n",
    "    print(\" vs training data=\" ,train_successcounter,\"/\",trainy.size, \" vs test data=\" ,\n",
    "          test_successcounter,\"/\",testy.size,int(100*test_successcounter/testy.size),\"%\", \"at max difference\",maxdifference )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.25\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "print(100*5/80)\n",
    "print(int(100*5/80))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training of GRU model\n",
      "Epoch 40/400 Done, Total Loss: 0.0734289588184592   Time Elapsed: 0.37645999999995183 seconds\n",
      "Epoch 80/400 Done, Total Loss: 0.036210386135713805   Time Elapsed: 0.3761469999999463 seconds\n",
      "Epoch 120/400 Done, Total Loss: 0.03219078792201236   Time Elapsed: 0.36603599999989456 seconds\n",
      "Epoch 160/400 Done, Total Loss: 0.03648303390539132   Time Elapsed: 0.3415789999999106 seconds\n",
      "Epoch 200/400 Done, Total Loss: 0.025179715645338416   Time Elapsed: 0.37473099999988335 seconds\n",
      "Epoch 240/400 Done, Total Loss: 0.028706216309902152   Time Elapsed: 0.37631099999998696 seconds\n",
      "Epoch 280/400 Done, Total Loss: 0.029606157117086696   Time Elapsed: 0.3785760000000664 seconds\n",
      "Epoch 320/400 Done, Total Loss: 0.027844558575993904   Time Elapsed: 0.389889999999923 seconds\n",
      "Epoch 360/400 Done, Total Loss: 0.020213136052548396   Time Elapsed: 0.3669790000001285 seconds\n",
      "Epoch 400/400 Done, Total Loss: 0.03600557470982884   Time Elapsed: 0.38266199999998207 seconds\n",
      "Total Training Time: 147.7044869999986 seconds\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vs Training Set\n",
      "Evaluation Time: 0.0015390000000934378\n",
      "sMAPE: 4.451804156945485%\n",
      "1.0452454\n",
      "Vs Test Set\n",
      "Evaluation Time: 0.0007640000001174485\n",
      "sMAPE: 12.096758754202224%\n",
      "\n",
      " vs training data= 588 / 675  vs test data= 175 / 225 77 % at max difference 0.2\n",
      "Vs Training Set\n",
      "Evaluation Time: 0.0010729999999057327\n",
      "sMAPE: 4.451804156945485%\n",
      "1.0452454\n",
      "Vs Test Set\n",
      "Evaluation Time: 0.0008039999997890845\n",
      "sMAPE: 12.096758754202224%\n",
      "\n",
      " vs training data= 626 / 675  vs test data= 187 / 225 83 % at max difference 0.3\n",
      "Vs Training Set\n",
      "Evaluation Time: 0.0014900000001034641\n",
      "sMAPE: 4.451804156945485%\n",
      "1.0452454\n",
      "Vs Test Set\n",
      "Evaluation Time: 0.0006240000000161672\n",
      "sMAPE: 12.096758754202224%\n",
      "\n",
      " vs training data= 637 / 675  vs test data= 197 / 225 87 % at max difference 0.4\n",
      "Vs Training Set\n",
      "Evaluation Time: 0.0011010000000624132\n",
      "sMAPE: 4.451804156945485%\n",
      "1.0452454\n",
      "Vs Test Set\n",
      "Evaluation Time: 0.0007739999998648273\n",
      "sMAPE: 12.096758754202224%\n",
      "\n",
      " vs training data= 648 / 675  vs test data= 204 / 225 90 % at max difference 0.5\n"
     ]
    }
   ],
   "source": [
    "lr = 0.002\n",
    "gru_model = train(train_loader, lr , hidden_dim=128, EPOCHS=400, model_type=\"GRU\")\n",
    "evaluatefull(gru_model, train_x, train_y, test_x, test_y,maxdifference=.2)\n",
    "evaluatefull(gru_model, train_x, train_y, test_x, test_y,maxdifference=.3)\n",
    "evaluatefull(gru_model, train_x, train_y, test_x, test_y,maxdifference=.4)\n",
    "evaluatefull(gru_model, train_x, train_y, test_x, test_y,maxdifference=.5)\n",
    "#vs training data= 490 / 495  vs test data= 128 / 165 (77%) from 128 hidden dim,  batch size of 4, lr =0.001,test_size=.25,  400 epoch\n",
    "#vs training data= 492 / 495  vs test data= 130 / 165 (78%) from 128 hidden dim,  batch size of 4, lr =0.0005,test_size=.25,  400 epoch\n",
    "#vs training data= 411 / 495  vs test data= 103 / 165 62%) from 128 hidden dim,  batch size of 4, lr =0.0001,test_size=.25,  400 epoch\n",
    "#vs training data= 451 / 495  vs test data= 113 / 165 68%) from 128 hidden dim,  batch size of 4, lr =0.0001,test_size=.25,  600 epoch\n",
    "#vs training data= 472 / 495  vs test data= 133 / 165 80%) from 128 hidden dim,  batch size of 4, lr =0.002,test_size=.25,  600 epoch\n",
    "\n",
    "\n",
    "#vs training data= 472 / 495  vs test data= 133 / 165 80%) from 128 hidden dim,  batch size of 4, lr =0.002,test_size=.25,  400 epoch\n",
    "#vs training data= 487 / 495  vs test data= 134 / 165 81% at 0.2 cuttoff\n",
    " #vs training data= 490 / 495  vs test data= 142 / 165 86.06060606060606 % at 0.3 cuttoff\n",
    " # vs training data= 491 / 495  vs test data= 146 / 165 88.48484848484848 % at 0.4 cuttoff\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#With full data set (uneven amts of success/fail) from 128 hidden dim,  batch size of 4, lr =0.0005,test_size=.25,  400 epoch\n",
    "#vs training data= 893 / 900  vs test data= 227 / 300 75 % at max difference 0.2\n",
    "# vs training data= 898 / 900  vs test data= 238 / 300 79 % at max difference 0.3\n",
    "# vs training data= 900 / 900  vs test data= 240 / 300 80 % at max difference 0.4\n",
    "# vs training data= 900 / 900  vs test data= 243 / 300 81 % at max difference 0.5\n",
    "\n",
    "\n",
    "#with even success/fail amounts (10-6):\n",
    "\n",
    "# vs training data= 494 / 495  vs test data= 131 / 165 79 % at max difference 0.2\n",
    "# vs training data= 495 / 495  vs test data= 138 / 165 83 % at max difference 0.3\n",
    "# vs training data= 495 / 495  vs test data= 140 / 165 84 % at max difference 0.4\n",
    "# vs training data= 495 / 495  vs test data= 144 / 165 87 % at max difference 0.5\n",
    "\n",
    "#with even success/fail amounts (10-4 and 10-6): (batch size of 4\n",
    "# vs training data= 653 / 675  vs test data= 168 / 225 74 % at max difference 0.2\n",
    "#vs training data= 663 / 675  vs test data= 180 / 225 80 % at max difference 0.3\n",
    "# vs training data= 667 / 675  vs test data= 189 / 225 84 % at max difference 0.4\n",
    "#vs training data= 670 / 675  vs test data= 196 / 225 87 % at max difference 0.5\n",
    "\n",
    "#with even success/fail amounts (10-4 and 10-6) (batch size of 8):\n",
    "# vs training data= 670 / 675  vs test data= 174 / 225 77 % at max difference 0.2\n",
    "# vs training data= 673 / 675  vs test data= 190 / 225 84 % at max difference 0.3\n",
    "# vs training data= 675 / 675  vs test data= 191 / 225 84 % at max difference 0.4\n",
    "# vs training data= 675 / 675  vs test data= 193 / 225 85 % at max difference 0.5\n",
    "\n",
    "#with even success/fail amounts (10-4 and 10-6) (batch size of 2):\n",
    "# vs training data= 662 / 675  vs test data= 178 / 225 79 % at max difference 0.2\n",
    "# vs training data= 673 / 675  vs test data= 186 / 225 82 % at max difference 0.3\n",
    "# vs training data= 673 / 675  vs test data= 191 / 225 84 % at max difference 0.4\n",
    "# vs training data= 674 / 675  vs test data= 199 / 225 88 % at max difference 0.5\n",
    "\n",
    "#with even success/fail amounts (10-4 and 10-6) (batch size of 2, lr=0.0005):\n",
    "# vs training data= 673 / 675  vs test data= 175 / 225 77 % at max difference 0.2\n",
    "# vs training data= 674 / 675  vs test data= 189 / 225 84 % at max difference 0.3\n",
    "# vs training data= 674 / 675  vs test data= 197 / 225 87 % at max difference 0.4\n",
    "# vs training data= 675 / 675  vs test data= 204 / 225 90 % at max difference 0.5\n",
    "\n",
    "#with even success/fail amounts (10-4 and 10-6) (batch size of 4, lr=0.002):\n",
    "# vs training data= 588 / 675  vs test data= 175 / 225 77 % at max difference 0.2\n",
    "#vs training data= 626 / 675  vs test data= 187 / 225 83 % at max difference 0.3\n",
    "## vs training data= 637 / 675  vs test data= 197 / 225 87 % at max difference 0.4\n",
    "# vs training data= 648 / 675  vs test data= 204 / 225 90 % at max difference 0.5\n",
    "\n",
    "#The target size means the label size!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vs Training Set\n",
      "Evaluation Time: 0.010782000000062908\n",
      "sMAPE: -2.6715693007246553%\n",
      "-0.006552458\n",
      "Vs Test Set\n",
      "Evaluation Time: 0.0007630000000062864\n",
      "sMAPE: 10.777743848216746%\n",
      "\n",
      " vs training data= 893 / 900  vs test data= 227 / 300 75 % at max difference 0.2\n",
      "Vs Training Set\n",
      "Evaluation Time: 0.0018829999999070424\n",
      "sMAPE: -2.6715693007246553%\n",
      "-0.006552458\n",
      "Vs Test Set\n",
      "Evaluation Time: 0.0007460000001628941\n",
      "sMAPE: 10.777743848216746%\n",
      "\n",
      " vs training data= 898 / 900  vs test data= 238 / 300 79 % at max difference 0.3\n",
      "Vs Training Set\n",
      "Evaluation Time: 0.0018939999999929569\n",
      "sMAPE: -2.6715693007246553%\n",
      "-0.006552458\n",
      "Vs Test Set\n",
      "Evaluation Time: 0.0007259999999860156\n",
      "sMAPE: 10.777743848216746%\n",
      "\n",
      " vs training data= 900 / 900  vs test data= 240 / 300 80 % at max difference 0.4\n",
      "Vs Training Set\n",
      "Evaluation Time: 0.001856999999972686\n",
      "sMAPE: -2.6715693007246553%\n",
      "-0.006552458\n",
      "Vs Test Set\n",
      "Evaluation Time: 0.000722999999879903\n",
      "sMAPE: 10.777743848216746%\n",
      "\n",
      " vs training data= 900 / 900  vs test data= 243 / 300 81 % at max difference 0.5\n"
     ]
    }
   ],
   "source": [
    "evaluatefull(gru_model, train_x, train_y, test_x, test_y,maxdifference=.2)\n",
    "evaluatefull(gru_model, train_x, train_y, test_x, test_y,maxdifference=.3)\n",
    "evaluatefull(gru_model, train_x, train_y, test_x, test_y,maxdifference=.4)\n",
    "evaluatefull(gru_model, train_x, train_y, test_x, test_y,maxdifference=.5)#,verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=gru_model\n",
    "i=1\n",
    "inp = torch.from_numpy(np.array(test_x))\n",
    "labs = torch.from_numpy(np.array(test_y))\n",
    "#h = model.init_hidden(inp.shape[0])\n",
    "h = model.init_hidden(inp.shape[0])\n",
    "#print(\"inp\",inp)\n",
    "\n",
    "#print(\"INP SHAPE\",inp.shape)\n",
    "#print(\"INP SHAPE[0]\",inp.shape[0])\n",
    "#print(\"labs\",labs)\n",
    "#print(\"h\",h)\n",
    "#print(\"h.shape\",h.shape)\n",
    "#print(inp.to(device).float())\n",
    "#print(inp.to(device).float().shape)\n",
    "print(inp.to(device).float().shape)\n",
    "\n",
    "out, h = model(inp.to(device).float(), h)\n",
    "#print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0    \n",
    "print(testy[i])\n",
    "print(gru_outputs[0][i])\n",
    "print(np.abs(testy[i]-gru_outputs[0][i]))\n",
    "print(np.abs(testy[i]-gru_outputs[0][i])<0.2)\n",
    "\n",
    "print(trainy[i],gru_outputs[0][i] )\n",
    "\n",
    "\"\"\"\n",
    "if np.abs(testy[i]-gru_outputs[0][i])<0.2 :\n",
    "   \n",
    "    print(trainy[i],gru_outputs[0][i], \"OK\" )\n",
    "else:\n",
    "    print(trainy[i],gru_outputs[0][i], \"X\" )\n",
    "    \n",
    "\n",
    "if 0.0-0.91831344>0.2 :\n",
    "    print(\"ok\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(test_y.reshape(-1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
