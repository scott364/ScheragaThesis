{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we'll be using a GRU model for a time series prediction task and we will compare the performance of the GRU model against an LSTM model as well. The dataset that we will be using is the Hourly Energy Consumption dataset which can be found on [Kaggle](https://www.kaggle.com/robikscube/hourly-energy-consumption). The dataset contains power consumption data across different regions around the United States recorded on an hourly basis.\n",
    "\n",
    "You can run the code implementation in this article on FloydHub using their GPUs on the cloud by clicking the following link and using the main.ipynb notebook.\n",
    "\n",
    "[![Run on FloydHub](https://static.floydhub.com/button/button-small.svg)](https://floydhub.com/run?template=https://github.com/gabrielloye/https://github.com/gabrielloye/GRU_Prediction)\n",
    "\n",
    "This will speed up the training process significantly. Alternatively, the link to the GitHub repository can be found [here]().\n",
    "\n",
    "The goal of this implementation is to create a model that can accurately predict the energy usage in the next hour given historical usage data. We will be using both the GRU and LSTM model to train on a set of historical data and evaluate both models on an unseen test set. To do so, weâ€™ll start with feature selection, data-preprocessing, followed by defining, training and eventually evaluating the models.\n",
    "\n",
    "We will be using the PyTorch library to implement both types of models along with other common Python libraries used in data analytics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.python-engineer.com/posts/pytorch-rnn-lstm-gru/\n",
    "\n",
    "#https://blog.floydhub.com/gru-with-pytorch/\n",
    "\n",
    "import os\n",
    "import time\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import random\n",
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from tqdm import tqdm_notebook\n",
    "from sklearn.preprocessing import MinMaxScaler,QuantileTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Local ###\n",
    "#from data_processing import *\n",
    "\n",
    "\n",
    "\n",
    "# Define data root directory\n",
    "\n",
    "#data_dir = \"./data/\"\n",
    "#print(os.listdir(data_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a total of **12** *.csv* files containing hourly energy trend data (*'est_hourly.paruqet'* and *'pjm_hourly_est.csv'* are not used). In our next step, we will be reading these files and pre-processing these data in this order:\n",
    "- Getting the time data of each individual time step and generalizing them\n",
    "    - Hour of the day *i.e. 0-23*\n",
    "    - Day of the week *i.e. 1-7*\n",
    "    - Month *i.e. 1-12*\n",
    "    - Day of the year *i.e. 1-365*\n",
    "    \n",
    "    \n",
    "- Scale the data to values between 0 and 1\n",
    "    - Algorithms tend to perform better or converge faster when features are on a relatively similar scale and/or close to normally distributed\n",
    "    - Scaling preserves the shape of the original distribution and doesn't reduce the importance of outliers.\n",
    "    \n",
    "    \n",
    "- Group the data into sequences to be used as inputs to the model and store their corresponding labels\n",
    "    - The **sequence length** or **lookback period** is the number of data points in history that the model will use to make the prediction\n",
    "    - The label will be the next data point in time after the last one in the input sequence\n",
    "    \n",
    "\n",
    "- The inputs and labels will then be split into training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# torch.cuda.is_available() checks and returns a Boolean True if a GPU is available, else it'll return False\n",
    "is_cuda = torch.cuda.is_available()\n",
    "\n",
    "# If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.\n",
    "if is_cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    \n",
    "print(device)\n",
    "\n",
    "\n",
    "def get_torch_device( v=0 ):\n",
    "    # torch.cuda.is_available() checks and returns a Boolean True if a GPU is available, else it'll return False\n",
    "    is_cuda = torch.cuda.is_available()\n",
    "    # If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.\n",
    "    if is_cuda:\n",
    "        device = torch.device(\"cuda\")\n",
    "        if v:  print( \"CUDA Available!\" )\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        if v:  print( \"NO CUDA\" )\n",
    "    return device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1440, 10)\n",
      "(3960, 10)\n",
      "(5400, 10)\n"
     ]
    }
   ],
   "source": [
    "#choppeddata=pd.read_csv('choppeddata_10_06_2021.csv')#.head()\n",
    "choppeddata1=pd.read_csv('choppeddata_10_04_2021_randomselector.csv')#.head()\n",
    "choppeddata2=pd.read_csv('choppeddata_10_06_2021_randomselector.csv')#.head()\n",
    "print(choppeddata1.shape)\n",
    "print(choppeddata2.shape)\n",
    "frames = [choppeddata1, choppeddata2]\n",
    "choppeddata = pd.concat(frames)\n",
    "print(choppeddata.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "choppeddata=pd.read_csv('choppeddata_10_06_2021_randomselector.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10440, 10)\n",
      "total runs: 1740\n"
     ]
    }
   ],
   "source": [
    "print(choppeddata.shape)\n",
    "runqty=int(choppeddata.shape[0]/6)\n",
    "print(\"total runs:\",runqty)\n",
    "choppedheaders=[]\n",
    "lookback=10 #save only the last 11 timesteps\n",
    "for i in range(lookback):  \n",
    "    label=str(i)\n",
    "    choppedheaders.append(\"header\"+label)\n",
    "\n",
    "#put chopped data in np.arrays\n",
    "State=np.zeros((runqty,5,lookback)) #96 runs,with 5 sets of data (x,y,z,roll,pitch) each, and each run is 11 timesteps long\n",
    "Labels=np.zeros((runqty,lookback)) #96 runs, each run is 11 timesteps long\n",
    "runcounter=0\n",
    "\n",
    "for i in range(0,choppeddata.shape[0],6):\n",
    "            State[runcounter][0][:]=(choppeddata[choppedheaders[:]].iloc[i]).tolist()\n",
    "            State[runcounter][1][:]=(choppeddata[choppedheaders[:]].iloc[i+1]).tolist()\n",
    "            State[runcounter][2][:]=(choppeddata[choppedheaders[:]].iloc[i+2]).tolist()\n",
    "            State[runcounter][3][:]=(choppeddata[choppedheaders[:]].iloc[i+3]).tolist()\n",
    "            State[runcounter][4][:]=(choppeddata[choppedheaders[:]].iloc[i+4]).tolist()\n",
    "            Labels[runcounter][:]=(choppeddata[choppedheaders[:]].iloc[i+5]).tolist()  #labels   \n",
    "            runcounter+=1\n",
    "#print(State[0])\n",
    "#print(Labels)\n",
    "#print(Labels[:,9]) #just getting finals labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape (1740, 5, 10)\n",
      "Test set Y size 21750\n",
      "Train set Y size 1305\n",
      "Test set C size 21750\n",
      "Test set Y size 435\n"
     ]
    }
   ],
   "source": [
    "#X= range(0,575,6)\n",
    "#y= range(0,575,6)\n",
    "\n",
    "X=State\n",
    "y=Labels[:,lookback-1]\n",
    "print(\"x.shape\",X.shape)\n",
    "\n",
    "y=y.reshape(runqty,1)\n",
    "\n",
    "random_seed=int(time.time())\n",
    "#print(int(time.time()))\n",
    "train_x, test_x, train_y,test_y = train_test_split(X, y, test_size=.25, #0.33, \n",
    "                                                   random_state=random_seed)\n",
    "#print(\"Train\")\n",
    "#print(train_x[0])\n",
    "#print(train_y[0])\n",
    "print(\"Test set Y size\", test_x.size)\n",
    "print(\"Train set Y size\", train_y.size)\n",
    "#print(test_x[0])\n",
    "#print(test_y[0])\n",
    "print(\"Test set C size\", test_x.size)\n",
    "print(\"Test set Y size\", test_y.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a total of 980,185 sequences of training data\n",
    "\n",
    "To improve the speed of our training, we can process the data in batches so that the model does not need to update its weights as frequently. The Torch *Dataset* and *DataLoader* classes are useful for splitting our data into batches and shuffling them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataloader.DataLoader object at 0x7f42884c6e10>\n"
     ]
    }
   ],
   "source": [
    "batch_size = 8\n",
    "#a batch size of 64, i.e. each element in the dataloader iterable will return a batch of 64 features and labels.\n",
    "\n",
    "train_data = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size, drop_last=True)\n",
    "\n",
    "test_data   = TensorDataset( torch.from_numpy( test_x ), torch.from_numpy( test_y ) )\n",
    "test_loader = DataLoader( test_data, shuffle = True, batch_size = batch_size, drop_last = True )\n",
    "print(train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also check if we have any GPUs to speed up our training time by many folds. If youâ€™re using FloydHub with GPU to run this code, the training time will be significantly reduced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUNet(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, n_layers, drop_prob=0.2):\n",
    "        super(GRUNet, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.gru = nn.GRU(input_dim, hidden_dim, n_layers, batch_first=True, dropout=drop_prob)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x, h):\n",
    "        out, h = self.gru(x, h)\n",
    "        out = self.fc(self.relu(out[:,-1]))\n",
    "        return out, h\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        hidden = weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device)\n",
    "        return hidden\n",
    "def train(train_loader, learn_rate, hidden_dim=128, EPOCHS=400, model_type=\"GRU\"):\n",
    "    #got  109 / 180 on training set, 29 / 60 on test set from 128 hidden dim, 50 epoch, batch size of 4, lr =0.001\n",
    "    #Got training data= 146 / 180, success vs test data= 38 / 60 with same as above but 100 epoch\n",
    "    #Got training data= 172 / 180, success vs test data= 46 / 60 with same as above but 200 epoch\n",
    "    #Got training data= 165 / 180, success vs test data= 52 / 60 with same as above but 200 epoch\n",
    "    \n",
    "    losslist=[]\n",
    "    # Setting common hyperparameters\n",
    "    input_dim = next(iter(train_loader))[0].shape[2]  #  = 11\n",
    "    #print(input_dim)\n",
    "    #print(\"input_dim\",input_dim)\n",
    "    output_dim = 1\n",
    "    n_layers = 2\n",
    "    # Instantiating the models\n",
    "    if model_type == \"GRU\":\n",
    "        model = GRUNet(input_dim, hidden_dim, output_dim, n_layers)\n",
    "    else:\n",
    "        model = LSTMNet(input_dim, hidden_dim, output_dim, n_layers)\n",
    "    model.to(device)\n",
    "    \n",
    "    # Defining loss function and optimizer\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learn_rate)\n",
    "    \n",
    "    model.train()\n",
    "    print(\"Starting Training of {} model\".format(model_type))\n",
    "    epoch_times = []\n",
    "    # Start training loop\n",
    "    for epoch in range(1,EPOCHS+1):\n",
    "        start_time = time.clock()\n",
    "        h = model.init_hidden(batch_size)\n",
    "        avg_loss = 0.\n",
    "        counter = 0\n",
    "        for x, label in train_loader:\n",
    "            #print(\"x\",x)\n",
    "            #print(\"label\",label)\n",
    "            counter += 1\n",
    "            if model_type == \"GRU\":\n",
    "                h = h.data\n",
    "            else:\n",
    "                h = tuple([e.data for e in h])\n",
    "            model.zero_grad()\n",
    "            \n",
    "            out, h = model(x.to(device).float(), h)\n",
    "            #print(\"out\",out)\n",
    "            loss = criterion(out, label.to(device).float())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            avg_loss += loss.item()\n",
    "            \n",
    "            if counter%20000 == 0:\n",
    "                print(\"Epoch {}......Step: {}/{}....... Average Loss for Epoch: {}\".format(epoch, counter, len(train_loader), avg_loss/counter))\n",
    "        current_time = time.clock()\n",
    "        if epoch%40 == 0:\n",
    "            print(\"Epoch {}/{} Done, Total Loss: {}   Time Elapsed: {} seconds\".format(epoch, EPOCHS, avg_loss/len(train_loader),str(current_time-start_time)))\n",
    "        \n",
    "            #print(\"Total\".format())\n",
    "        losslist.append(avg_loss/len(train_loader))\n",
    "        epoch_times.append(current_time-start_time)\n",
    "    print(\"Total Training Time: {} seconds\".format(str(sum(epoch_times))))\n",
    "    plt.plot(losslist)\n",
    "    plt.title(\"Loss\")\n",
    "    plt.show()\n",
    "    return model\n",
    "\n",
    "def evaluate(model, test_x, test_y):\n",
    "    model.eval()\n",
    "    outputs = []\n",
    "    targets = []\n",
    "    start_time = time.clock()\n",
    "    #for i in test_x.keys():\n",
    "    for i in range( len( test_x ) ):    \n",
    "        inp = torch.from_numpy(np.array(test_x[i])) # should be 5x1\n",
    "        labs = torch.from_numpy(np.array(test_y[i])) #should be 1x1\n",
    "        h = model.init_hidden(inp.shape[0])\n",
    "        #print(\"inp\",inp)\n",
    "        #print(\"labs\",labs)\n",
    "        #print(\"h\",h)\n",
    "        out, h = model(inp.to(device).float(), h)\n",
    "        #outputs.append(label_scalers[i].inverse_transform(out.cpu().detach().numpy()).reshape(-1))\n",
    "        #targets.append(label_scalers[i].inverse_transform(labs.numpy()).reshape(-1))\n",
    "        outputs.append( out.cpu().detach().numpy().reshape(-1) )\n",
    "        targets.append( labs.numpy().reshape(-1) )\n",
    "        \n",
    "    print(\"Evaluation Time: {}\".format(str(time.clock()-start_time)))\n",
    "    sMAPE = 0\n",
    "    for i in range(len(outputs)):\n",
    "        sMAPE += np.mean(abs(outputs[i]-targets[i])/(targets[i]+outputs[i])/2)/len(outputs)\n",
    "    print(\"sMAPE: {}%\".format(sMAPE*100))\n",
    "    return outputs, targets, sMAPE\n",
    "                               \n",
    "def evaluate2(model, test_x, test_y):\n",
    "    model.eval()\n",
    "    outputs = []\n",
    "    targets = []  #labels\n",
    "    start_time = time.clock()\n",
    "    #for i in test_x.keys():\n",
    "    #for i in range( len( test_x ) ):    \n",
    "    inp = torch.from_numpy(np.array(test_x)) # should be 5x1\n",
    "    labs = torch.from_numpy(np.array(test_y)) #should be 1x1\n",
    "    h = model.init_hidden(inp.shape[0])\n",
    "    #print(\"inp\",inp)\n",
    "    #print(\"labs\",labs)\n",
    "    #print(\"h\",h)\n",
    "    out, h = model(inp.to(device).float(), h)\n",
    "    #outputs.append(label_scalers[i].inverse_transform(out.cpu().detach().numpy()).reshape(-1))\n",
    "    #targets.append(label_scalers[i].inverse_transform(labs.numpy()).reshape(-1))\n",
    "    outputs.append( out.cpu().detach().numpy().reshape(-1) )\n",
    "    targets.append( labs.numpy().reshape(-1) )\n",
    "        \n",
    "    print(\"Evaluation Time: {}\".format(str(time.clock()-start_time)))\n",
    "    sMAPE = 0\n",
    "    for i in range(len(outputs)):\n",
    "        sMAPE += np.mean(abs(outputs[i]-targets[i])/(targets[i]+outputs[i])/2)/len(outputs)\n",
    "    print(\"sMAPE: {}%\".format(sMAPE*100))\n",
    "    return outputs, targets, sMAPE                               \n",
    "\n",
    "def evaluatefull(model, train_x, train_y, test_x, test_y,maxdifference=0.2, verbose=False):\n",
    "\n",
    "    m = nn.ReLU()\n",
    "    #m = nn.Sigmoid()\n",
    "    #output = m(input)\n",
    "    print(\"Vs Training Set\")\n",
    "    gru_outputs, targets, gru_sMAPE = evaluate2(gru_model, train_x, train_y)\n",
    "    #print(test_y)\n",
    "    #print(gru_outputs)\n",
    "    #print(gru_outputs[0][5])\n",
    "\n",
    "\n",
    "    testy=test_y.reshape(-1)\n",
    "    trainy=train_y.reshape(-1)\n",
    "\n",
    "\n",
    "    #print(\"Train size:\",trainy.size)\n",
    "    print(gru_outputs[0][4])\n",
    "    train_successcounter=0\n",
    "    for i in range(int(trainy.size)):\n",
    "        #print(testy[i],gru_outputs[0][i],m(torch.tensor(gru_outputs[0][i])))\n",
    "        #print(train[i],gru_outputs[0][i],m(torch.tensor(gru_outputs[0][i])))\n",
    "\n",
    "\n",
    "        #print(trainy[i],gru_outputs[0][i], m(torch.tensor(gru_outputs[0][i])))\n",
    "\n",
    "\n",
    "        if abs(trainy[i]-gru_outputs[0][i])<maxdifference :\n",
    "            train_successcounter+=1\n",
    "        #print(testy[i])\n",
    "        #print\n",
    "        #output = m(input)\n",
    "\n",
    "\n",
    "\n",
    "    test_successcounter=0\n",
    "    print(\"Vs Test Set\")\n",
    "    gru_outputs, targets, gru_sMAPE = evaluate2(gru_model, test_x, test_y)\n",
    "    #print(\"test size: \",testy.size)\n",
    "\n",
    "    for i in range(int(testy.size)):\n",
    "\n",
    "\n",
    "        #, m(torch.tensor(gru_outputs[0][i])))\n",
    "\n",
    "\n",
    "        if abs(testy[i]-gru_outputs[0][i])<maxdifference :\n",
    "            test_successcounter+=1\n",
    "            if verbose==True:\n",
    "                print(testy[i],gru_outputs[0][i], \"OK\" )\n",
    "        else:\n",
    "            if verbose==True:\n",
    "                print(testy[i],gru_outputs[0][i], \"X\" )\n",
    "            #print(testy[i])\n",
    "        #print\n",
    "        #output = m(input)\n",
    "    print(\"\")\n",
    "    print(\" vs training data=\" ,train_successcounter,\"/\",trainy.size, \" vs test data=\" ,\n",
    "          test_successcounter,\"/\",testy.size,int(100*test_successcounter/testy.size),\"%\", \"at max difference\",maxdifference )\n",
    "    return ( train_successcounter ,test_successcounter)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape (1740, 5, 10)\n",
      "Starting Training of GRU model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/scott/anaconda3/envs/thesis/lib/python3.7/site-packages/ipykernel_launcher.py:49: DeprecationWarning: time.clock has been deprecated in Python 3.3 and will be removed from Python 3.8: use time.perf_counter or time.process_time instead\n",
      "/home/scott/anaconda3/envs/thesis/lib/python3.7/site-packages/ipykernel_launcher.py:72: DeprecationWarning: time.clock has been deprecated in Python 3.3 and will be removed from Python 3.8: use time.perf_counter or time.process_time instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40/800 Done, Total Loss: 0.09940223872627309   Time Elapsed: 0.3139070000000004 seconds\n",
      "Epoch 80/800 Done, Total Loss: 0.07411464997152997   Time Elapsed: 0.33328699999999856 seconds\n",
      "Epoch 120/800 Done, Total Loss: 0.06771066813498301   Time Elapsed: 0.3223649999999978 seconds\n",
      "Epoch 160/800 Done, Total Loss: 0.06192543859928532   Time Elapsed: 0.3455070000000049 seconds\n",
      "Epoch 200/800 Done, Total Loss: 0.05277394069011603   Time Elapsed: 0.34501800000001026 seconds\n",
      "Epoch 240/800 Done, Total Loss: 0.05026336745187594   Time Elapsed: 0.3482869999999991 seconds\n",
      "Epoch 280/800 Done, Total Loss: 0.03854424246026923   Time Elapsed: 0.35098399999999685 seconds\n",
      "Epoch 320/800 Done, Total Loss: 0.0324388760926939   Time Elapsed: 0.3422949999999929 seconds\n",
      "Epoch 360/800 Done, Total Loss: 0.025070016335930242   Time Elapsed: 0.3437059999999974 seconds\n",
      "Epoch 400/800 Done, Total Loss: 0.021804759789549637   Time Elapsed: 0.3242529999999988 seconds\n",
      "Epoch 440/800 Done, Total Loss: 0.013680624692026595   Time Elapsed: 0.36904900000001817 seconds\n",
      "Epoch 480/800 Done, Total Loss: 0.014595843730865766   Time Elapsed: 0.3415080000000046 seconds\n",
      "Epoch 520/800 Done, Total Loss: 0.014068428477689486   Time Elapsed: 0.3335790000000145 seconds\n",
      "Epoch 560/800 Done, Total Loss: 0.009440979408957292   Time Elapsed: 0.3710100000000125 seconds\n",
      "Epoch 600/800 Done, Total Loss: 0.00841625865625269   Time Elapsed: 0.4022099999999966 seconds\n",
      "Epoch 640/800 Done, Total Loss: 0.005281158215494861   Time Elapsed: 0.347251 seconds\n",
      "Epoch 680/800 Done, Total Loss: 0.008639346348426013   Time Elapsed: 0.35522000000000276 seconds\n",
      "Epoch 720/800 Done, Total Loss: 0.006460470838066579   Time Elapsed: 0.3414470000000165 seconds\n",
      "Epoch 760/800 Done, Total Loss: 0.008586188694719294   Time Elapsed: 0.3225570000000175 seconds\n",
      "Epoch 800/800 Done, Total Loss: 0.006415587304977048   Time Elapsed: 0.33665200000001505 seconds\n",
      "Total Training Time: 274.45926700000126 seconds\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vs Training Set\n",
      "Evaluation Time: 0.002200000000016189\n",
      "sMAPE: 1.020875580912846%\n",
      "1.0085869\n",
      "Vs Test Set\n",
      "Evaluation Time: 0.0009130000000254768\n",
      "sMAPE: 6.213942430970406%\n",
      "\n",
      " vs training data= 1279 / 1305  vs test data= 358 / 435 82 % at max difference 0.2\n",
      "Vs Training Set\n",
      "Evaluation Time: 0.0021750000000224645\n",
      "sMAPE: 1.020875580912846%\n",
      "1.0085869\n",
      "Vs Test Set\n",
      "Evaluation Time: 0.0010629999999878237\n",
      "sMAPE: 6.213942430970406%\n",
      "\n",
      " vs training data= 1298 / 1305  vs test data= 377 / 435 86 % at max difference 0.3\n",
      "Vs Training Set\n",
      "Evaluation Time: 0.00119000000000824\n",
      "sMAPE: 1.020875580912846%\n",
      "1.0085869\n",
      "Vs Test Set\n",
      "Evaluation Time: 0.0010879999999815482\n",
      "sMAPE: 6.213942430970406%\n",
      "\n",
      " vs training data= 1303 / 1305  vs test data= 386 / 435 88 % at max difference 0.4\n",
      "Vs Training Set\n",
      "Evaluation Time: 0.0020039999999994507\n",
      "sMAPE: 1.020875580912846%\n",
      "1.0085869\n",
      "Vs Test Set\n",
      "Evaluation Time: 0.0008250000000202817\n",
      "sMAPE: 6.213942430970406%\n",
      "\n",
      " vs training data= 1305 / 1305  vs test data= 396 / 435 91 % at max difference 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/scott/anaconda3/envs/thesis/lib/python3.7/site-packages/ipykernel_launcher.py:115: DeprecationWarning: time.clock has been deprecated in Python 3.3 and will be removed from Python 3.8: use time.perf_counter or time.process_time instead\n",
      "/home/scott/anaconda3/envs/thesis/lib/python3.7/site-packages/ipykernel_launcher.py:130: DeprecationWarning: time.clock has been deprecated in Python 3.3 and will be removed from Python 3.8: use time.perf_counter or time.process_time instead\n"
     ]
    }
   ],
   "source": [
    "lr = 0.0005\n",
    "batch_size = 8\n",
    "#a batch size of 64, i.e. each element in the dataloader iterable will return a batch of 64 features and labels.\n",
    "\n",
    "X=State\n",
    "y=Labels[:,lookback-1]\n",
    "print(\"x.shape\",X.shape)\n",
    "y=y.reshape(runqty,1)\n",
    "\n",
    "\n",
    "random_seed=int(time.time())\n",
    "train_x, test_x, train_y,test_y = train_test_split(X, y, test_size=.25, #0.33, \n",
    "                                                   random_state=random_seed)\n",
    "\n",
    "train_data = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size, drop_last=True)\n",
    "test_data   = TensorDataset( torch.from_numpy( test_x ), torch.from_numpy( test_y ) )\n",
    "test_loader = DataLoader( test_data, shuffle = True, batch_size = batch_size, drop_last = True )\n",
    "\n",
    "\n",
    "gru_model = train(train_loader, lr , hidden_dim=128, EPOCHS=800, model_type=\"GRU\")\n",
    "train2 ,test2=evaluatefull(gru_model, train_x, train_y, test_x, test_y,maxdifference=.2)\n",
    "train3 ,test3=evaluatefull(gru_model, train_x, train_y, test_x, test_y,maxdifference=.3)\n",
    "train4 ,test4=evaluatefull(gru_model, train_x, train_y, test_x, test_y,maxdifference=.4)\n",
    "train5 ,test5=evaluatefull(gru_model, train_x, train_y, test_x, test_y,maxdifference=.5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_episode(model, data,  maxdifference=0.2, verbose=False):\n",
    "\n",
    "   \n",
    "    model.eval()\n",
    "    inp = torch.from_numpy(np.array(data)) # should be 5x1\n",
    "    h = model.init_hidden(inp.shape[0])\n",
    "    #print(\"inp\",inp)\n",
    "    #print(\"labs\",labs)\n",
    "    #print(\"h\",h)\n",
    "    out, h = model(inp.to(device).float(), h)\n",
    "    #print(\"model output\",out)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10_10_2021\n",
      "model saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/scott/anaconda3/envs/thesis/lib/python3.7/site-packages/torch/serialization.py:360: UserWarning: Couldn't retrieve source code for container of type GRUNet. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import time\n",
    "from datetime import date\n",
    "\n",
    "today = date.today()    \n",
    "todaydate = today.strftime(\"%m_%d_%Y\")\n",
    "print(todaydate)\n",
    "torch.save(gru_model,\"currentmodel_\"+todaydate+\".pt\")\n",
    "print(\"model saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test model on new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded\n"
     ]
    }
   ],
   "source": [
    "gru_model3=torch.load('currentmodel_10_10_2021.pt')\n",
    "gru_model3.eval()\n",
    "print(\"loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index= 213\n",
      "(1, 5, 10)\n",
      "prediction -0.02125750482082367   actual [0.]\n"
     ]
    }
   ],
   "source": [
    "randomindex=random.randint(0,225)\n",
    "print(\"index=\",randomindex)\n",
    "\n",
    "exampledata=np.expand_dims(test_x[207, 0:5, 0:10], axis=0)\n",
    "\n",
    "print(exampledata.shape)\n",
    "prediction=evaluate_episode(gru_model3, exampledata)\n",
    "\n",
    "print(\"prediction\",float(prediction), \"  actual\",test_y[randomindex])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## simlulate a buffer of 10 timesteps entering the classifier over a 1 episode, and classifying them. Filling empty data with zeroes or ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index= 92\n",
      "final partial data\n",
      "[[[1.         0.76950357 0.77840831 0.76101565 0.78156948 0.76469994\n",
      "   0.77954628 0.78327592 0.78053232 0.77010096]\n",
      "  [1.         0.38728178 0.41727097 0.41364554 0.42047341 0.42322906\n",
      "   0.44245988 0.42964934 0.44216477 0.43072489]\n",
      "  [1.         0.91590208 0.9221236  0.95616662 0.93024534 0.96158582\n",
      "   0.94491823 0.94678631 0.92449498 0.93107626]\n",
      "  [1.         0.63594521 0.60908194 0.61194267 0.5986146  0.59769942\n",
      "   0.58124505 0.58662354 0.57633089 0.58539882]\n",
      "  [1.         0.79298457 0.80435802 0.79846642 0.80939073 0.80338526\n",
      "   0.81597383 0.81442646 0.81783244 0.81415342]]]\n",
      "\n",
      "full data\n",
      "[[[0.76950357 0.77840831 0.76101565 0.78156948 0.76469994 0.77954628\n",
      "   0.78327592 0.78053232 0.77010096 0.78535546]\n",
      "  [0.38728178 0.41727097 0.41364554 0.42047341 0.42322906 0.44245988\n",
      "   0.42964934 0.44216477 0.43072489 0.44280338]\n",
      "  [0.91590208 0.9221236  0.95616662 0.93024534 0.96158582 0.94491823\n",
      "   0.94678631 0.92449498 0.93107626 0.94433889]\n",
      "  [0.63594521 0.60908194 0.61194267 0.5986146  0.59769942 0.58124505\n",
      "   0.58662354 0.57633089 0.58539882 0.57569005]\n",
      "  [0.79298457 0.80435802 0.79846642 0.80939073 0.80338526 0.81597383\n",
      "   0.81442646 0.81783244 0.81415342 0.82118649]]]\n",
      "predictions: [-0.15276971459388733, -0.004863232374191284, 0.1955796629190445, -0.17194050550460815, -0.10351568460464478, -0.04318016767501831, -0.09391683340072632, -0.028662577271461487, -0.07946464419364929]\n",
      "\n",
      "\n",
      "prediction from 10 timesteps -0.02869977056980133 actual [0.]\n"
     ]
    }
   ],
   "source": [
    "outputlist=[]\n",
    "\n",
    "randomindex=random.randint(0,225)\n",
    "print(\"index=\",randomindex)\n",
    "exampledata=np.expand_dims(test_x[randomindex, 0:5, 0:10], axis=0)\n",
    "\n",
    "\n",
    "#print(temparray.shape)\n",
    "#temparray=np.expand_dims(temparray, axis=1)\n",
    "\n",
    "#print(temparray.shape)\n",
    "#print(temparray)\n",
    "\n",
    "#temparray2=test_x[randomindex, 0:5, 0]\n",
    "#temparray2=np.expand_dims(temparray2, axis=1)\n",
    "\n",
    "for i in range(9):\n",
    "    if i!=10:\n",
    "        temparray=np.ones((5,1)) #test_x[randomindex, 0:5, 0]   #zeroes or \"ones\" here seems to work equally well. \n",
    "    \n",
    "    for j in range(8-i):\n",
    "        #temparray2=test_x[randomindex, 0:5, 0]\n",
    "        #temparray2=np.expand_dims(temparray2, axis=1)\n",
    "        temparray=np.append(temparray,np.ones((5,1)),axis=1)       #zeroes or \"ones\" here seems to work equally well. \n",
    "        #temparray=np.append(temparray,temparray2,axis=1)\n",
    "        #temparray=np.append(np.zeros((5,1)),temparray,axis=1)   \n",
    "    \n",
    "    for j in range(i+1):\n",
    "\n",
    "        temparray2=test_x[randomindex, 0:5, j]\n",
    "        temparray2=np.expand_dims(temparray2, axis=1)\n",
    "        temparray=np.append(temparray,temparray2,axis=1)\n",
    "        #temparray=np.append(np.zeros((5,1)),temparray,axis=1)\n",
    "\n",
    "    \n",
    "    #print(temparray)\n",
    "    temparray=np.expand_dims(temparray, axis=0)\n",
    "    outputpartial=evaluate_episode(gru_model3, temparray)\n",
    "    \n",
    "    \n",
    "    outputlist.append(float(outputpartial))\n",
    "\n",
    "print(\"final partial data\")\n",
    "print(temparray)   \n",
    "print(\"\")\n",
    "print(\"full data\")\n",
    "print(exampledata)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#print(\"prediction from\",x,\" timesteps\",float(outputpartial),\"actual\",test_y[randomindex])\n",
    "print(\"predictions:\",outputlist)\n",
    "\n",
    "\n",
    "#print(\"full data\")\n",
    "#print(exampledata)\n",
    "print(\"\")\n",
    "#print(\"evaluating all 10 timesteps\")\n",
    "\n",
    "outputfull=evaluate_episode(gru_model3, exampledata)\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "#print(\"evaluating 1st timestep repeated 10 times\")\n",
    "\n",
    "\n",
    "#print(\"prediction from 1 timestep\",float(outputpartial),\"actual\",test_y[randomindex])\n",
    "print(\"prediction from 10 timesteps\",float(outputfull),\"actual\",test_y[randomindex])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## classifying progression of 10 actual forces and torques in a sucessful sequence longer than 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7200, 30)\n",
      "['header0', 'header1', 'header2', 'header3', 'header4', 'header5', 'header6', 'header7', 'header8', 'header9', 'header10', 'header11', 'header12', 'header13', 'header14', 'header15', 'header16', 'header17', 'header18', 'header19', 'header20', 'header21', 'header22', 'header23', 'header24', 'header25', 'header26', 'header27', 'header28', 'header29']\n"
     ]
    }
   ],
   "source": [
    "originaldata=pd.read_csv('forcetorquebuttonresults_renormalized_10_06_2021.csv')#.head()\n",
    "print(originaldata.shape)\n",
    "headers=[]\n",
    "lookback=30 #save only the last 11 timesteps\n",
    "for i in range(lookback):  \n",
    "    label=str(i)\n",
    "    headers.append(\"header\"+label)\n",
    "print(headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      header20  header21  header22  header23  header24  header25  header26  \\\n",
      "4039  0.313334  0.297908  0.300916  0.295691  0.304691  0.297850  0.298842   \n",
      "4040  0.887794  0.927183  0.923606  0.929387  0.921559  0.932170  0.925471   \n",
      "4041  0.689476  0.707595  0.706714  0.707579  0.702587  0.708988  0.708745   \n",
      "4042  0.516504  0.504406  0.494093  0.490169  0.484280  0.477127  0.479447   \n",
      "4043  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "\n",
      "      header27  header28  header29  \n",
      "4039  0.298431  0.323285       NaN  \n",
      "4040  0.922702  0.851257       NaN  \n",
      "4041  0.709223  0.693128       NaN  \n",
      "4042  0.476531  0.484857       NaN  \n",
      "4043  0.000000  1.000000       NaN  \n"
     ]
    }
   ],
   "source": [
    "print(originaldata[headers[20:30]].iloc[4039:4044])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.54251199 0.52489482 0.52926319 0.5065195  0.49991385 0.49536954\n",
      "  0.48716645 0.48923134 0.48428939 0.51027604]\n",
      " [0.37858933 0.31333356 0.29790778 0.30091579 0.29569087 0.30469148\n",
      "  0.29785046 0.29884238 0.29843066 0.32328525]\n",
      " [0.93041265 0.88779361 0.92718331 0.92360628 0.92938742 0.92155946\n",
      "  0.9321705  0.92547146 0.92270207 0.85125721]\n",
      " [0.63809043 0.68947554 0.70759451 0.70671384 0.70757924 0.7025874\n",
      "  0.70898769 0.70874526 0.70922296 0.69312779]\n",
      " [0.54499817 0.51650444 0.50440577 0.49409301 0.49016859 0.48428045\n",
      "  0.47712741 0.4794472  0.4765313  0.48485743]]\n",
      "(1, 5, 10)\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "\n",
      "prediction from 10 timesteps 1.0148999691009521 actual 1.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#print(originaldata[headers[0:30]].iloc[4039:4044])\n",
    "#print(originaldata[headers[0:30]].iloc[4039:4044].to_numpy()\n",
    "classifytest=originaldata[headers[19:29]].iloc[4038:4043].to_numpy()\n",
    "labelstest=originaldata[headers[19:29]].iloc[4043].to_numpy()\n",
    "print(classifytest)\n",
    "classifytest=np.expand_dims(classifytest, axis=0)\n",
    "print(classifytest.shape)\n",
    "print(labelstest)\n",
    "\n",
    "outputfull=evaluate_episode(gru_model3, classifytest)\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "#print(\"evaluating 1st timestep repeated 10 times\")\n",
    "\n",
    "\n",
    "#print(\"prediction from 1 timestep\",float(outputpartial),\"actual\",test_y[randomindex])\n",
    "print(\"prediction from 10 timesteps\",float(outputfull),\"actual\",labelstest[9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction from timestep 0 - 10  : 0.025398463010787964 actual 0.0 OK\n",
      "prediction from timestep 1 - 11  : 0.22636616230010986 actual 0.0 OK\n",
      "prediction from timestep 2 - 12  : 0.8862465023994446 actual 0.0 OK\n",
      "prediction from timestep 3 - 13  : -0.40418505668640137 actual 0.0 OK\n",
      "prediction from timestep 4 - 14  : 0.002025976777076721 actual 0.0 OK\n",
      "prediction from timestep 5 - 15  : 0.8055680394172668 actual 0.0 OK\n",
      "prediction from timestep 6 - 16  : -0.028045445680618286 actual 0.0 OK\n",
      "prediction from timestep 7 - 17  : 0.10840070247650146 actual 0.0 OK\n",
      "prediction from timestep 8 - 18  : 0.07783600687980652 actual 0.0 OK\n",
      "prediction from timestep 9 - 19  : -0.17324498295783997 actual 0.0 OK\n",
      "prediction from timestep 10 - 20  : -0.1079346239566803 actual 0.0 OK\n",
      "prediction from timestep 11 - 21  : 0.055035024881362915 actual 0.0 OK\n",
      "prediction from timestep 12 - 22  : 0.16071492433547974 actual 0.0 OK\n",
      "prediction from timestep 13 - 23  : 0.10687632858753204 actual 0.0 OK\n",
      "prediction from timestep 14 - 24  : 0.12886187434196472 actual 0.0 OK\n",
      "prediction from timestep 15 - 25  : -0.1356879472732544 actual 0.0 OK\n",
      "prediction from timestep 16 - 26  : 0.7972680926322937 actual 0.0 OK\n",
      "prediction from timestep 17 - 27  : 0.3671107888221741 actual 0.0 OK\n",
      "prediction from timestep 18 - 28  : 0.046740055084228516 actual 0.0 OK\n",
      "prediction from timestep 19 - 29  : 1.0327951908111572 actual 1.0 OK\n",
      "okcounter 20\n"
     ]
    }
   ],
   "source": [
    "\n",
    "okcounter=0\n",
    "for i in range(20):\n",
    "    classifytest=originaldata[headers[i:10+i]].iloc[4296:4301].to_numpy()\n",
    "    labelstest=originaldata[headers[i:10+i]].iloc[4301].to_numpy()\n",
    "    #print(classifytest)\n",
    "    classifytest=np.expand_dims(classifytest, axis=0)\n",
    "    #print(classifytest.shape)\n",
    "    #print(labelstest)\n",
    "\n",
    "    outputfull=evaluate_episode(gru_model3, classifytest)\n",
    "\n",
    "    #print(\"\")\n",
    "\n",
    "    #print(\"evaluating 1st timestep repeated 10 times\")\n",
    "\n",
    "\n",
    "    #print(\"prediction from 1 timestep\",float(outputpartial),\"actual\",test_y[randomindex])\n",
    "    \n",
    "    if abs(float(outputfull)-labelstest[9])>0.9:\n",
    "        result=\"X\"\n",
    "    else:\n",
    "        result=\"OK\"\n",
    "        okcounter+=1\n",
    "    print(\"prediction from timestep\",i,\"-\",i+10,\" :\",float(outputfull),\"actual\",labelstest[9], result)\n",
    "print(\"okcounter\",okcounter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## OLD STUFF BELOW------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape (900, 5, 10)\n",
      "Starting Training of GRU model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/scott/anaconda3/envs/thesis/lib/python3.7/site-packages/ipykernel_launcher.py:49: DeprecationWarning: time.clock has been deprecated in Python 3.3 and will be removed from Python 3.8: use time.perf_counter or time.process_time instead\n",
      "/home/scott/anaconda3/envs/thesis/lib/python3.7/site-packages/ipykernel_launcher.py:72: DeprecationWarning: time.clock has been deprecated in Python 3.3 and will be removed from Python 3.8: use time.perf_counter or time.process_time instead\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_14105/1255051593.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m     \u001b[0mgru_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mhidden_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEPOCHS\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m400\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"GRU\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m     \u001b[0mtrain2\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mtest2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevaluatefull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgru_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmaxdifference\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0mtrain3\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mtest3\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevaluatefull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgru_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmaxdifference\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m.3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_14105/3058980606.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(train_loader, learn_rate, hidden_dim, EPOCHS, model_type)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m             \u001b[0;31m#print(\"out\",out)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/thesis/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_14105/3058980606.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, h)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgru\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/thesis/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/thesis/lib/python3.7/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    714\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    715\u001b[0m             result = _VF.gru(input, hx, self._flat_weights, self.bias, self.num_layers,\n\u001b[0;32m--> 716\u001b[0;31m                              self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[0m\u001b[1;32m    717\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m             result = _VF.gru(input, batch_sizes, hx, self._flat_weights, self.bias,\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "lr = 0.001\n",
    "batch_size = 4\n",
    "#a batch size of 64, i.e. each element in the dataloader iterable will return a batch of 64 features and labels.\n",
    "\n",
    "X=State\n",
    "y=Labels[:,lookback-1]\n",
    "print(\"x.shape\",X.shape)\n",
    "y=y.reshape(runqty,1)\n",
    "\n",
    "\n",
    "\n",
    "#print(int(time.time()))\n",
    "\n",
    "trainlist2=[]\n",
    "testlist2=[]\n",
    "trainlist3=[]\n",
    "testlist3=[]\n",
    "trainlist4=[]\n",
    "testlist4=[]\n",
    "trainlist5=[]\n",
    "testlist5=[]\n",
    "\n",
    "trainlist2b=[]\n",
    "testlist2b=[]\n",
    "trainlist3b=[]\n",
    "testlist3b=[]\n",
    "trainlist4b=[]\n",
    "testlist4b=[]\n",
    "trainlist5b=[]\n",
    "testlist5b=[]\n",
    "test_size=.25\n",
    "for i in range(5):\n",
    "    random_seed=int(time.time())\n",
    "    train_x, test_x, train_y,test_y = train_test_split(X, y, test_size=.25, #0.33, \n",
    "                                                       random_state=random_seed)\n",
    "\n",
    "    train_data = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\n",
    "    train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size, drop_last=True)\n",
    "    test_data   = TensorDataset( torch.from_numpy( test_x ), torch.from_numpy( test_y ) )\n",
    "    test_loader = DataLoader( test_data, shuffle = True, batch_size = batch_size, drop_last = True )\n",
    "\n",
    "\n",
    "    gru_model = train(train_loader, lr , hidden_dim=128, EPOCHS=400, model_type=\"GRU\")\n",
    "    train2 ,test2=evaluatefull(gru_model, train_x, train_y, test_x, test_y,maxdifference=.2)\n",
    "    train3 ,test3=evaluatefull(gru_model, train_x, train_y, test_x, test_y,maxdifference=.3)\n",
    "    train4 ,test4=evaluatefull(gru_model, train_x, train_y, test_x, test_y,maxdifference=.4)\n",
    "    train5 ,test5=evaluatefull(gru_model, train_x, train_y, test_x, test_y,maxdifference=.5)\n",
    "    trainlist2.append(train2)\n",
    "    testlist2.append(test2)\n",
    "    trainlist3.append(train3)\n",
    "    testlist3.append(test3)\n",
    "    trainlist4.append(train4)\n",
    "    testlist4.append(test4)\n",
    "    trainlist5.append(train5)\n",
    "    testlist5.append(test5)\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "test_size=.33\n",
    "for i in range(5):\n",
    "    random_seed=int(time.time())\n",
    "    train_x, test_x, train_y,test_y = train_test_split(X, y, test_size=.33, #0.33, \n",
    "                                                       random_state=random_seed)\n",
    "\n",
    "    train_data = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\n",
    "    train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size, drop_last=True)\n",
    "    test_data   = TensorDataset( torch.from_numpy( test_x ), torch.from_numpy( test_y ) )\n",
    "    test_loader = DataLoader( test_data, shuffle = True, batch_size = batch_size, drop_last = True )\n",
    "\n",
    "\n",
    "    gru_model = train(train_loader, lr , hidden_dim=128, EPOCHS=400, model_type=\"GRU\")\n",
    "    train2 ,test2=evaluatefull(gru_model, train_x, train_y, test_x, test_y,maxdifference=.2)\n",
    "    train3 ,test3=evaluatefull(gru_model, train_x, train_y, test_x, test_y,maxdifference=.3)\n",
    "    train4 ,test4=evaluatefull(gru_model, train_x, train_y, test_x, test_y,maxdifference=.4)\n",
    "    train5 ,test5=evaluatefull(gru_model, train_x, train_y, test_x, test_y,maxdifference=.5)\n",
    "    trainlist2b.append(train2)\n",
    "    testlist2bappend(test2)\n",
    "    trainlist3b.append(train3)\n",
    "    testlist3b.append(test3)\n",
    "    trainlist4b.append(train4)\n",
    "    testlist4b.append(test4)\n",
    "    trainlist5b.append(train5)\n",
    "    testlist5b.append(test5)\n",
    "\n",
    "    \n",
    "print(\"at test_size= .25 : \")\n",
    "print(\"trainlist2\",trainlist2)\n",
    "print(\"trainlist3\",trainlist3)\n",
    "print(\"trainlist4\",trainlist4)\n",
    "print(\"trainlist5\",trainlist5)\n",
    "\n",
    "print(\"testlist2\",testlist2)\n",
    "print(\"testlist3\",testlist3)\n",
    "print(\"testlist4\",testlist4)\n",
    "print(\"testlist5\",testlist5)\n",
    "\n",
    "print(\"at test_size=.33 : \")\n",
    "print(\"trainlist2\",trainlist2b)\n",
    "print(\"trainlist3\",trainlist3b)\n",
    "print(\"trainlist4\",trainlist4b)\n",
    "print(\"trainlist5\",trainlist5b)\n",
    "\n",
    "print(\"testlist2\",testlist2b)\n",
    "print(\"testlist3\",testlist3b)\n",
    "print(\"testlist4\",testlist4b)\n",
    "print(\"testlist5\",testlist5b)\n",
    " \n",
    "\n",
    "#vs training data= 490 / 495  vs test data= 128 / 165 (77%) from 128 hidden dim,  batch size of 4, lr =0.001,test_size=.25,  400 epoch\n",
    "#vs training data= 492 / 495  vs test data= 130 / 165 (78%) from 128 hidden dim,  batch size of 4, lr =0.0005,test_size=.25,  400 epoch\n",
    "#vs training data= 411 / 495  vs test data= 103 / 165 62%) from 128 hidden dim,  batch size of 4, lr =0.0001,test_size=.25,  400 epoch\n",
    "#vs training data= 451 / 495  vs test data= 113 / 165 68%) from 128 hidden dim,  batch size of 4, lr =0.0001,test_size=.25,  600 epoch\n",
    "#vs training data= 472 / 495  vs test data= 133 / 165 80%) from 128 hidden dim,  batch size of 4, lr =0.002,test_size=.25,  600 epoch\n",
    "\n",
    "\n",
    "#vs training data= 472 / 495  vs test data= 133 / 165 80%) from 128 hidden dim,  batch size of 4, lr =0.002,test_size=.25,  400 epoch\n",
    "#vs training data= 487 / 495  vs test data= 134 / 165 81% at 0.2 cuttoff\n",
    " #vs training data= 490 / 495  vs test data= 142 / 165 86.06060606060606 % at 0.3 cuttoff\n",
    " # vs training data= 491 / 495  vs test data= 146 / 165 88.48484848484848 % at 0.4 cuttoff\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#With full data set (uneven amts of success/fail) from 128 hidden dim,  batch size of 4, lr =0.0005,test_size=.25,  400 epoch\n",
    "#vs training data= 893 / 900  vs test data= 227 / 300 75 % at max difference 0.2\n",
    "# vs training data= 898 / 900  vs test data= 238 / 300 79 % at max difference 0.3\n",
    "# vs training data= 900 / 900  vs test data= 240 / 300 80 % at max difference 0.4\n",
    "# vs training data= 900 / 900  vs test data= 243 / 300 81 % at max difference 0.5\n",
    "\n",
    "\n",
    "#with even success/fail amounts (10-6):\n",
    "\n",
    "# vs training data= 494 / 495  vs test data= 131 / 165 79 % at max difference 0.2\n",
    "# vs training data= 495 / 495  vs test data= 138 / 165 83 % at max difference 0.3\n",
    "# vs training data= 495 / 495  vs test data= 140 / 165 84 % at max difference 0.4\n",
    "# vs training data= 495 / 495  vs test data= 144 / 165 87 % at max difference 0.5\n",
    "\n",
    "#with even success/fail amounts (10-4 and 10-6): (batch size of 4\n",
    "# vs training data= 653 / 675  vs test data= 168 / 225 74 % at max difference 0.2\n",
    "#vs training data= 663 / 675  vs test data= 180 / 225 80 % at max difference 0.3\n",
    "# vs training data= 667 / 675  vs test data= 189 / 225 84 % at max difference 0.4\n",
    "#vs training data= 670 / 675  vs test data= 196 / 225 87 % at max difference 0.5\n",
    "\n",
    "#with even success/fail amounts (10-4 and 10-6) (batch size of 8):\n",
    "# vs training data= 670 / 675  vs test data= 174 / 225 77 % at max difference 0.2\n",
    "# vs training data= 673 / 675  vs test data= 190 / 225 84 % at max difference 0.3\n",
    "# vs training data= 675 / 675  vs test data= 191 / 225 84 % at max difference 0.4\n",
    "# vs training data= 675 / 675  vs test data= 193 / 225 85 % at max difference 0.5\n",
    "\n",
    "#with even success/fail amounts (10-4 and 10-6) (batch size of 2):\n",
    "# vs training data= 662 / 675  vs test data= 178 / 225 79 % at max difference 0.2\n",
    "# vs training data= 673 / 675  vs test data= 186 / 225 82 % at max difference 0.3\n",
    "# vs training data= 673 / 675  vs test data= 191 / 225 84 % at max difference 0.4\n",
    "# vs training data= 674 / 675  vs test data= 199 / 225 88 % at max difference 0.5\n",
    "\n",
    "#with even success/fail amounts (10-4 and 10-6) (batch size of 2, lr=0.0005):\n",
    "# vs training data= 673 / 675  vs test data= 175 / 225 77 % at max difference 0.2\n",
    "# vs training data= 674 / 675  vs test data= 189 / 225 84 % at max difference 0.3\n",
    "# vs training data= 674 / 675  vs test data= 197 / 225 87 % at max difference 0.4\n",
    "# vs training data= 675 / 675  vs test data= 204 / 225 90 % at max difference 0.5\n",
    "\n",
    "#with even success/fail amounts (10-4 and 10-6) (batch size of 4, lr=0.002):\n",
    "# vs training data= 588 / 675  vs test data= 175 / 225 77 % at max difference 0.2\n",
    "#vs training data= 626 / 675  vs test data= 187 / 225 83 % at max difference 0.3\n",
    "## vs training data= 637 / 675  vs test data= 197 / 225 87 % at max difference 0.4\n",
    "# vs training data= 648 / 675  vs test data= 204 / 225 90 % at max difference 0.5\n",
    "\n",
    "#The target size means the label size!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_episode(model, data,  maxdifference=0.2, verbose=False):\n",
    "\n",
    "   \n",
    "    model.eval()\n",
    "    inp = torch.from_numpy(np.array(data)) # should be 5x1\n",
    "    h = model.init_hidden(inp.shape[0])\n",
    "    #print(\"inp\",inp)\n",
    "    #print(\"labs\",labs)\n",
    "    #print(\"h\",h)\n",
    "    out, h = model(inp.to(device).float(), h)\n",
    "    print(\"model output\",out)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 10)\n",
      "(1, 5, 10)\n",
      "model output tensor([[1.0033]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "actual [1.]\n"
     ]
    }
   ],
   "source": [
    "#print(train_x[0])\n",
    "print(train_x[0].shape)\n",
    "\n",
    "\n",
    "exampledata=np.expand_dims(train_x[0], axis=0)\n",
    "\n",
    "print(exampledata.shape)\n",
    "evaluate_episode(gru_model, exampledata)\n",
    "print(\"actual\",train_y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=gru_model\n",
    "i=1\n",
    "inp = torch.from_numpy(np.array(test_x))\n",
    "labs = torch.from_numpy(np.array(test_y))\n",
    "#h = model.init_hidden(inp.shape[0])\n",
    "h = model.init_hidden(inp.shape[0])\n",
    "#print(\"inp\",inp)\n",
    "\n",
    "#print(\"INP SHAPE\",inp.shape)\n",
    "#print(\"INP SHAPE[0]\",inp.shape[0])\n",
    "#print(\"labs\",labs)\n",
    "#print(\"h\",h)\n",
    "#print(\"h.shape\",h.shape)\n",
    "#print(inp.to(device).float())\n",
    "#print(inp.to(device).float().shape)\n",
    "print(inp.to(device).float().shape)\n",
    "\n",
    "out, h = model(inp.to(device).float(), h)\n",
    "#print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(test_y.reshape(-1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
