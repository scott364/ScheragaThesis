{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we'll be using a GRU model for a time series prediction task and we will compare the performance of the GRU model against an LSTM model as well. The dataset that we will be using is the Hourly Energy Consumption dataset which can be found on [Kaggle](https://www.kaggle.com/robikscube/hourly-energy-consumption). The dataset contains power consumption data across different regions around the United States recorded on an hourly basis.\n",
    "\n",
    "You can run the code implementation in this article on FloydHub using their GPUs on the cloud by clicking the following link and using the main.ipynb notebook.\n",
    "\n",
    "[![Run on FloydHub](https://static.floydhub.com/button/button-small.svg)](https://floydhub.com/run?template=https://github.com/gabrielloye/https://github.com/gabrielloye/GRU_Prediction)\n",
    "\n",
    "This will speed up the training process significantly. Alternatively, the link to the GitHub repository can be found [here]().\n",
    "\n",
    "The goal of this implementation is to create a model that can accurately predict the energy usage in the next hour given historical usage data. We will be using both the GRU and LSTM model to train on a set of historical data and evaluate both models on an unseen test set. To do so, weâ€™ll start with feature selection, data-preprocessing, followed by defining, training and eventually evaluating the models.\n",
    "\n",
    "We will be using the PyTorch library to implement both types of models along with other common Python libraries used in data analytics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.python-engineer.com/posts/pytorch-rnn-lstm-gru/\n",
    "\n",
    "#https://blog.floydhub.com/gru-with-pytorch/\n",
    "\n",
    "import os\n",
    "import time\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import random\n",
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from tqdm import tqdm_notebook\n",
    "from sklearn.preprocessing import MinMaxScaler,QuantileTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Local ###\n",
    "#from data_processing import *\n",
    "\n",
    "\n",
    "\n",
    "# Define data root directory\n",
    "\n",
    "#data_dir = \"./data/\"\n",
    "#print(os.listdir(data_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a total of **12** *.csv* files containing hourly energy trend data (*'est_hourly.paruqet'* and *'pjm_hourly_est.csv'* are not used). In our next step, we will be reading these files and pre-processing these data in this order:\n",
    "- Getting the time data of each individual time step and generalizing them\n",
    "    - Hour of the day *i.e. 0-23*\n",
    "    - Day of the week *i.e. 1-7*\n",
    "    - Month *i.e. 1-12*\n",
    "    - Day of the year *i.e. 1-365*\n",
    "    \n",
    "    \n",
    "- Scale the data to values between 0 and 1\n",
    "    - Algorithms tend to perform better or converge faster when features are on a relatively similar scale and/or close to normally distributed\n",
    "    - Scaling preserves the shape of the original distribution and doesn't reduce the importance of outliers.\n",
    "    \n",
    "    \n",
    "- Group the data into sequences to be used as inputs to the model and store their corresponding labels\n",
    "    - The **sequence length** or **lookback period** is the number of data points in history that the model will use to make the prediction\n",
    "    - The label will be the next data point in time after the last one in the input sequence\n",
    "    \n",
    "\n",
    "- The inputs and labels will then be split into training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# torch.cuda.is_available() checks and returns a Boolean True if a GPU is available, else it'll return False\n",
    "is_cuda = torch.cuda.is_available()\n",
    "\n",
    "# If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.\n",
    "if is_cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    \n",
    "print(device)\n",
    "\n",
    "\n",
    "def get_torch_device( v=0 ):\n",
    "    # torch.cuda.is_available() checks and returns a Boolean True if a GPU is available, else it'll return False\n",
    "    is_cuda = torch.cuda.is_available()\n",
    "    # If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.\n",
    "    if is_cuda:\n",
    "        device = torch.device(\"cuda\")\n",
    "        if v:  print( \"CUDA Available!\" )\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        if v:  print( \"NO CUDA\" )\n",
    "    return device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16176, 10)\n",
      "(20880, 10)\n",
      "(37056, 10)\n"
     ]
    }
   ],
   "source": [
    "#choppeddata=pd.read_csv('choppeddata_10_06_2021.csv')#.head()\n",
    "choppeddata1=pd.read_csv('choppeddata_10_04_2021_randomselector_uneven.csv')#.head()\n",
    "choppeddata2=pd.read_csv('choppeddata_10_06_2021_randomselector_uneven.csv')#.head()\n",
    "print(choppeddata1.shape)\n",
    "print(choppeddata2.shape)\n",
    "frames = [choppeddata1, choppeddata2]\n",
    "choppeddata = pd.concat(frames)\n",
    "print(choppeddata.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(37056, 10)\n",
      "total runs: 6176\n"
     ]
    }
   ],
   "source": [
    "print(choppeddata.shape)\n",
    "runqty=int(choppeddata.shape[0]/6)\n",
    "print(\"total runs:\",runqty)\n",
    "choppedheaders=[]\n",
    "lookback=10 #save only the last 11 timesteps\n",
    "for i in range(lookback):  \n",
    "    label=str(i)\n",
    "    choppedheaders.append(\"header\"+label)\n",
    "\n",
    "#put chopped data in np.arrays\n",
    "State=np.zeros((runqty,5,lookback)) #96 runs,with 5 sets of data (x,y,z,roll,pitch) each, and each run is 11 timesteps long\n",
    "Labels=np.zeros((runqty,lookback)) #96 runs, each run is 11 timesteps long\n",
    "runcounter=0\n",
    "\n",
    "for i in range(0,choppeddata.shape[0],6):\n",
    "            State[runcounter][0][:]=(choppeddata[choppedheaders[:]].iloc[i]).tolist()\n",
    "            State[runcounter][1][:]=(choppeddata[choppedheaders[:]].iloc[i+1]).tolist()\n",
    "            State[runcounter][2][:]=(choppeddata[choppedheaders[:]].iloc[i+2]).tolist()\n",
    "            State[runcounter][3][:]=(choppeddata[choppedheaders[:]].iloc[i+3]).tolist()\n",
    "            State[runcounter][4][:]=(choppeddata[choppedheaders[:]].iloc[i+4]).tolist()\n",
    "            Labels[runcounter][:]=(choppeddata[choppedheaders[:]].iloc[i+5]).tolist()  #labels   \n",
    "            runcounter+=1\n",
    "#print(State[0])\n",
    "#print(Labels)\n",
    "#print(Labels[:,9]) #just getting finals labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape (6176, 5, 10)\n",
      "Test set X size 77200\n",
      "Train set Y size 4632\n",
      "Test set X size 77200\n",
      "Test set Y size 1544\n"
     ]
    }
   ],
   "source": [
    "#X= range(0,575,6)\n",
    "#y= range(0,575,6)\n",
    "\n",
    "X=State\n",
    "y=Labels[:,lookback-1]\n",
    "print(\"x.shape\",X.shape)\n",
    "\n",
    "y=y.reshape(runqty,1)\n",
    "\n",
    "random_seed=int(time.time())\n",
    "#print(int(time.time()))\n",
    "train_x, test_x, train_y,test_y = train_test_split(X, y, test_size=.25, #0.33, \n",
    "                                                   random_state=random_seed)\n",
    "#print(\"Train\")\n",
    "#print(train_x[0])\n",
    "#print(train_y[0])\n",
    "print(\"Test set X size\", test_x.size)\n",
    "print(\"Train set Y size\", train_y.size)\n",
    "#print(test_x[0])\n",
    "#print(test_y[0])\n",
    "print(\"Test set X size\", test_x.size)\n",
    "print(\"Test set Y size\", test_y.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a total of 980,185 sequences of training data\n",
    "\n",
    "To improve the speed of our training, we can process the data in batches so that the model does not need to update its weights as frequently. The Torch *Dataset* and *DataLoader* classes are useful for splitting our data into batches and shuffling them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataloader.DataLoader object at 0x7f8ebb2f8dd0>\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "#a batch size of 64, i.e. each element in the dataloader iterable will return a batch of 64 features and labels.\n",
    "\n",
    "train_data = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size, drop_last=True)\n",
    "\n",
    "test_data   = TensorDataset( torch.from_numpy( test_x ), torch.from_numpy( test_y ) )\n",
    "test_loader = DataLoader( test_data, shuffle = True, batch_size = batch_size, drop_last = True )\n",
    "print(train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also check if we have any GPUs to speed up our training time by many folds. If youâ€™re using FloydHub with GPU to run this code, the training time will be significantly reduced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUNet(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, n_layers, drop_prob=0.2):\n",
    "        super(GRUNet, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.gru = nn.GRU(input_dim, hidden_dim, n_layers, batch_first=True, dropout=drop_prob)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x, h):\n",
    "        out, h = self.gru(x, h)\n",
    "        out = self.fc(self.relu(out[:,-1]))\n",
    "        return out, h\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        hidden = weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device)\n",
    "        return hidden\n",
    "def train(train_loader, learn_rate, hidden_dim=128, EPOCHS=400, model_type=\"GRU\"):\n",
    "    #got  109 / 180 on training set, 29 / 60 on test set from 128 hidden dim, 50 epoch, batch size of 4, lr =0.001\n",
    "    #Got training data= 146 / 180, success vs test data= 38 / 60 with same as above but 100 epoch\n",
    "    #Got training data= 172 / 180, success vs test data= 46 / 60 with same as above but 200 epoch\n",
    "    #Got training data= 165 / 180, success vs test data= 52 / 60 with same as above but 200 epoch\n",
    "    \n",
    "    losslist=[]\n",
    "    # Setting common hyperparameters\n",
    "    input_dim = next(iter(train_loader))[0].shape[2]  #  = 11\n",
    "    #print(input_dim)\n",
    "    #print(\"input_dim\",input_dim)\n",
    "    output_dim = 1\n",
    "    n_layers = 2\n",
    "    # Instantiating the models\n",
    "    if model_type == \"GRU\":\n",
    "        model = GRUNet(input_dim, hidden_dim, output_dim, n_layers)\n",
    "    else:\n",
    "        model = LSTMNet(input_dim, hidden_dim, output_dim, n_layers)\n",
    "    model.to(device)\n",
    "    \n",
    "    # Defining loss function and optimizer\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learn_rate)\n",
    "    \n",
    "    model.train()\n",
    "    print(\"Starting Training of {} model\".format(model_type))\n",
    "    epoch_times = []\n",
    "    # Start training loop\n",
    "    for epoch in range(1,EPOCHS+1):\n",
    "        start_time = time.clock()\n",
    "        h = model.init_hidden(batch_size)\n",
    "        avg_loss = 0.\n",
    "        counter = 0\n",
    "        for x, label in train_loader:\n",
    "            #print(\"x\",x)\n",
    "            #print(\"label\",label)\n",
    "            counter += 1\n",
    "            if model_type == \"GRU\":\n",
    "                h = h.data\n",
    "            else:\n",
    "                h = tuple([e.data for e in h])\n",
    "            model.zero_grad()\n",
    "            \n",
    "            out, h = model(x.to(device).float(), h)\n",
    "            #print(\"out\",out)\n",
    "            loss = criterion(out, label.to(device).float())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            avg_loss += loss.item()\n",
    "            \n",
    "            if counter%20000 == 0:\n",
    "                print(\"Epoch {}......Step: {}/{}....... Average Loss for Epoch: {}\".format(epoch, counter, len(train_loader), avg_loss/counter))\n",
    "        current_time = time.clock()\n",
    "        if epoch%40 == 0:\n",
    "            print(\"Epoch {}/{} Done, Total Loss: {}   Time Elapsed: {} seconds\".format(epoch, EPOCHS, avg_loss/len(train_loader),str(current_time-start_time)))\n",
    "        \n",
    "            #print(\"Total\".format())\n",
    "        losslist.append(avg_loss/len(train_loader))\n",
    "        epoch_times.append(current_time-start_time)\n",
    "    print(\"Total Training Time: {} seconds\".format(str(sum(epoch_times))))\n",
    "    plt.plot(losslist)\n",
    "    plt.title(\"Loss\")\n",
    "    plt.show()\n",
    "    return model\n",
    "\n",
    "def evaluate(model, test_x, test_y):\n",
    "    model.eval()\n",
    "    outputs = []\n",
    "    targets = []\n",
    "    start_time = time.clock()\n",
    "    #for i in test_x.keys():\n",
    "    for i in range( len( test_x ) ):    \n",
    "        inp = torch.from_numpy(np.array(test_x[i])) # should be 5x1\n",
    "        labs = torch.from_numpy(np.array(test_y[i])) #should be 1x1\n",
    "        h = model.init_hidden(inp.shape[0])\n",
    "        #print(\"inp\",inp)\n",
    "        #print(\"labs\",labs)\n",
    "        #print(\"h\",h)\n",
    "        out, h = model(inp.to(device).float(), h)\n",
    "        #outputs.append(label_scalers[i].inverse_transform(out.cpu().detach().numpy()).reshape(-1))\n",
    "        #targets.append(label_scalers[i].inverse_transform(labs.numpy()).reshape(-1))\n",
    "        outputs.append( out.cpu().detach().numpy().reshape(-1) )\n",
    "        targets.append( labs.numpy().reshape(-1) )\n",
    "        \n",
    "    print(\"Evaluation Time: {}\".format(str(time.clock()-start_time)))\n",
    "    sMAPE = 0\n",
    "    for i in range(len(outputs)):\n",
    "        sMAPE += np.mean(abs(outputs[i]-targets[i])/(targets[i]+outputs[i])/2)/len(outputs)\n",
    "    print(\"sMAPE: {}%\".format(sMAPE*100))\n",
    "    return outputs, targets, sMAPE\n",
    "                               \n",
    "def evaluate2(model, test_x, test_y):\n",
    "    model.eval()\n",
    "    outputs = []\n",
    "    targets = []  #labels\n",
    "    start_time = time.clock()\n",
    "    #for i in test_x.keys():\n",
    "    #for i in range( len( test_x ) ):    \n",
    "    inp = torch.from_numpy(np.array(test_x)) # should be 5x1\n",
    "    labs = torch.from_numpy(np.array(test_y)) #should be 1x1\n",
    "    h = model.init_hidden(inp.shape[0])\n",
    "    #print(\"inp\",inp)\n",
    "    #print(\"labs\",labs)\n",
    "    #print(\"h\",h)\n",
    "    out, h = model(inp.to(device).float(), h)\n",
    "    #outputs.append(label_scalers[i].inverse_transform(out.cpu().detach().numpy()).reshape(-1))\n",
    "    #targets.append(label_scalers[i].inverse_transform(labs.numpy()).reshape(-1))\n",
    "    outputs.append( out.cpu().detach().numpy().reshape(-1) )\n",
    "    targets.append( labs.numpy().reshape(-1) )\n",
    "        \n",
    "    print(\"Evaluation Time: {}\".format(str(time.clock()-start_time)))\n",
    "    sMAPE = 0\n",
    "    for i in range(len(outputs)):\n",
    "        sMAPE += np.mean(abs(outputs[i]-targets[i])/(targets[i]+outputs[i])/2)/len(outputs)\n",
    "    print(\"sMAPE: {}%\".format(sMAPE*100))\n",
    "    return outputs, targets, sMAPE                               \n",
    "\n",
    "def evaluatefull(model, train_x, train_y, test_x, test_y,maxdifference=0.2, verbose=False):\n",
    "\n",
    "    m = nn.ReLU()\n",
    "    #m = nn.Sigmoid()\n",
    "    #output = m(input)\n",
    "    print(\"Vs Training Set\")\n",
    "    gru_outputs, targets, gru_sMAPE = evaluate2(gru_model, train_x, train_y)\n",
    "    #print(test_y)\n",
    "    #print(gru_outputs)\n",
    "    #print(gru_outputs[0][5])\n",
    "\n",
    "\n",
    "    testy=test_y.reshape(-1)\n",
    "    trainy=train_y.reshape(-1)\n",
    "\n",
    "\n",
    "    #print(\"Train size:\",trainy.size)\n",
    "    print(gru_outputs[0][4])\n",
    "    train_successcounter=0\n",
    "    for i in range(int(trainy.size)):\n",
    "        #print(testy[i],gru_outputs[0][i],m(torch.tensor(gru_outputs[0][i])))\n",
    "        #print(train[i],gru_outputs[0][i],m(torch.tensor(gru_outputs[0][i])))\n",
    "\n",
    "\n",
    "        #print(trainy[i],gru_outputs[0][i], m(torch.tensor(gru_outputs[0][i])))\n",
    "\n",
    "\n",
    "        if abs(trainy[i]-gru_outputs[0][i])<maxdifference :\n",
    "            train_successcounter+=1\n",
    "        #print(testy[i])\n",
    "        #print\n",
    "        #output = m(input)\n",
    "\n",
    "\n",
    "\n",
    "    test_successcounter=0\n",
    "    print(\"Vs Test Set\")\n",
    "    gru_outputs, targets, gru_sMAPE = evaluate2(gru_model, test_x, test_y)\n",
    "    #print(\"test size: \",testy.size)\n",
    "\n",
    "    for i in range(int(testy.size)):\n",
    "\n",
    "\n",
    "        #, m(torch.tensor(gru_outputs[0][i])))\n",
    "\n",
    "\n",
    "        if abs(testy[i]-gru_outputs[0][i])<maxdifference :\n",
    "            test_successcounter+=1\n",
    "            if verbose==True:\n",
    "                print(testy[i],gru_outputs[0][i], \"OK\" )\n",
    "        else:\n",
    "            if verbose==True:\n",
    "                print(testy[i],gru_outputs[0][i], \"X\" )\n",
    "            #print(testy[i])\n",
    "        #print\n",
    "        #output = m(input)\n",
    "    print(\"\")\n",
    "    print(\" vs training data=\" ,train_successcounter,\"/\",trainy.size, \" vs test data=\" ,\n",
    "          test_successcounter,\"/\",testy.size,int(100*test_successcounter/testy.size),\"%\", \"at max difference\",maxdifference )\n",
    "    return ( train_successcounter ,test_successcounter)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape (6176, 5, 10)\n",
      "Starting Training of GRU model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/scott/anaconda3/envs/thesis/lib/python3.7/site-packages/ipykernel_launcher.py:49: DeprecationWarning: time.clock has been deprecated in Python 3.3 and will be removed from Python 3.8: use time.perf_counter or time.process_time instead\n",
      "/home/scott/anaconda3/envs/thesis/lib/python3.7/site-packages/ipykernel_launcher.py:72: DeprecationWarning: time.clock has been deprecated in Python 3.3 and will be removed from Python 3.8: use time.perf_counter or time.process_time instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40/1000 Done, Total Loss: 0.0597114030064808   Time Elapsed: 0.31270000000000664 seconds\n",
      "Epoch 80/1000 Done, Total Loss: 0.054444592646581844   Time Elapsed: 0.3202349999999967 seconds\n",
      "Epoch 120/1000 Done, Total Loss: 0.048947669743534386   Time Elapsed: 0.2973039999999969 seconds\n",
      "Epoch 160/1000 Done, Total Loss: 0.047309003838260345   Time Elapsed: 0.3084129999999732 seconds\n",
      "Epoch 200/1000 Done, Total Loss: 0.042733543882301696   Time Elapsed: 0.30973900000000754 seconds\n",
      "Epoch 240/1000 Done, Total Loss: 0.03848541768093128   Time Elapsed: 0.29849000000001524 seconds\n",
      "Epoch 280/1000 Done, Total Loss: 0.036653354890101277   Time Elapsed: 0.30891900000000305 seconds\n",
      "Epoch 320/1000 Done, Total Loss: 0.03306031373439408   Time Elapsed: 0.31453399999998055 seconds\n",
      "Epoch 360/1000 Done, Total Loss: 0.030221995600085292   Time Elapsed: 0.32676100000000474 seconds\n",
      "Epoch 400/1000 Done, Total Loss: 0.027038710195079654   Time Elapsed: 0.3090049999999849 seconds\n",
      "Epoch 440/1000 Done, Total Loss: 0.023638673964241106   Time Elapsed: 0.3245509999999854 seconds\n",
      "Epoch 480/1000 Done, Total Loss: 0.02074019258240393   Time Elapsed: 0.32538600000000883 seconds\n",
      "Epoch 520/1000 Done, Total Loss: 0.02062139993843933   Time Elapsed: 0.3306560000000047 seconds\n",
      "Epoch 560/1000 Done, Total Loss: 0.014780625207802385   Time Elapsed: 0.33578699999998207 seconds\n",
      "Epoch 600/1000 Done, Total Loss: 0.012738553488614142   Time Elapsed: 0.3099799999999959 seconds\n",
      "Epoch 640/1000 Done, Total Loss: 0.011726065305992961   Time Elapsed: 0.30873600000001034 seconds\n",
      "Epoch 680/1000 Done, Total Loss: 0.009150070046113493   Time Elapsed: 0.30754300000000967 seconds\n",
      "Epoch 720/1000 Done, Total Loss: 0.009513174529678913   Time Elapsed: 0.2987810000000195 seconds\n",
      "Epoch 760/1000 Done, Total Loss: 0.008078028189225329   Time Elapsed: 0.32178299999998217 seconds\n",
      "Epoch 800/1000 Done, Total Loss: 0.007370876736078774   Time Elapsed: 0.32079600000002984 seconds\n",
      "Epoch 840/1000 Done, Total Loss: 0.007013274658270853   Time Elapsed: 0.307534999999973 seconds\n",
      "Epoch 880/1000 Done, Total Loss: 0.006467250111907358   Time Elapsed: 0.31506400000000667 seconds\n",
      "Epoch 920/1000 Done, Total Loss: 0.0053572228854237534   Time Elapsed: 0.3036470000000122 seconds\n",
      "Epoch 960/1000 Done, Total Loss: 0.005845272561095448   Time Elapsed: 0.30007599999999 seconds\n",
      "Epoch 1000/1000 Done, Total Loss: 0.0043431213602565955   Time Elapsed: 0.3051999999999566 seconds\n",
      "Total Training Time: 312.5904429999993 seconds\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vs Training Set\n",
      "Evaluation Time: 0.004255999999998039\n",
      "sMAPE: -9.200380134996786%\n",
      "0.016716316\n",
      "Vs Test Set\n",
      "Evaluation Time: 0.0016839999999547217\n",
      "sMAPE: 1.5201214278382031%\n",
      "\n",
      " vs training data= 4613 / 4632  vs test data= 1353 / 1544 87 % at max difference 0.2\n",
      "Vs Training Set\n",
      "Evaluation Time: 0.003287000000000262\n",
      "sMAPE: -9.200380134996786%\n",
      "0.016716316\n",
      "Vs Test Set\n",
      "Evaluation Time: 0.0020370000000298205\n",
      "sMAPE: 1.5201214278382031%\n",
      "\n",
      " vs training data= 4625 / 4632  vs test data= 1412 / 1544 91 % at max difference 0.3\n",
      "Vs Training Set\n",
      "Evaluation Time: 0.0037360000000035143\n",
      "sMAPE: -9.200380134996786%\n",
      "0.016716316\n",
      "Vs Test Set\n",
      "Evaluation Time: 0.001630999999974847\n",
      "sMAPE: 1.5201214278382031%\n",
      "\n",
      " vs training data= 4630 / 4632  vs test data= 1442 / 1544 93 % at max difference 0.4\n",
      "Vs Training Set\n",
      "Evaluation Time: 0.0028910000000337277\n",
      "sMAPE: -9.200380134996786%\n",
      "0.016716316\n",
      "Vs Test Set\n",
      "Evaluation Time: 0.001998000000014599\n",
      "sMAPE: 1.5201214278382031%\n",
      "\n",
      " vs training data= 4630 / 4632  vs test data= 1461 / 1544 94 % at max difference 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/scott/anaconda3/envs/thesis/lib/python3.7/site-packages/ipykernel_launcher.py:115: DeprecationWarning: time.clock has been deprecated in Python 3.3 and will be removed from Python 3.8: use time.perf_counter or time.process_time instead\n",
      "/home/scott/anaconda3/envs/thesis/lib/python3.7/site-packages/ipykernel_launcher.py:130: DeprecationWarning: time.clock has been deprecated in Python 3.3 and will be removed from Python 3.8: use time.perf_counter or time.process_time instead\n"
     ]
    }
   ],
   "source": [
    "lr = 0.0005\n",
    "batch_size = 32\n",
    "#a batch size of 64, i.e. each element in the dataloader iterable will return a batch of 64 features and labels.\n",
    "\n",
    "X=State\n",
    "y=Labels[:,lookback-1]\n",
    "print(\"x.shape\",X.shape)\n",
    "y=y.reshape(runqty,1)\n",
    "\n",
    "\n",
    "random_seed=int(time.time())\n",
    "train_x, test_x, train_y,test_y = train_test_split(X, y, test_size=.25, #0.33, \n",
    "                                                   random_state=random_seed)\n",
    "\n",
    "train_data = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size, drop_last=True)\n",
    "test_data   = TensorDataset( torch.from_numpy( test_x ), torch.from_numpy( test_y ) )\n",
    "test_loader = DataLoader( test_data, shuffle = True, batch_size = batch_size, drop_last = True )\n",
    "\n",
    "\n",
    "gru_model = train(train_loader, lr , hidden_dim=128, EPOCHS=1000, model_type=\"GRU\")\n",
    "train2 ,test2=evaluatefull(gru_model, train_x, train_y, test_x, test_y,maxdifference=.2)\n",
    "train3 ,test3=evaluatefull(gru_model, train_x, train_y, test_x, test_y,maxdifference=.3)\n",
    "train4 ,test4=evaluatefull(gru_model, train_x, train_y, test_x, test_y,maxdifference=.4)\n",
    "train5 ,test5=evaluatefull(gru_model, train_x, train_y, test_x, test_y,maxdifference=.5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_episode(model, data,  maxdifference=0.2, verbose=False):\n",
    "\n",
    "   \n",
    "    model.eval()\n",
    "    inp = torch.from_numpy(np.array(data)) # should be 5x1\n",
    "    h = model.init_hidden(inp.shape[0])\n",
    "    #print(\"inp\",inp)\n",
    "    #print(\"labs\",labs)\n",
    "    #print(\"h\",h)\n",
    "    out, h = model(inp.to(device).float(), h)\n",
    "    #print(\"model output\",out)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10_11_2021\n",
      "model saved\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import time\n",
    "from datetime import date\n",
    "\n",
    "today = date.today()    \n",
    "todaydate = today.strftime(\"%m_%d_%Y\")\n",
    "print(todaydate)\n",
    "torch.save(gru_model,\"currentmodel_\"+todaydate+\".pt\")\n",
    "print(\"model saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test model on new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded\n"
     ]
    }
   ],
   "source": [
    "gru_model3=torch.load('currentmodel_10_11_2021.pt')\n",
    "gru_model3.eval()\n",
    "print(\"loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index= 44\n",
      "(1, 5, 10)\n",
      "prediction 0.2498023509979248   actual [0.]\n"
     ]
    }
   ],
   "source": [
    "randomindex=random.randint(0,225)\n",
    "print(\"index=\",randomindex)\n",
    "\n",
    "exampledata=np.expand_dims(test_x[207, 0:5, 0:10], axis=0)\n",
    "\n",
    "print(exampledata.shape)\n",
    "prediction=evaluate_episode(gru_model3, exampledata)\n",
    "\n",
    "print(\"prediction\",float(prediction), \"  actual\",test_y[randomindex])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## simlulate a buffer of 10 timesteps entering the classifier over a 1 episode, and classifying them. Filling empty data with zeroes or ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index= 188\n",
      "final partial data\n",
      "[[[1.         0.64779416 0.68590197 0.69517914 0.69903447 0.71459429\n",
      "   0.71267351 0.71409191 0.71673093 0.72399386]\n",
      "  [1.         0.52154944 0.52310862 0.53282553 0.52718083 0.53709991\n",
      "   0.52978522 0.54277099 0.53688102 0.54443726]\n",
      "  [1.         0.8726286  0.96482141 0.95083008 0.94447182 0.93691809\n",
      "   0.95239155 0.9493714  0.95276295 0.95638539]\n",
      "  [1.         0.50752062 0.50852903 0.49810241 0.50102952 0.4947311\n",
      "   0.50046923 0.49000781 0.49422322 0.48543027]\n",
      "  [1.         0.62007232 0.67079484 0.67376652 0.67304434 0.68618886\n",
      "   0.68665851 0.69233506 0.69529406 0.70288601]]]\n",
      "\n",
      "full data\n",
      "[[[0.64779416 0.68590197 0.69517914 0.69903447 0.71459429 0.71267351\n",
      "   0.71409191 0.71673093 0.72399386 0.7219988 ]\n",
      "  [0.52154944 0.52310862 0.53282553 0.52718083 0.53709991 0.52978522\n",
      "   0.54277099 0.53688102 0.54443726 0.53999553]\n",
      "  [0.8726286  0.96482141 0.95083008 0.94447182 0.93691809 0.95239155\n",
      "   0.9493714  0.95276295 0.95638539 0.94038139]\n",
      "  [0.50752062 0.50852903 0.49810241 0.50102952 0.4947311  0.50046923\n",
      "   0.49000781 0.49422322 0.48543027 0.49026894]\n",
      "  [0.62007232 0.67079484 0.67376652 0.67304434 0.68618886 0.68665851\n",
      "   0.69233506 0.69529406 0.70288601 0.70206539]]]\n",
      "predictions: [-0.15575464069843292, 0.23807184398174286, 0.38706398010253906, 0.21107828617095947, 0.10108674317598343, 0.1720225214958191, 0.42928677797317505, 0.3034760355949402, 0.030546963214874268]\n",
      "\n",
      "\n",
      "prediction from 10 timesteps 0.4351702332496643 actual [0.]\n"
     ]
    }
   ],
   "source": [
    "outputlist=[]\n",
    "\n",
    "randomindex=random.randint(0,225)\n",
    "print(\"index=\",randomindex)\n",
    "exampledata=np.expand_dims(test_x[randomindex, 0:5, 0:10], axis=0)\n",
    "\n",
    "\n",
    "#print(temparray.shape)\n",
    "#temparray=np.expand_dims(temparray, axis=1)\n",
    "\n",
    "#print(temparray.shape)\n",
    "#print(temparray)\n",
    "\n",
    "#temparray2=test_x[randomindex, 0:5, 0]\n",
    "#temparray2=np.expand_dims(temparray2, axis=1)\n",
    "\n",
    "for i in range(9):\n",
    "    if i!=10:\n",
    "        temparray=np.ones((5,1)) #test_x[randomindex, 0:5, 0]   #zeroes or \"ones\" here seems to work equally well. \n",
    "    \n",
    "    for j in range(8-i):\n",
    "        #temparray2=test_x[randomindex, 0:5, 0]\n",
    "        #temparray2=np.expand_dims(temparray2, axis=1)\n",
    "        temparray=np.append(temparray,np.ones((5,1)),axis=1)       #zeroes or \"ones\" here seems to work equally well. \n",
    "        #temparray=np.append(temparray,temparray2,axis=1)\n",
    "        #temparray=np.append(np.zeros((5,1)),temparray,axis=1)   \n",
    "    \n",
    "    for j in range(i+1):\n",
    "\n",
    "        temparray2=test_x[randomindex, 0:5, j]\n",
    "        temparray2=np.expand_dims(temparray2, axis=1)\n",
    "        temparray=np.append(temparray,temparray2,axis=1)\n",
    "        #temparray=np.append(np.zeros((5,1)),temparray,axis=1)\n",
    "\n",
    "    \n",
    "    #print(temparray)\n",
    "    temparray=np.expand_dims(temparray, axis=0)\n",
    "    outputpartial=evaluate_episode(gru_model3, temparray)\n",
    "    \n",
    "    \n",
    "    outputlist.append(float(outputpartial))\n",
    "\n",
    "print(\"final partial data\")\n",
    "print(temparray)   \n",
    "print(\"\")\n",
    "print(\"full data\")\n",
    "print(exampledata)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#print(\"prediction from\",x,\" timesteps\",float(outputpartial),\"actual\",test_y[randomindex])\n",
    "print(\"predictions:\",outputlist)\n",
    "\n",
    "\n",
    "#print(\"full data\")\n",
    "#print(exampledata)\n",
    "print(\"\")\n",
    "#print(\"evaluating all 10 timesteps\")\n",
    "\n",
    "outputfull=evaluate_episode(gru_model3, exampledata)\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "#print(\"evaluating 1st timestep repeated 10 times\")\n",
    "\n",
    "\n",
    "#print(\"prediction from 1 timestep\",float(outputpartial),\"actual\",test_y[randomindex])\n",
    "print(\"prediction from 10 timesteps\",float(outputfull),\"actual\",test_y[randomindex])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## classifying progression of 10 actual forces and torques in a sucessful sequence longer than 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7200, 30)\n",
      "['header0', 'header1', 'header2', 'header3', 'header4', 'header5', 'header6', 'header7', 'header8', 'header9', 'header10', 'header11', 'header12', 'header13', 'header14', 'header15', 'header16', 'header17', 'header18', 'header19', 'header20', 'header21', 'header22', 'header23', 'header24', 'header25', 'header26', 'header27', 'header28', 'header29']\n"
     ]
    }
   ],
   "source": [
    "originaldata=pd.read_csv('forcetorquebuttonresults_renormalized_10_06_2021.csv')#.head()\n",
    "print(originaldata.shape)\n",
    "headers=[]\n",
    "lookback=30 #save only the last 11 timesteps\n",
    "for i in range(lookback):  \n",
    "    label=str(i)\n",
    "    headers.append(\"header\"+label)\n",
    "print(headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>header0</th>\n",
       "      <th>header1</th>\n",
       "      <th>header2</th>\n",
       "      <th>header3</th>\n",
       "      <th>header4</th>\n",
       "      <th>header5</th>\n",
       "      <th>header6</th>\n",
       "      <th>header7</th>\n",
       "      <th>header8</th>\n",
       "      <th>header9</th>\n",
       "      <th>...</th>\n",
       "      <th>header20</th>\n",
       "      <th>header21</th>\n",
       "      <th>header22</th>\n",
       "      <th>header23</th>\n",
       "      <th>header24</th>\n",
       "      <th>header25</th>\n",
       "      <th>header26</th>\n",
       "      <th>header27</th>\n",
       "      <th>header28</th>\n",
       "      <th>header29</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4039</th>\n",
       "      <td>0.376258</td>\n",
       "      <td>0.371196</td>\n",
       "      <td>0.374959</td>\n",
       "      <td>0.381994</td>\n",
       "      <td>0.378138</td>\n",
       "      <td>0.375871</td>\n",
       "      <td>0.375117</td>\n",
       "      <td>0.372087</td>\n",
       "      <td>0.372973</td>\n",
       "      <td>0.372174</td>\n",
       "      <td>...</td>\n",
       "      <td>0.313334</td>\n",
       "      <td>0.297908</td>\n",
       "      <td>0.300916</td>\n",
       "      <td>0.295691</td>\n",
       "      <td>0.304691</td>\n",
       "      <td>0.297850</td>\n",
       "      <td>0.298842</td>\n",
       "      <td>0.298431</td>\n",
       "      <td>0.323285</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4040</th>\n",
       "      <td>0.932410</td>\n",
       "      <td>0.930584</td>\n",
       "      <td>0.929513</td>\n",
       "      <td>0.928026</td>\n",
       "      <td>0.928930</td>\n",
       "      <td>0.929664</td>\n",
       "      <td>0.929868</td>\n",
       "      <td>0.929412</td>\n",
       "      <td>0.930946</td>\n",
       "      <td>0.929839</td>\n",
       "      <td>...</td>\n",
       "      <td>0.887794</td>\n",
       "      <td>0.927183</td>\n",
       "      <td>0.923606</td>\n",
       "      <td>0.929387</td>\n",
       "      <td>0.921559</td>\n",
       "      <td>0.932170</td>\n",
       "      <td>0.925471</td>\n",
       "      <td>0.922702</td>\n",
       "      <td>0.851257</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4041</th>\n",
       "      <td>0.639841</td>\n",
       "      <td>0.640750</td>\n",
       "      <td>0.639536</td>\n",
       "      <td>0.639553</td>\n",
       "      <td>0.639504</td>\n",
       "      <td>0.639946</td>\n",
       "      <td>0.638688</td>\n",
       "      <td>0.639430</td>\n",
       "      <td>0.639626</td>\n",
       "      <td>0.639553</td>\n",
       "      <td>...</td>\n",
       "      <td>0.689476</td>\n",
       "      <td>0.707595</td>\n",
       "      <td>0.706714</td>\n",
       "      <td>0.707579</td>\n",
       "      <td>0.702587</td>\n",
       "      <td>0.708988</td>\n",
       "      <td>0.708745</td>\n",
       "      <td>0.709223</td>\n",
       "      <td>0.693128</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4042</th>\n",
       "      <td>0.546462</td>\n",
       "      <td>0.547252</td>\n",
       "      <td>0.546158</td>\n",
       "      <td>0.546371</td>\n",
       "      <td>0.546241</td>\n",
       "      <td>0.545721</td>\n",
       "      <td>0.546231</td>\n",
       "      <td>0.545329</td>\n",
       "      <td>0.545053</td>\n",
       "      <td>0.544635</td>\n",
       "      <td>...</td>\n",
       "      <td>0.516504</td>\n",
       "      <td>0.504406</td>\n",
       "      <td>0.494093</td>\n",
       "      <td>0.490169</td>\n",
       "      <td>0.484280</td>\n",
       "      <td>0.477127</td>\n",
       "      <td>0.479447</td>\n",
       "      <td>0.476531</td>\n",
       "      <td>0.484857</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4043</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       header0   header1   header2   header3   header4   header5   header6  \\\n",
       "4039  0.376258  0.371196  0.374959  0.381994  0.378138  0.375871  0.375117   \n",
       "4040  0.932410  0.930584  0.929513  0.928026  0.928930  0.929664  0.929868   \n",
       "4041  0.639841  0.640750  0.639536  0.639553  0.639504  0.639946  0.638688   \n",
       "4042  0.546462  0.547252  0.546158  0.546371  0.546241  0.545721  0.546231   \n",
       "4043  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "       header7   header8   header9  ...  header20  header21  header22  \\\n",
       "4039  0.372087  0.372973  0.372174  ...  0.313334  0.297908  0.300916   \n",
       "4040  0.929412  0.930946  0.929839  ...  0.887794  0.927183  0.923606   \n",
       "4041  0.639430  0.639626  0.639553  ...  0.689476  0.707595  0.706714   \n",
       "4042  0.545329  0.545053  0.544635  ...  0.516504  0.504406  0.494093   \n",
       "4043  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "\n",
       "      header23  header24  header25  header26  header27  header28  header29  \n",
       "4039  0.295691  0.304691  0.297850  0.298842  0.298431  0.323285       NaN  \n",
       "4040  0.929387  0.921559  0.932170  0.925471  0.922702  0.851257       NaN  \n",
       "4041  0.707579  0.702587  0.708988  0.708745  0.709223  0.693128       NaN  \n",
       "4042  0.490169  0.484280  0.477127  0.479447  0.476531  0.484857       NaN  \n",
       "4043  0.000000  0.000000  0.000000  0.000000  0.000000  1.000000       NaN  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "originaldata.iloc[4039:4044]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.54251199 0.52489482 0.52926319 0.5065195  0.49991385 0.49536954\n",
      "  0.48716645 0.48923134 0.48428939 0.51027604]\n",
      " [0.37858933 0.31333356 0.29790778 0.30091579 0.29569087 0.30469148\n",
      "  0.29785046 0.29884238 0.29843066 0.32328525]\n",
      " [0.93041265 0.88779361 0.92718331 0.92360628 0.92938742 0.92155946\n",
      "  0.9321705  0.92547146 0.92270207 0.85125721]\n",
      " [0.63809043 0.68947554 0.70759451 0.70671384 0.70757924 0.7025874\n",
      "  0.70898769 0.70874526 0.70922296 0.69312779]\n",
      " [0.54499817 0.51650444 0.50440577 0.49409301 0.49016859 0.48428045\n",
      "  0.47712741 0.4794472  0.4765313  0.48485743]]\n",
      "(1, 5, 10)\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "\n",
      "prediction from 10 timesteps 1.0079834461212158 actual 1.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#print(originaldata[headers[0:30]].iloc[4039:4044])\n",
    "#print(originaldata[headers[0:30]].iloc[4039:4044].to_numpy()\n",
    "classifytest=originaldata[headers[19:29]].iloc[4038:4043].to_numpy()\n",
    "labelstest=originaldata[headers[19:29]].iloc[4043].to_numpy()\n",
    "print(classifytest)\n",
    "classifytest=np.expand_dims(classifytest, axis=0)\n",
    "print(classifytest.shape)\n",
    "print(labelstest)\n",
    "\n",
    "outputfull=evaluate_episode(gru_model3, classifytest)\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "#print(\"evaluating 1st timestep repeated 10 times\")\n",
    "\n",
    "\n",
    "#print(\"prediction from 1 timestep\",float(outputpartial),\"actual\",test_y[randomindex])\n",
    "print(\"prediction from 10 timesteps\",float(outputfull),\"actual\",labelstest[9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction from timestep 0 - 10  : -0.06907479465007782 actual 0.0 OK\n",
      "prediction from timestep 1 - 11  : -0.15065325796604156 actual 0.0 OK\n",
      "prediction from timestep 2 - 12  : 0.04671326279640198 actual 0.0 OK\n",
      "prediction from timestep 3 - 13  : -0.17566485702991486 actual 0.0 OK\n",
      "prediction from timestep 4 - 14  : 0.00574512779712677 actual 0.0 OK\n",
      "prediction from timestep 5 - 15  : -0.005189254879951477 actual 0.0 OK\n",
      "prediction from timestep 6 - 16  : -0.0017557889223098755 actual 0.0 OK\n",
      "prediction from timestep 7 - 17  : -0.16984139382839203 actual 0.0 OK\n",
      "prediction from timestep 8 - 18  : -0.056690603494644165 actual 0.0 OK\n",
      "prediction from timestep 9 - 19  : 0.07976764440536499 actual 0.0 OK\n",
      "prediction from timestep 10 - 20  : -0.011513561010360718 actual 0.0 OK\n",
      "prediction from timestep 11 - 21  : -0.0013253092765808105 actual 0.0 OK\n",
      "prediction from timestep 12 - 22  : -0.03464917838573456 actual 0.0 OK\n",
      "prediction from timestep 13 - 23  : -0.01900404691696167 actual 0.0 OK\n",
      "prediction from timestep 14 - 24  : -0.13939113914966583 actual 0.0 OK\n",
      "prediction from timestep 15 - 25  : 0.029168039560317993 actual 0.0 OK\n",
      "prediction from timestep 16 - 26  : 0.25161445140838623 actual 0.0 OK\n",
      "prediction from timestep 17 - 27  : 0.09541738778352737 actual 0.0 OK\n",
      "prediction from timestep 18 - 28  : 0.02386459708213806 actual 0.0 OK\n",
      "prediction from timestep 19 - 29  : 0.995641827583313 actual 1.0 OK\n",
      "okcounter 20\n"
     ]
    }
   ],
   "source": [
    "\n",
    "okcounter=0\n",
    "for i in range(20):\n",
    "    classifytest=originaldata[headers[i:10+i]].iloc[4296:4301].to_numpy()\n",
    "    labelstest=originaldata[headers[i:10+i]].iloc[4301].to_numpy()\n",
    "    #print(classifytest)\n",
    "    classifytest=np.expand_dims(classifytest, axis=0)\n",
    "    #print(classifytest.shape)\n",
    "    #print(labelstest)\n",
    "\n",
    "    outputfull=evaluate_episode(gru_model3, classifytest)\n",
    "\n",
    "    #print(\"\")\n",
    "\n",
    "    #print(\"evaluating 1st timestep repeated 10 times\")\n",
    "\n",
    "\n",
    "    #print(\"prediction from 1 timestep\",float(outputpartial),\"actual\",test_y[randomindex])\n",
    "    \n",
    "    if abs(float(outputfull)-labelstest[9])>0.3:\n",
    "        result=\"X\"\n",
    "    else:\n",
    "        result=\"OK\"\n",
    "        okcounter+=1\n",
    "    print(\"prediction from timestep\",i,\"-\",i+10,\" :\",float(outputfull),\"actual\",labelstest[9], result)\n",
    "print(\"okcounter\",okcounter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>header0</th>\n",
       "      <th>header1</th>\n",
       "      <th>header2</th>\n",
       "      <th>header3</th>\n",
       "      <th>header4</th>\n",
       "      <th>header5</th>\n",
       "      <th>header6</th>\n",
       "      <th>header7</th>\n",
       "      <th>header8</th>\n",
       "      <th>header9</th>\n",
       "      <th>...</th>\n",
       "      <th>header20</th>\n",
       "      <th>header21</th>\n",
       "      <th>header22</th>\n",
       "      <th>header23</th>\n",
       "      <th>header24</th>\n",
       "      <th>header25</th>\n",
       "      <th>header26</th>\n",
       "      <th>header27</th>\n",
       "      <th>header28</th>\n",
       "      <th>header29</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4890</th>\n",
       "      <td>0.519653</td>\n",
       "      <td>0.629009</td>\n",
       "      <td>0.666283</td>\n",
       "      <td>0.757858</td>\n",
       "      <td>0.748447</td>\n",
       "      <td>0.748698</td>\n",
       "      <td>0.751999</td>\n",
       "      <td>0.743304</td>\n",
       "      <td>0.741959</td>\n",
       "      <td>0.748569</td>\n",
       "      <td>...</td>\n",
       "      <td>0.785070</td>\n",
       "      <td>0.768524</td>\n",
       "      <td>0.756693</td>\n",
       "      <td>0.763045</td>\n",
       "      <td>0.771942</td>\n",
       "      <td>0.774374</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4891</th>\n",
       "      <td>0.372140</td>\n",
       "      <td>0.333817</td>\n",
       "      <td>0.319136</td>\n",
       "      <td>0.330614</td>\n",
       "      <td>0.324397</td>\n",
       "      <td>0.318553</td>\n",
       "      <td>0.308498</td>\n",
       "      <td>0.310825</td>\n",
       "      <td>0.340429</td>\n",
       "      <td>0.380643</td>\n",
       "      <td>...</td>\n",
       "      <td>0.510981</td>\n",
       "      <td>0.545773</td>\n",
       "      <td>0.534438</td>\n",
       "      <td>0.555681</td>\n",
       "      <td>0.578675</td>\n",
       "      <td>0.561716</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4892</th>\n",
       "      <td>0.932075</td>\n",
       "      <td>0.880805</td>\n",
       "      <td>0.871212</td>\n",
       "      <td>0.782250</td>\n",
       "      <td>0.917737</td>\n",
       "      <td>0.913229</td>\n",
       "      <td>0.930457</td>\n",
       "      <td>0.932578</td>\n",
       "      <td>0.922336</td>\n",
       "      <td>0.936276</td>\n",
       "      <td>...</td>\n",
       "      <td>0.951116</td>\n",
       "      <td>0.928739</td>\n",
       "      <td>0.959949</td>\n",
       "      <td>0.949016</td>\n",
       "      <td>0.937812</td>\n",
       "      <td>0.899633</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4893</th>\n",
       "      <td>0.641091</td>\n",
       "      <td>0.669016</td>\n",
       "      <td>0.684142</td>\n",
       "      <td>0.675920</td>\n",
       "      <td>0.699291</td>\n",
       "      <td>0.686433</td>\n",
       "      <td>0.692765</td>\n",
       "      <td>0.690845</td>\n",
       "      <td>0.670727</td>\n",
       "      <td>0.643720</td>\n",
       "      <td>...</td>\n",
       "      <td>0.498684</td>\n",
       "      <td>0.474195</td>\n",
       "      <td>0.481877</td>\n",
       "      <td>0.466092</td>\n",
       "      <td>0.447252</td>\n",
       "      <td>0.446521</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4894</th>\n",
       "      <td>0.550714</td>\n",
       "      <td>0.634439</td>\n",
       "      <td>0.686677</td>\n",
       "      <td>0.743846</td>\n",
       "      <td>0.772609</td>\n",
       "      <td>0.768783</td>\n",
       "      <td>0.769648</td>\n",
       "      <td>0.771199</td>\n",
       "      <td>0.775574</td>\n",
       "      <td>0.782100</td>\n",
       "      <td>...</td>\n",
       "      <td>0.835276</td>\n",
       "      <td>0.823710</td>\n",
       "      <td>0.817117</td>\n",
       "      <td>0.822949</td>\n",
       "      <td>0.825044</td>\n",
       "      <td>0.824857</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4895</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       header0   header1   header2   header3   header4   header5   header6  \\\n",
       "4890  0.519653  0.629009  0.666283  0.757858  0.748447  0.748698  0.751999   \n",
       "4891  0.372140  0.333817  0.319136  0.330614  0.324397  0.318553  0.308498   \n",
       "4892  0.932075  0.880805  0.871212  0.782250  0.917737  0.913229  0.930457   \n",
       "4893  0.641091  0.669016  0.684142  0.675920  0.699291  0.686433  0.692765   \n",
       "4894  0.550714  0.634439  0.686677  0.743846  0.772609  0.768783  0.769648   \n",
       "4895  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "       header7   header8   header9  ...  header20  header21  header22  \\\n",
       "4890  0.743304  0.741959  0.748569  ...  0.785070  0.768524  0.756693   \n",
       "4891  0.310825  0.340429  0.380643  ...  0.510981  0.545773  0.534438   \n",
       "4892  0.932578  0.922336  0.936276  ...  0.951116  0.928739  0.959949   \n",
       "4893  0.690845  0.670727  0.643720  ...  0.498684  0.474195  0.481877   \n",
       "4894  0.771199  0.775574  0.782100  ...  0.835276  0.823710  0.817117   \n",
       "4895  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "\n",
       "      header23  header24  header25  header26  header27  header28  header29  \n",
       "4890  0.763045  0.771942  0.774374       NaN       NaN       NaN       NaN  \n",
       "4891  0.555681  0.578675  0.561716       NaN       NaN       NaN       NaN  \n",
       "4892  0.949016  0.937812  0.899633       NaN       NaN       NaN       NaN  \n",
       "4893  0.466092  0.447252  0.446521       NaN       NaN       NaN       NaN  \n",
       "4894  0.822949  0.825044  0.824857       NaN       NaN       NaN       NaN  \n",
       "4895  0.000000  0.000000  1.000000       NaN       NaN       NaN       NaN  \n",
       "\n",
       "[6 rows x 30 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "originaldata.iloc[4890:4896]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction from timestep 0 - 10  : 0.009794026613235474 actual 0.0 OK\n",
      "prediction from timestep 1 - 11  : -0.0471053272485733 actual 0.0 OK\n",
      "prediction from timestep 2 - 12  : -0.038722336292266846 actual 0.0 OK\n",
      "prediction from timestep 3 - 13  : -0.025433629751205444 actual 0.0 OK\n",
      "prediction from timestep 4 - 14  : 0.012715950608253479 actual 0.0 OK\n",
      "prediction from timestep 5 - 15  : -6.215274333953857e-05 actual 0.0 OK\n",
      "prediction from timestep 6 - 16  : 0.01754795014858246 actual 0.0 OK\n",
      "prediction from timestep 7 - 17  : 0.024392813444137573 actual 0.0 OK\n",
      "prediction from timestep 8 - 18  : -0.040488868951797485 actual 0.0 OK\n",
      "prediction from timestep 9 - 19  : -0.02546258270740509 actual 0.0 OK\n",
      "prediction from timestep 10 - 20  : -0.0829649567604065 actual 0.0 OK\n",
      "prediction from timestep 11 - 21  : -0.042378008365631104 actual 0.0 OK\n",
      "prediction from timestep 12 - 22  : 0.030086442828178406 actual 0.0 OK\n",
      "prediction from timestep 13 - 23  : -0.019095852971076965 actual 0.0 OK\n",
      "prediction from timestep 14 - 24  : -0.026990339159965515 actual 0.0 OK\n",
      "prediction from timestep 15 - 25  : 0.21670079231262207 actual 0.0 OK\n",
      "prediction from timestep 16 - 26  : 0.9175497889518738 actual 1.0 OK\n",
      "prediction from timestep 17 - 27  : nan actual nan OK\n",
      "prediction from timestep 18 - 28  : nan actual nan OK\n",
      "prediction from timestep 19 - 29  : nan actual nan OK\n",
      "okcounter 20\n"
     ]
    }
   ],
   "source": [
    "okcounter=0\n",
    "for i in range(20):\n",
    "    classifytest=originaldata[headers[i:10+i]].iloc[4890:4895].to_numpy()\n",
    "    labelstest=originaldata[headers[i:10+i]].iloc[4895].to_numpy()\n",
    "    #print(classifytest)\n",
    "    classifytest=np.expand_dims(classifytest, axis=0)\n",
    "    #print(classifytest.shape)\n",
    "    #print(labelstest)\n",
    "\n",
    "    outputfull=evaluate_episode(gru_model3, classifytest)\n",
    "\n",
    "    #print(\"\")\n",
    "\n",
    "    #print(\"evaluating 1st timestep repeated 10 times\")\n",
    "\n",
    "\n",
    "    #print(\"prediction from 1 timestep\",float(outputpartial),\"actual\",test_y[randomindex])\n",
    "    \n",
    "    if abs(float(outputfull)-labelstest[9])>0.3:\n",
    "        result=\"X\"\n",
    "    else:\n",
    "        result=\"OK\"\n",
    "        okcounter+=1\n",
    "    print(\"prediction from timestep\",i,\"-\",i+10,\" :\",float(outputfull),\"actual\",labelstest[9], result)\n",
    "print(\"okcounter\",okcounter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
